Resource,Prompt,Rego intent,Difficulty,Reference output,Intent
"aws_route53_record, aws_route53_zone",Set up a record that maps a domain name to an IPv4 address using Route 53 resources,"package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""A""
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""A""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"
"aws_route53_record, aws_route53_zone",Set up a record that maps a domain name to an IPv6 address using Route 53 resources,"package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""AAAA""
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""AAAA""
  ttl     = ""300""
  records = [""2001:0db8:85a3:0000:0000:8a2e:0370:7334""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""AAAA""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"
"aws_route53_record, aws_route53_zone","Set up a Pointer record for reverse DNS using Route 53 resources. The domain name should be ""host.example53.com"" and name the zone ""reverse_zone""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""2.0.192.in-addr.arpa""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""PTR""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""host.example.com""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.reverse_zone.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

# Create Reverse DNS Hosted Zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""2.0.192.in-addr.arpa""
}

# Create a PTR Record for a specific IP address within that zone
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""53.2.0.192.in-addr.arpa""
  type    = ""PTR""
  ttl     = ""3600""
  records = [""host.example.com""]
}","Has one ""aws_route53_zone"" resource
    with ""name"" ending in "".in-addr.arpa""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""PTR""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"
"aws_route53_record, aws_route53_zone","Set up a TXT recordfor domain ownership verification purposes using Route 53 resources. The verification string should be ""passwordpassword"" and the name of the zone should be ""example""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""TXT""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""passwordpassword""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example"" {
  name = ""example53.com""
}

# TXT Record for domain verification or other purposes
resource ""aws_route53_record"" ""example_txt"" {
  zone_id = aws_route53_zone.example.zone_id
  name    = ""sub.example53.com""
  type    = ""TXT""
  ttl     = ""300""
  records = [""passwordpassword""] 
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""TXT""
    with ""ttl""
    with ""records"" set to a string
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_configuration_template",Create a template of an elastic beanstalk application,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false

default is_valid_eb_app_template = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}



is_valid_eb_app_template {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_configuration_template""
    resource.expressions.name
    resource.expressions.solution_stack_name
    startswith(resource.expressions.application.references[0], ""aws_elastic_beanstalk_application"")
}


is_configuration_valid {
    is_valid_eb_app
    is_valid_eb_app_template
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}

resource ""aws_elastic_beanstalk_configuration_template"" ""tf_template"" {
  name                = ""tf-test-template-config""
  application         = aws_elastic_beanstalk_application.tftest.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.0.9 running Python 3.11""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_configuration_template"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name"" "
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_configuration_template",Create a template of an elastic beanstalk application that is running a version of Go,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false

default is_valid_eb_app_template = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}



is_valid_eb_app_template {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_configuration_template""
    resource.expressions.name
    endswith(resource.expressions.solution_stack_name.constant_value, ""Go 1"")
    startswith(resource.expressions.application.references[0], ""aws_elastic_beanstalk_application"")
}


is_configuration_valid {
    is_valid_eb_app
    is_valid_eb_app_template
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}

resource ""aws_elastic_beanstalk_configuration_template"" ""tf_template"" {
  name                = ""tf-test-template-config""
  application         = aws_elastic_beanstalk_application.tftest.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.10.1 running Go 1""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_configuration_template"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name"" that references a valid ""Go"" version"
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name"""
aws_kinesis_stream,Generate a basic Amazon Kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_stream {
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_stream
}",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""drow1""
  shard_count      = 1
  retention_period = 48

  shard_level_metrics = [
    ""IncomingBytes"",
    ""OutgoingBytes"",
  ]

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""test""
  }
}","Has one ""aws_kinesis_stream"" resource"
aws_kinesis_video_stream,Generate a basic Kinesis Video Stream resource,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_video_stream {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_video_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_video_stream
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_kinesis_video_stream"" ""default"" {
  name                    = ""terraform-kinesis-video-stream""
  data_retention_in_hours = 1
  device_name             = ""kinesis-video-device-name""
  media_type              = ""video/h264""

  tags = {
    Name = ""terraform-kinesis-video-stream""
  }
}","Has one ""aws_kinesis_video_stream"" resource"
"aws_kinesis_stream, aws_kinesis_stream_consumer",Generate a resource to manage a Kinesis Stream Consumer,"package terraform.validation

import future.keywords.in

default has_valid_resources = false


has_valid_kinesis_stream {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_kinesis_stream_consumer {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream_consumer""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.stream_arn
}

has_valid_resources {
	has_valid_kinesis_stream
    has_valid_kinesis_stream_consumer
}
",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""example-stream""
  shard_count = 1
}

resource ""aws_kinesis_stream_consumer"" ""example"" {
  name       = ""example-consumer""
  stream_arn = aws_kinesis_stream.example.arn
}","Has one ""aws_kinesis_stream"" resource and one ""aws_kinesis_stream_consumer"" resource"
aws_kinesis_analytics_application,Generate a basic Amazon Kinesis Analytics Application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
}

has_valid_resources {
    has_valid_kinesis_analytics_application
}",1,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_analytics_application"" ""test_application"" {
  name = ""kinesis-analytics-application-test""
}","Has one ""aws_kinesis_analytics_application"" resource"
aws_route53_zone,"Set up an aws_route_53 zone named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false


# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
}
",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com"""
"aws_route53_record, aws_route53_zone","Set up a non-alias aws_route_53 record that is linked to an aws_route53_zone resource named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
	some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"
"aws_s3_bucket, aws_s3_bucket_metric, aws_s3_bucket_object","Set up an AWS S3 bucket named 'my_bucket' with forced destruction enabled for cleanup purposes. Create an S3 bucket metric named 'my_bucket_metric' to monitor the entire bucket's activity. Additionally, include an S3 bucket object named 'my_object' with a specific key and content to be stored in the bucket.","package terraform.validation

default valid_configuration = false
default has_s3_bucket = false
default has_s3_bucket_metrix = false
default has_s3_bucket_object = false


# Check for if any aws_s3_bucket with ""bucket""
has_s3_bucket {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.name == ""my_bucket""
    resource.expressions.bucket.constant_value != null
}

has_s3_bucket_metrix{
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket_metric""
    resource.name == ""my_bucket_matric""
    resource.expressions.bucket.references != null
    resource.expressions.name.constant_value != null
}

has_s3_bucket_object{
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket_object""
    resource.name == ""my_bucket_object""
    reference_check(resource)
    resource.expressions.key.constant_value != null
}

reference_check(object){
	some i
    object.expressions.bucket.references[i] == ""aws_s3_bucket.my_bucket.id""
}

valid_configuration{
    has_s3_bucket
    has_s3_bucket_metrix
    
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""test"" {
  bucket        = ""metricbeat-test-bucket""
  force_destroy = true // Required for cleanup
}

resource ""aws_s3_bucket_metric"" ""test"" {
  bucket = aws_s3_bucket.test.id
  name   = ""EntireBucket""
}

resource ""aws_s3_bucket_object"" ""test"" {
  key     = ""someobject""
  bucket  = aws_s3_bucket.test.id
  content = ""something""
}","has one ""aws_s3_bucket"" resource
    with one ""bucket""

has one ""aws_s3_bucket_metric""
    with one ""bucket""
    with one ""name

has one ""aws_s3_bucket_object""
    with one ""bucket""
    with one ""key""
"
"aws_s3_bucket, aws_s3_bucket_versioning","Create a bucket ""sample"". Implement versioning resource for the AWS S3 bucket named 'sample' with the versioning status set to 'Enabled' and specify the expected_bucket_owner as '123456789012' to ensure ownership consistency.","package terraform.validation

default has_s3_bucket_versioning = false

default has_s3_bucket = false

default valid_configuration = false

has_s3_bucket {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.name == ""sample""
}

has_s3_bucket_versioning {
    versioning := input.planned_values.root_module.resources[_]
    versioning.type == ""aws_s3_bucket_versioning""
    versioning.values.bucket == ""sample""
    versioning.values.expected_bucket_owner == ""123456789012""
    versioning.values.versioning_configuration[_].status == ""Enabled""

}

valid_configuration {
        has_s3_bucket
    has_s3_bucket_versioning
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""sample"" {
  bucket = ""sample""
}

resource ""aws_s3_bucket_versioning"" ""sample"" {
  bucket = ""sample""
  versioning_configuration {
    status = ""Enabled""
  }
  expected_bucket_owner = ""123456789012""
}","has one aws_s3_bucket resource
with bucket

has one ""aws_s3_bucket_versioning"" resource
     with bucket
     with versionaning_configuration
          with status"
"aws_s3_bucket, aws_s3_bucket_logging","Create a bucket ""a"". Then configure logging for the bucket to send access logs to another bucket named 'logging-680235478471' with a prefix of 'log/' in the target bucket.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_logging = false

default valid_configuration = false

has_s3_bucket {
    bucket := input.configuration.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""a""
}

has_s3_bucket_logging {
    logging := input.configuration.root_module.resources[_]
    logging.type == ""aws_s3_bucket_logging""
    logging.expression.bucket.constant_value == ""a""
    logging.expression.target_bucket.constant_value == ""logging-680235478471""
    logging.expression.target_prefix.constant_value == ""log/""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_logging
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""testbucketineu-west2""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket        = ""testbucketineu-west2""
  target_bucket = ""logging-680235478471""
  target_prefix = ""log/""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_logging resource
with bucket
with target_bucket
with target_prefix"
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration",Create an AWS S3 bucket named 'example-bucket' with object lock enabled. Configure object lock governance mode with a retention period of 90 days for objects in the bucket.,"package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_object_lock_configuration = false

default valid_configuration = false


has_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example-bucket""
}

has_s3_bucket_object_lock_configuration {
    lock_config := input.planned_values.root_module.resources[_]
    lock_config.type == ""aws_s3_bucket_object_lock_configuration""
    lock_config.values.bucket == ""example-bucket""
    default_retention := lock_config.values.rule[_].default_retention[_]
    default_retention.mode == ""GOVERNANCE""
        default_retention.days == 90
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_object_lock_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    default_retention {
      mode  = ""GOVERNANCE""
      days  = 90
    }
  }
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_object_lock_configuration resource
with bucket
with rule
     with default_retention
          with mode (GOVERNANCE or COMPLIANCE)
          with days or years"
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration",Create an AWS S3 bucket named 'example-bucket' with object lock enabled. Configure object lock compliance mode with a retention period of 30 days for objects in the bucket.,"package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_object_lock_configuration = false

default valid_configuration = false


has_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example-bucket""
}

has_s3_bucket_object_lock_configuration {
    lock_config := input.planned_values.root_module.resources[_]
    lock_config.type == ""aws_s3_bucket_object_lock_configuration""
    lock_config.values.bucket == ""example-bucket""
    default_retention := lock_config.values.rule[_].default_retention[_]
    default_retention.mode == ""COMPLIANCE""
        default_retention.days == 30
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_object_lock_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    default_retention {
      mode  = ""COMPLIANCE""
      days  = 30
    }
  }
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_object_lock_configuration resource
with bucket
with rule
     with default_retention
          with mode (GOVERNANCE or COMPLIANCE)
          with days or years"
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration","create a bucket 'pike-680235478471'. Set up request payment configuration for an AWS S3 bucket named 'pike-680235478471', specifying the payer as 'Requester'.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_request_payment_configuration = false

default valid_configuration = false

has_s3_bucket {
    some i
    bucket := input.planned_values.root_module.resources[i]
    bucket.type == ""aws_s3_bucket""
    bucket.values.bucket == ""pike-680235478471""
}

has_s3_bucket_request_payment_configuration {
    some i
    payment_config := input.planned_values.root_module.resources[i]
    payment_config.type == ""aws_s3_bucket_request_payment_configuration""
    payment_config.values.bucket == ""pike-680235478471""
    payment_config.values.payer == ""Requester""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_request_payment_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""pike"" {
  bucket = ""pike-680235478471""
  payer  = ""Requester""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_request_payment_configuration resource
with bucket
with payer (BucketOwner or Requester)"
"aws_s3_bucket, aws_s3_bucket_public_access_block","Implement public access block settings for an AWS S3 bucket named 'pike-680235478471' to block public ACLs, public policies, and restrict public buckets.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_public_access_block = false

default valid_configuration = false

has_s3_bucket {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.values.bucket == ""pike-680235478471""
}

has_s3_bucket_public_access_block {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket_public_access_block""
    resource.values.bucket ==  ""pike-680235478471""
    resource.values.block_public_acls == true
    resource.values.block_public_policy == true
    resource.values.ignore_public_acls == true
    resource.values.restrict_public_buckets == true

}


valid_configuration {
    has_s3_bucket
    has_s3_bucket_public_access_block
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = ""pike-680235478471""

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_public_access_block resource
with bucket
with block_public_acls (boolean)
with block_public_policy (boolean)
with ignore_public_acls (boolean)
with restrict_public_buckets (boolean)"
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration","Create a Terraform configuration for an AWS environment. Start by defining a KMS key with a specific description and a set deletion window in days. Next, define an S3 bucket with a specified name. Finally, configure server-side encryption for the S3 bucket using the previously defined KMS key. The encryption should use the KMS algorithm specified in AWS. Include resource blocks for each component and ensure the bucket encryption references the KMS key's ARN and uses the KMS algorithm for encryption.","package terraform.validation

default has_aws_kms_key = false
default has_aws_s3_bucket = false
default has_aws_s3_bucket_server_side_encryption_configuration = false

has_aws_kms_key {
    key := input.planned_values.root_module.resources[_]
    key.type == ""aws_kms_key""
    key.name == ""mykey""
    key.values.description == ""This key is used to encrypt bucket objects""
    key.values.deletion_window_in_days == 10
}

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""mybucket""
    bucket.values.bucket == ""mybucket""
}

has_aws_s3_bucket_server_side_encryption_configuration {
    encryption_config := input.planned_values.root_module.resources[_]
    encryption_config.type == ""aws_s3_bucket_server_side_encryption_configuration""
    encryption_config.name == ""example""
    encryption_config.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    rule := encryption_config.values.rule.apply_server_side_encryption_by_default
    rule.kms_master_key_id == input.planned_values.root_module.resources[_].values.arn  # Correct KMS ARN reference
    rule.sse_algorithm == ""aws:kms""
}

valid_configuration {
    has_aws_kms_key
    has_aws_s3_bucket
    has_aws_s3_bucket_server_side_encryption_configuration
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Resource ""aws_kms_key""
has one ""description""
has one ""deletion_window_in_days""

Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_server_side_encryption_configuration""
has one ""bucket""
within ""rule"", it specifies:
""kms_master_key_id""
""sse_algorithm"""
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_ownership_controls","Generate a Terraform configuration for managing AWS S3 resources. Start by creating an S3 bucket with a specific name. Follow this with a configuration block for S3 bucket ownership controls, referencing the bucket's ID and setting the object ownership policy to a predefined setting. Then, define an S3 bucket ACL resource that depends on the successful application of the ownership controls. This ACL should set the bucket access to private and also reference the bucket's ID. Ensure each resource block is defined clearly and connected appropriately through references.","package terraform.validation

default has_aws_s3_bucket = false
default has_aws_s3_bucket_ownership_controls = false
default has_aws_s3_bucket_acl = false

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example""
    bucket.values.bucket == ""my-tf-example-bucket""
}

has_aws_s3_bucket_ownership_controls {
    ownership_controls := input.planned_values.root_module.resources[_]
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    ownership_controls.name == ""example""
    ownership_controls.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    ownership_controls.values.rule.object_ownership == ""BucketOwnerPreferred""
}

has_aws_s3_bucket_acl {
    acl := input.planned_values.root_module.resources[_]
    acl.type == ""aws_s3_bucket_acl""
    acl.name == ""example""
    acl.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    acl.values.acl == ""private""
    dependency := acl.depends_on[_]
    dependency == ""aws_s3_bucket_ownership_controls.example""
}

valid_configuration {
    has_aws_s3_bucket
    has_aws_s3_bucket_ownership_controls
    has_aws_s3_bucket_acl
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-tf-example-bucket""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]

  bucket = aws_s3_bucket.example.id
  acl    = ""private""
}","Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_ownership_controls""
has one ""bucket""
within ""rule"", it specifies:
""object_ownership""

Resource ""aws_s3_bucket_acl""
has a dependency on ""aws_s3_bucket_ownership_controls.example""
has one ""bucket""
has one ""acl""
"
aws_s3_bucket_object,"Craft a Terraform configuration for creating an AWS S3 bucket object. Define an S3 bucket object (with value ""object"") with specific attributes including the bucket name (with value ""your_bucket_name""), object key (with value ""new_object_key""), and the path to the source file (with value ""path/to/file""). ","package terraform.validation

default has_aws_s3_bucket_object = false

has_aws_s3_bucket_object {
    object := input.planned_values.root_module.resources[_]
    object.type == ""aws_s3_bucket_object""
    object.name == ""object""
    object.values.bucket == ""your_bucket_name""
    object.values.key == ""new_object_key""
    object.values.source == ""path/to/file""
}

valid_configuration {
    has_aws_s3_bucket_object
}",1,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket_object"" ""object"" {
  bucket = ""your_bucket_name""
  key    = ""new_object_key""
  source = ""path/to/file""
}","Resource ""aws_s3_bucket_object""
has one ""bucket""
has one ""key""
has one ""source""
has one ""etag"""
"aws_s3_bucket, aws_s3_bucket_ownership_controls","Generate a Terraform configuration for creating an AWS S3 bucket and setting its ownership controls. Define a resource for an S3 bucket with a specified name. Follow this by configuring ownership controls for this bucket, referencing its ID. Within the ownership controls, set a rule to specify the object ownership as 'BucketOwnerPreferred'. Ensure that the resources are properly linked using references, and the configuration is clearly laid out to reflect these settings.","package terraform.validation

default has_aws_s3_bucket_example = false
default has_aws_s3_bucket_ownership_controls_example = false

has_aws_s3_bucket_example {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example""
    bucket.values.bucket == ""example""
}

has_aws_s3_bucket_ownership_controls_example {
    controls := input.planned_values.root_module.resources[_]
    controls.type == ""aws_s3_bucket_ownership_controls""
    controls.name == ""example""
    controls.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensures it's the correct reference to ""example"" bucket
    controls.values.rule.object_ownership == ""BucketOwnerPreferred""
}

valid_configuration {
    has_aws_s3_bucket_example
    has_aws_s3_bucket_ownership_controls_example
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}","Resource ""aws_s3_bucket"" 
has one ""bucket""

Resource ""aws_s3_bucket_ownership_controls""
has one ""bucket""
within the ""rule"", it specifies:
""object_ownership"""
aws_ami,Create an AWS aws_ami resource for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any
aws_ami,Create an AWS aws_ami for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name"
aws_elasticache_cluster,Provide a Redis Instance,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""redis""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""redis""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 1
  parameter_group_name = ""default.redis3.2""
  engine_version       = ""3.2.10""
  port                 = 6379
}","requires cluster id, engine=redis, node_type, num_cache_nodes, parameter_group_name"
aws_elasticache_user,Provide an ElastiCache user resource with redis,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.engine == ""REDIS""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}","requires access string, engine = REDIS, user id and user name"
aws_elasticache_user,Provide an ElastiCache user resource with iam,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""iam""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type = ""iam""
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type = ""iam""}"
aws_elasticache_user,Provide an ElastiCache user resource with password.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""password""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type      = ""password""
    passwords = [""password1"", ""password2""]
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type= ""password"", passwords = [""password1"", ""password2""]}"
"aws_elasticache_user, aws_elasticache_user_group",Provide an ElastiCache user group resource.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}","creates elasticache user resources(requires access string, engine = REDIS, user id and user name) and sets engine , creates group resource using the user ids"
aws_redshift_cluster,Create a RedShift cluster resource with a single node,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 1
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
}
",1,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""tf-redshift-cluster""
  database_name      = ""mydb""
  master_username    = ""exampleuser""
  master_password    = ""Mustbe8characters""
  node_type          = ""dc2.large""
  cluster_type       = ""single-node""
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 1"
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60"
"aws_redshift_cluster, aws_redshift_endpoint_authorization",Create a 2 node RedShift cluster and create a new Amazon Redshift endpoint authorization for an account with AWS id 012345678910,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

# Function to check if Redshift snapshot schedule is every 12 hours
aws_redshift_endpoint_authorization_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_endpoint_authorization""
    resource.change.after.account == ""012345678910""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_endpoint_authorization_valid(input.resource_changes)
}",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_endpoint_authorization"" ""example"" {
  account            = ""012345678910""
  cluster_identifier = aws_redshift_cluster.example.cluster_identifier
}",Has an aws_redshift_resouce with 2 nodes and creates a aws_redshift_endpoint_authorization resource with account =012345678910
"aws_redshift_cluster, aws_redshift_snapshot_schedule, aws_redshift_snapshot_schedule_association",Create a 1 node RedShift cluster and automatically create a snapshot every 12 hours,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 1
}

# Function to check if Redshift snapshot schedule is every 12 hours
aws_redshift_snapshot_schedule_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_snapshot_schedule""
    resource.change.after.definitions[_] == ""rate(12 hours)""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_snapshot_schedule_valid(input.resource_changes)
}",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 1

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_snapshot_schedule"" ""default"" {
  identifier = ""tf-redshift-snapshot-schedule""
  definitions = [
    ""rate(12 hours)"",
  ]
}

resource ""aws_redshift_snapshot_schedule_association"" ""default"" {
  cluster_identifier  = aws_redshift_cluster.example.id
  schedule_identifier = aws_redshift_snapshot_schedule.default.id
}",Has an aws_redshift resouce with 1 node and aws_redshift_snapshot_schedule resource with rate set to 12 hours. Then have a aws_redshift_snapshot_schedle_association to conect the 2
aws_redshift_cluster,Setup a 2 node RedShift cluster in us-east-1 and replicate it to us-east-2.,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
    resource.change.after.snapshot_copy[_].destination_region == ""us-east-2""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
}
",2,"provider ""aws"" {
  region = ""us-east-1""
  alias  = ""useast1""
}

# Redshift Cluster in us-east-1
resource ""aws_redshift_cluster"" ""primary_cluster"" {
  provider = aws.useast1

  cluster_identifier  = ""primary-cluster""
  database_name       = ""mydatabase""
  master_username     = ""myusername""
  master_password     = ""Mypassword1""
  node_type           = ""dc2.large""
  cluster_type        = ""multi-node""
  number_of_nodes     = 2
  skip_final_snapshot = true

  snapshot_copy {
    destination_region = ""us-east-2""
  }

}
",Creates a redshift_clluster in us-east-1 and a snapshot_copy in us-east-2
aws_lambda_layer_version,"Create a Lambda Layer Version resource from ""lambda_layer_payload.zip""","package main

import future.keywords.in

default allow = false

# Check if the AWS Lambda Layer Version is valid
aws_lambda_layer_version_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_layer_version""
    resource.change.after.filename == ""lambda_layer_payload.zip""
}

# Aggregate all checks
allow {
    aws_lambda_layer_version_valid(input.resource_changes)
}",1,"resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename   = ""lambda_layer_payload.zip""
  layer_name = ""lambda_layer_name""

  compatible_runtimes = [""nodejs16.x""]
}","Create a aws_lambda_layer_version by using file_name = ""lambda_layer_payload.zip"". Layer name can be any."
"aws_cloudwatch_composite_alarm, aws_cloudwatch_metric_alarm",Create a CloudWatch Composite Alarm resource.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_composite_alarm""
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}",2,"resource ""aws_cloudwatch_composite_alarm"" ""example"" {
  alarm_name        = ""example-composite-alarm""

  alarm_rule = <<EOF
ALARM(${aws_cloudwatch_metric_alarm.foobar.alarm_name})
EOF

}

resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = ""2""
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = ""120""
  statistic                 = ""Average""
  threshold                 = ""80""
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",Creates an aws_cloudwatch_composite_alarm which requires at least one other aws_cloudwatch_metric_alarm
aws_cloudwatch_metric_alarm,Create a CloudWatch Metric Alarm resource.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_metric_alarm""
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}",2,"resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 120
  statistic                 = ""Average""
  threshold                 = 80
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",Createsr a aws_cloudwatch_metric_alarm
aws_cloudwatch_metric_alarm,Generate a Terraform HCL code snippet to define an AWS CloudWatch metric alarm named 'foobar' that monitors the 'CPUUtilization' metric of AWS EC2 instances. The alarm should trigger when the average CPU utilization is greater than or equal to 80% for two consecutive 2-minute periods. Ensure that no actions are taken when there's insufficient data.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_metric_alarm""
    resource.change.after.comparison_operator == ""GreaterThanOrEqualToThreshold""
    resource.change.after.threshold == 80
    resource.change.after.evaluation_periods == 2
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}
",2,"resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 120
  statistic                 = ""Average""
  threshold                 = 80
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",
aws_dynamodb_contributor_insights,Create a DynamoDB Contributor Insights resource for a specific table with custom settings,"package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Contributor Insights is being created for a specific table
aws_dynamodb_contributor_insights_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_contributor_insights""
}

# Aggregate all checks
allow {
    aws_dynamodb_contributor_insights_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_contributor_insights"" ""test"" {
  table_name = ""ExampleTableName""
}",Create a DynamoDB table and enable contributor insights
"aws_dynamodb_kinesis_streaming_destination, aws_dynamodb_table, aws_kinesis_stream",Create a Kinesis Streaming Destination for a specific DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Kinesis Streaming Destination is being created for a specific table
aws_dynamodb_kinesis_streaming_destination_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_kinesis_streaming_destination""
}

# Aggregate all checks
allow {
    aws_dynamodb_kinesis_streaming_destination_valid(input.resource_changes)
}",2,"resource ""aws_dynamodb_table"" ""example"" {
  name     = ""orders""
  hash_key = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""order_item_changes""
  shard_count = 1
}

resource ""aws_dynamodb_kinesis_streaming_destination"" ""example"" {
  stream_arn = aws_kinesis_stream.example.arn
  table_name = aws_dynamodb_table.example.name
}","create a stream_arn (name), dynamodb table(hash_key, name, attribute (name, type)), create the kineses streaming"
aws_dax_parameter_group,Create a custom DAX parameter group,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_parameter_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_parameter_group""
}

# Aggregate all checks
allow {
    aws_dax_parameter_group_valid(input.resource_changes)
}",2,"resource ""aws_dax_parameter_group"" ""example"" {
  name = ""example""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""100000""
  }
}","require name of the group, list of parameters (key, value) pairs"
aws_dynamodb_table,Configure on-demand capacity mode for a DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dynamodb_table_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_table""
    resource.change.after.billing_mode == ""PAY_PER_REQUEST""
}

# Aggregate all checks
allow {
    aws_dynamodb_table_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_table"" ""basic-dynamodb-table"" {
  name           = ""example""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""UserId""

  attribute {
    name = ""UserId""
    type = ""S""
  }
}","dynamodb table -> hash_key, name, attribute (name, type), billing mode pay per read"
aws_sagemaker_code_repository,"Create a sagemaker_code_repository to aws_sagemaker_code_repository to ""https://github.com/hashicorp/terraform-provider-aws.git""","package main

import future.keywords.in

default allow = false


aws_sagemaker_code_repository_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_code_repository""
    resource.change.after.git_config[0].repository_url ==  ""https://github.com/hashicorp/terraform-provider-aws.git""
}

# Aggregate all checks
allow {
    aws_sagemaker_code_repository_valid(input.resource_changes)
}",1,"resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""example""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}",
"aws_iam_role, aws_sagemaker_image, aws_iam_policy_document",Create a aws_sagemaker_image,"package main

import future.keywords.in

default allow = false

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_sagemaker_image_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_image""
}


# Aggregate all checks
allow {
    aws_sagemaker_image_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
}",2,"resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}

resource ""aws_sagemaker_image"" ""example"" {
  image_name = ""example""
  role_arn   = aws_iam_role.example.arn
}",
aws_sagemaker_human_task_ui,"Create a SageMaker Human Task UI resource from sagemaker-human-task-ui-template.html containing ""<h1>
    TEST
</h1>""","package main

import future.keywords.in

default allow = false

aws_sagemaker_human_task_ui_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_human_task_ui""
    resource.change.after.ui_template[0].content == ""\u003ch1\u003e\n    TEST\n\u003c/h1\u003e""

}

# Aggregate all checks
allow {
    aws_sagemaker_human_task_ui_valid(input.resource_changes)
}",1,"resource ""aws_sagemaker_human_task_ui"" ""example"" {
  human_task_ui_name = ""example""

  ui_template {
    content = file(""sagemaker-human-task-ui-template.html"")
  }
}",
aws_ami,Create the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",1,"data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any
aws_elasticache_cluster,Provide a resource to reduce the number of database calls.,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name"
aws_elasticache_user,authenticate a elasticache user with redis,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.engine == ""REDIS""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}","requires access string, engine = REDIS, user id and user name"
aws_elasticache_user,authenticate a elasticache user with iam,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""iam""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type = ""iam""
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type = ""iam""}"
aws_elasticache_user,authenticate a elasticache user with passwords,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""password""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type      = ""password""
    passwords = [""password1"", ""password2""]
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type= ""password"", passwords = [""password1"", ""password2""]}"
"aws_elasticache_user, aws_elasticache_user_group",Provides an ElastiCache user group resource with 3 users,"package main

import future.keywords.in

default allow = false

# Check if AWS ElastiCache user group exists
aws_elasticache_user_group_exists(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
    resource.change.actions[_] == ""create""
    count(resource.change.after[user_ids])==3
}

# Check if AWS ElastiCache user exists
aws_elasticache_user_exists(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.actions[_] == ""create""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_exists(input.resource_changes)
    aws_elasticache_user_exists(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}",
aws_dynamodb_contributor_insights,"Enable contributor insights on a table named ""ExampleTableName""","package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Contributor Insights is being created for a specific table
aws_dynamodb_contributor_insights_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_contributor_insights""
}

# Aggregate all checks
allow {
    aws_dynamodb_contributor_insights_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_contributor_insights"" ""test"" {
  table_name = ""ExampleTableName""
}",Create a DynamoDB table and enable contributor insights
aws_dax_parameter_group,Create a DAX parameter group.,"package main

import future.keywords.in

default allow = false

# Check for DAX parameter group creation
dax_parameter_group_created(resources) {
some resource in resources
resource.type == ""aws_dax_parameter_group""
resource.change.actions[_] == ""create""
}

# Allow DAX parameter group creation with specific parameters
allow {
dax_parameter_group_created(input.resource_changes)
}",2,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

resource ""aws_dax_parameter_group"" ""example"" {
name = ""example""

parameters {
name = ""query-ttl-millis""
value = ""100000""
}

parameters {
name = ""record-ttl-millis""
value = ""100000""
}
}","Has one ""aws_dax_parameter_group"""
aws_dynamodb_table,Enable point-in-time recovery for the DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if point-in-time recovery is enabled
point_in_time_recovery_enabled(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.point_in_time_recovery[_].enabled == true
}

# Allow if DynamoDB table is modified and point-in-time recovery is enabled
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
point_in_time_recovery_enabled(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

point_in_time_recovery {
enabled = true # Set to true to enable point-in-time recovery
}

# Define other table settings as needed
}","Has one ""aws_dynamodb_table"" resource
with one ""point_in_time_recovery"" with ""enabled"" = true"
aws_dynamodb_table,Configure a custom Time to Live (TTL) attribute for data expiration.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if TTL is enabled with the correct attribute name
ttl_configured(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.ttl[_].enabled == true
resource.change.after.ttl[_].attribute_name == ""custom_ttl_attribute""
}

# Allow if DynamoDB table is modified and TTL is configured correctly
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
ttl_configured(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

ttl {
attribute_name = ""custom_ttl_attribute"" # Replace with your custom TTL attribute name
enabled = true # Set to true to enable TTL on the table
}
}","Has one ""aws_dynamodb_table"" resource
with one ""ttl""
with one ""attribute_name"" = ""custom_ttl_attribute""
with one ""enabled"" = true"
aws_dynamodb_table,Configure a DynamoDB table with server-side encryption enabled.,"package main

import future.keywords.in

default allow = false

dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

encryption_at_rest_enabled(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.server_side_encryption[_].enabled == true
}

allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
encryption_at_rest_enabled(resource)
}",2,"resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

server_side_encryption {
enabled = true
}
}","Has one ""aws_dynamodb_table"" resource
with one ""server_side_encryption""
with one ""enabled"" = true"
aws_dynamodb_table,Create a DynamoDB table with data encryption at rest enabled for the table.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if stream is enabled with new and old images
stream_configured(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.stream_enabled == true
resource.change.after.stream_view_type == ""NEW_AND_OLD_IMAGES"" 
}

# Allow if DynamoDB table is modified and stream is configured correctly
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
stream_configured(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change this to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

stream_enabled = true # Enable streams
stream_view_type = ""NEW_AND_OLD_IMAGES"" # Choose the appropriate stream view type
}","Has one ""aws_dynamodb_table"" resource
with one ""stream_enabled"" = true
with one ""stream_view_type"" = ""NEW_AND_OLD_IMAGES"""
aws_vpc,Create an AWS VPC resource with an example CIDR block and IPv6 enabled,"package vpc_ipv6
import future.keywords.in

default valid := false

valid {
	some changed_resource in input.resource_changes
	changed_resource.type == ""aws_vpc""
	""create"" in changed_resource.change.actions
	args := changed_resource.change.after
	net.cidr_is_valid(args.cidr_block)
	args.assign_generated_ipv6_cidr_block == true
}
",1,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  assign_generated_ipv6_cidr_block = true
}
","Has one resource ""aws_vpc""        
with one ""cidr_block"" with any valid IPv4 address
with one ""assign_generated_ipv6_cidr_block"" with value ""true"""
"aws_internet_gateway, aws_vpc",Create an AWS VPC resource with an Internet Gateway attached to it,"package vpc_netgate_basic
import future.keywords.in

default valid := false

valid {
	some ig_resource in input.configuration.root_module.resources
	ig_resource.type == ""aws_internet_gateway""

	some vpc_resource in input.configuration.root_module.resources
	vpc_resource.type == ""aws_vpc""
	vpc_resource.address in ig_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_internet_gateway"" ""example_igw"" {
  vpc_id = aws_vpc.main.id
}","Has one resource ""aws_vpc""

Has one resource ""aws_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""
"aws_egress_only_internet_gateway, aws_vpc",Create an AWS VPC with an Egress-Only Internet Gateway attached to it,"package vpc_egress_netgate
import future.keywords.in

default valid := false

valid {
	some eg_ig_resource in input.configuration.root_module.resources
	eg_ig_resource.type == ""aws_egress_only_internet_gateway""

	some vpc_resource in input.configuration.root_module.resources
	vpc_resource.type == ""aws_vpc""
	vpc_resource.address in eg_ig_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
}

resource ""aws_egress_only_internet_gateway"" ""example_egress_igw"" {
  vpc_id = aws_vpc.main.id
}","Has one resource ""aws_vpc""

Has one resource ""aws_egress_only_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""
"aws_subnet, aws_subnet, aws_vpc",Create an AWS VPC resource with one public subnet and one private subnet,"package vpc_two_subnet
import future.keywords.in

default valid := false

valid {
        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""

        some subnet1 in input.configuration.root_module.resources
        subnet1.type == ""aws_subnet""
        vpc_resource.address in subnet1.expressions.vpc_id.references

        some subnet2 in input.configuration.root_module.resources
        subnet2.type == ""aws_subnet""
        vpc_resource.address in subnet2.expressions.vpc_id.references

        not subnet1 == subnet2
}
",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""my_public_subnet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""my_public_subnet""
  }
}

resource ""aws_subnet"" ""my_private_subnet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  tags = {
    Name = ""my_private_subnet""
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has another resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""
"aws_vpc, aws_vpc, aws_vpc_peering_connection",Create two AWS VPCs and establish a peering connection between them,"package vpc_peer_connect
import future.keywords.in

default valid := false        

valid {
        some vpc_resource1 in input.configuration.root_module.resources
        vpc_resource1.type == ""aws_vpc""
    
    some vpc_resource2 in input.configuration.root_module.resources
        vpc_resource2.type == ""aws_vpc""

    not vpc_resource1 == vpc_resource2
    
        some peer_connection_resource in input.configuration.root_module.resources
        peer_connection_resource.type == ""aws_vpc_peering_connection""
    vpc_resource1.address in peer_connection_resource.expressions.vpc_id.references
    vpc_resource2.address in peer_connection_resource.expressions.peer_vpc_id.references
}",2,"resource ""aws_vpc"" ""example_vpc1"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_vpc"" ""example_vpc2"" {
  cidr_block = ""10.1.0.0/16""
}

resource ""aws_vpc_peering_connection"" ""example_peer"" {
  vpc_id        = aws_vpc.example_vpc1.id
  peer_vpc_id   = aws_vpc.example_vpc2.id
  auto_accept   = true
}","Has two resources ""aws_vpc""

Has one resource ""aws_vpc_peering_connection""
with one ""vpc_id"" with value of one aws_vpc
with one ""peer_vpc_id"" with value of the other aws_vpc"
"aws_internet_gateway, aws_route_table, aws_vpc",Create an AWS VPC resource with an Internet Gateway attached and a route to a custom route table directing traffic for a specific CIDR block through the Internet Gateway,"package vpc_gateway_route
import future.keywords.in

default valid := false

valid {
    some route_resource in input.configuration.root_module.resources
        route_resource.type == ""aws_route_table""

        some ig_resource in input.configuration.root_module.resources
        ig_resource.type == ""aws_internet_gateway""   
    ig_resource.address in route_resource.expressions.route.references

        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""
        vpc_resource.address in ig_resource.expressions.vpc_id.references
        vpc_resource.address in route_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_internet_gateway"" ""example_igw"" {
  vpc_id = aws_vpc.main.id
}

resource ""aws_route_table"" ""example_public_rt"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""10.0.1.0/24""
    gateway_id = aws_internet_gateway.example_igw.id
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has one resource ""aws_route_table""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""
with one route"
"aws_neptune_cluster, aws_neptune_cluster_parameter_group",Set up a basic AWS Neptune cluster with a custom parameter group,"package aws_neptune_cluster
import future.keywords.in

default neptune_cluster_valid := false
default cluster_parameter_group_valid := false

neptune_cluster_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""
    cluster_parameter_group_valid
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""

    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references

    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    cluster.expressions.engine_version.constant_value < ""1.2.0.0""
    cluster_parameter_group.expressions.family.constant_value == ""neptune1""   
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""

    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references

    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    cluster.expressions.engine_version.constant_value >= ""1.2.0.0""
    cluster_parameter_group.expressions.family.constant_value == ""neptune1.2""   
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""
    
    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references
    
    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    not cluster.expressions.engine_version.constant_value # defaults as latest version
    cluster_parameter_group.expressions.family.constant_value == ""neptune1.2""   
}",2,"resource ""aws_neptune_cluster_parameter_group"" ""example"" {
  family      = ""neptune1.2""
  name        = ""example""
  description = ""neptune cluster parameter group""

  parameter {
    name  = ""neptune_enable_audit_log""
    value = 1
  }
}

resource ""aws_neptune_cluster"" ""default"" {
  cluster_identifier                  = ""neptune-cluster-demo""
  engine                              = ""neptune""
  backup_retention_period             = 5
  preferred_backup_window             = ""07:00-09:00""
  skip_final_snapshot                 = true
  iam_database_authentication_enabled = true
  apply_immediately                   = true
  neptune_cluster_parameter_group_name = aws_neptune_cluster_parameter_group.example.id
}","Has ""aws_neptune_cluster""
with ""neptune_cluster_parameter_group_name""

Has ""aws_neptune_cluster_parameter_group""
with family = ""neptune1.2"""
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector. Then create an AWS Chime Voice Connector Logging resource with enable_sip_logs disabled and enable_media_metric_logs enabled,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_sip_logs.constant_value == false
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_sip_logs false
with enable_media_metric_logs true"
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector. Then create an AWS Chime Voice Connector Logging resource with logging of ONLY media metrics,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_sip_logs.constant_value == false
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_sip_logs false
with enable_media_metric_logs true"
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector with encryption and log media metrics.,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=true

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_media_metric_logs true"
"aws_chime_voice_connector, aws_chime_voice_connector_streaming","Create an AWS Chime Voice Connector with encryption. Then configure the streaming of the voice connector with retention period of 5 hours, notifications sent to SNS, and disable streaming to Amazon Kinesis.","package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_streaming in input.configuration.root_module.resources
    vc_streaming.type = ""aws_chime_voice_connector_streaming""
    vc_streaming.expressions.data_retention.constant_value == 5
    vc_streaming.expressions.disabled.constant_value == false
    ""SNS"" in vc_streaming.expressions.streaming_notification_targets.constant_value
    vc.address in vc_streaming.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_streaming"" ""default"" {
  disabled = false
  voice_connector_id  = aws_chime_voice_connector.default.id
  data_retention = 5
  streaming_notification_targets = [""SNS""]
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=true

Has one resource ""aws_chime_voice_connector_streaming""
with voice_connector_id equal to the id of the voice connector
with streaming_enabled = false
with data_retention = 5
streaming_notification_targets = [""SNS""]"
"aws_chime_voice_connector, aws_chime_voice_connector_streaming",Create an AWS Chime Voice Connector. Then set up a configuration to stream media to Amazon Kinesis.,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_streaming in input.configuration.root_module.resources
    vc_streaming.type = ""aws_chime_voice_connector_streaming""
    vc.address in vc_streaming.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_streaming"" ""default"" {
  voice_connector_id  = aws_chime_voice_connector.default.id
  data_retention = 5
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_streaming""
with voice_connector_id equal to the id of the voice connector
with data_retention to some number"
"aws_iam_user, aws_iam_user_ssh_key",Create a basic AWS IAM user with a basic SSH key attached.,"package iam_user_ssh
import future.keywords.in

default valid := false

valid {
    some user_ssh_resource in input.configuration.root_module.resources
    user_ssh_resource.type == ""aws_iam_user_ssh_key""

    some user_resource in input.configuration.root_module.resources
    user_resource.type == ""aws_iam_user""
    user_resource.address in user_ssh_resource.expressions.username.references
}",2,"resource ""aws_iam_user"" ""user"" {
  name = ""test-user""
  path = ""/""
}

resource ""aws_iam_user_ssh_key"" ""user"" {
  username   = aws_iam_user.user.name
  encoding   = ""SSH""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 mytest@mydomain.com""
}","Has one ""aws_iam_user"" resource
with one ""name"" with any value

Has one ""aws_iam_user_ssh_key"" resource
with one ""username"" with value ""aws_iam_user.{USER NAME}.name""
with one ""encoding"" with value ""SSH""
with one ""public_key"" with any value"
aws_iam_group,Create a basic AWS IAM group example,"package iam_group_basic
import future.keywords.in

default valid := false

valid {
    some group_resource in input.configuration.root_module.resources
    group_resource.type == ""aws_iam_group""
}",1,"resource ""aws_iam_group"" ""group"" {
  name = ""my-group""
  path = ""/users/""
}","Has one ""aws_iam_group"" resource
with one ""name"" with any value"
aws_iam_user,Create a basic AWS IAM user example,"package iam_user_basic
import future.keywords.in

default valid := false

valid {
	some user_resource in input.configuration.root_module.resources
	user_resource.type == ""aws_iam_user""
}",1,"resource ""aws_iam_user"" ""lb"" {
  name = ""loadbalancer""
  path = ""/system/""

  tags = {
    tag-key = ""tag-value""
  }
}","Has one ""aws_iam_user"" resource
with one ""name"" with any value"
aws_iam_virtual_mfa_device,Create a basic AWS IAM Virtual MFA Device resource,"package iam_mfa_basic
import future.keywords.in

default valid := false

valid {
	some mfa in input.configuration.root_module.resources
	mfa.type == ""aws_iam_virtual_mfa_device""
}",1,"resource ""aws_iam_virtual_mfa_device"" ""example"" {
  virtual_mfa_device_name = ""example""
}","Has one ""aws_iam_virtual_mfa_device"" resource
with one ""virtual_mfa_device_name"" with any string"
aws_s3_bucket,Create a S3 bucket with an example name,"package aws_bucket
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
}",1,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-tf-test-bucket""
}",Has an aws_s3_bucket resource
aws_s3_bucket,Create an AWS resource to help me store images that I want to display on my website,"package aws_s3
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
}",1,"resource ""aws_s3_bucket"" ""example-bucket"" {
  bucket = ""test-bucket""
}",Has an aws_s3_bucket resource
"aws_s3_bucket, aws_s3_bucket_accelerate_configuration","Create S3 bucket with bucket name = ""mybucket"" and attach an acclerate configuration resource for the bucket with enabled status.","package s3_bucket_accelerate
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some accelerate_config in input.configuration.root_module.resources
        accelerate_config.type == ""aws_s3_bucket_accelerate_configuration""
        bucket.address in accelerate_config.expressions.bucket.references
        accelerate_config.expressions.status.constant_value == ""Enabled""
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some accelerate_config in input.configuration.root_module.resources
        accelerate_config.type == ""aws_s3_bucket_accelerate_configuration""
        accelerate_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        accelerate_config.expressions.status.constant_value == ""Enabled""
}
",2,"resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_accelerate_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id
  status = ""Enabled""
}","Has an aws_s3_bucket resource

Has an aws_bucket_accelerate_configuration resource
with bucket = bucked id OR name
with status = ""Enabled"" (case sensitive)"
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_ownership_controls","Create S3 bucket with bucket name = ""mybucket"" and set the ownership control of the S3 bucket to be bucket owner preferred.
Then create an ACL resource that depends on ownership control and makes bucket private. Use bucket references, NOT the name of the bucket.","package s3_bucket_acl
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    bucket.address in ownership_controls.expressions.bucket.references
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""

    some acl in input.configuration.root_module.resources
    acl.type == ""aws_s3_bucket_acl""
    bucket.address in acl.expressions.bucket.references
    acl.expressions.acl.constant_value == ""private""
    ownership_controls.address in acl.depends_on
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]

  bucket = aws_s3_bucket.example.id
  acl    = ""private""
}
","Has an aws_s3_bucket resource

Has an aws_s3_bucket_ownership_controls
with bucket = bucket id
and rule with object_ownership = ""BucketOwnerPreferred""

Has an aws_s3_bucket_acl"
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_analytics_configuration","Create S3 bucket with bucket name = ""mybucket"". Add analytics configuration for entire S3 bucket and export results to a second S3 bucket. Use bucket references, NOT the names of the buckets.","package s3_bucket_analytics
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some other_bucket in input.configuration.root_module.resources
        other_bucket.type == ""aws_s3_bucket""
        not bucket == other_bucket

        some analytics in input.configuration.root_module.resources
        analytics.type == ""aws_s3_bucket_analytics_configuration""
        bucket.address in analytics.expressions.bucket.references
    some export in analytics.expressions.storage_class_analysis
    some dest in export.data_export
    some bucket_dest in dest.destination
    some arn in bucket_dest.s3_bucket_destination
    other_bucket.address in arn.bucket_arn.references
}",2,"resource ""aws_s3_bucket_analytics_configuration"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""

  storage_class_analysis {
    data_export {
      destination {
        s3_bucket_destination {
          bucket_arn = aws_s3_bucket.analytics.arn
        }
      }
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""analytics"" {
  bucket = ""analytics destination""
}","Has an aws_s3_bucket resource

Has an aws_bucket_analytics_configuration resource
with bucket = bucked id
and storage_class_analysis with (refer to desired output)"
"aws_s3_bucket, aws_s3_bucket_intelligent_tiering_configuration","Create S3 bucket with bucket name = ""mybucket"". Add intelligent tiering configuration resource for the bucket.","package s3_bucket_intelligent_tiering
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some intelligent_tiering in input.configuration.root_module.resources
    intelligent_tiering.type == ""aws_s3_bucket_intelligent_tiering_configuration""
    bucket.address in intelligent_tiering.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some intelligent_tiering in input.configuration.root_module.resources
    intelligent_tiering.type == ""aws_s3_bucket_intelligent_tiering_configuration""
    intelligent_tiering.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket_intelligent_tiering_configuration"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""

  tiering {
    access_tier = ""DEEP_ARCHIVE_ACCESS""
    days        = 180
  }
  tiering {
    access_tier = ""ARCHIVE_ACCESS""
    days        = 125
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_intelligent_tiering
with bucket = bucked id OR name
and correct tiering attributes"
"aws_s3_bucket, aws_s3_bucket_metric","Create a S3 bucket with bucket name = ""mybucket"" and a bucket metric resource that adds metrics configuration for the entire bucket.","package s3_bucket_metric
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    bucket.address in metrics.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    metrics.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_metric"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_metric
with bucket = bucked id OR name"
"aws_s3_bucket, aws_s3_bucket_notification, aws_sns_topic",Create an AWS S3 bucket and send a SNS notification whenever a .log object is created in the bucket. Do not include policies required for this. ,"package aws_s3_bucket_notification
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    
    some sns_topic in input.configuration.root_module.resources
    sns_topic.type == ""aws_sns_topic""

    some notification in input.configuration.root_module.resources
    notification.type = ""aws_s3_bucket_notification""
    bucket.address in notification.expressions.bucket.references
    some topic in notification.expressions.topic
    some event in topic.events.constant_value
    event == ""s3:ObjectCreated:*""
    topic.filter_suffix.constant_value == "".log""
    sns_topic.address in topic.topic_arn.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some sns_topic in input.configuration.root_module.resources
    sns_topic.type == ""aws_sns_topic""

    some notification in input.configuration.root_module.resources
    notification.type = ""aws_s3_bucket_notification""
    bucket.expressions.bucket.constant_value == notification.expressions.bucket.constant_value
    some topic in notification.expressions.topic
    some event in topic.events.constant_value
    event == ""s3:ObjectCreated:*""
    topic.filter_suffix.constant_value == "".log""
    sns_topic.address in topic.topic_arn.references
}",2,"resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
}

resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_sns_topic""

Has one resource ""aws_s3_bucket_notification""
with bucket = bucket id OR name
with topic
    with topic arn = sns topic arn 
    and events = [""s3:ObjectCreated:*""]
    and filter_suffix = "".log"""
"aws_s3_bucket, aws_s3_bucket_object","Create S3 bucket with bucket name = ""mybucket"". Then create a bucket object resource for a file at source location ""path/to/file"" and upload the object to the bucket.","package s3_bucket_object
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_bucket_object""
        bucket.address in object.expressions.bucket.references
        object.expressions.key
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_bucket_object""
        object.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        object.expressions.key
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}
resource ""aws_s3_bucket_object"" ""object"" {
  bucket = ""your_bucket_name""
  key    = ""new_object_key""
  source = ""path/to/file""
}","Has an aws_s3_bucket resource

Has an aws_s3_object resource
with bucket = bucket id OR name
with source = ""path/to/file"""
"aws_s3_bucket, aws_s3_object","I have a PDF with path ""assets/test.pdf"". Make an AWS resource to store this PDF and upload it.","package aws_s3_object
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_object""
        object.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        object.expressions.source.constant_value == ""assets/test.pdf""
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_object""
        bucket.address in object.expressions.bucket.references
        object.expressions.source.constant_value == ""assets/test.pdf""
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""test-bucket""
}

resource ""aws_s3_object"" ""object"" {
  bucket = ""my-bucket""
  key    = ""new_object_key""
  source = ""assets/test.pdf""
}","Has an aws_s3_bucket resource

Has an aws_s3_object resource
with bucket = bucket id OR name
with source = ""assets/test.pdf"""
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration","Create a S3 bucket with bucket name = ""mybucket"" and object lock enabled. Then, add an object lock configuration resource for the new bucket.","package s3_bucket_object_lock
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""
        bucket.expressions.object_lock_enabled.constant_value == true

        some object_lock in input.configuration.root_module.resources
        object_lock.type == ""aws_s3_bucket_object_lock_configuration""
        bucket.address in object_lock.expressions.bucket.references
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""
        bucket.expressions.object_lock_enabled.constant_value == true

        some object_lock in input.configuration.root_module.resources
        object_lock.type == ""aws_s3_bucket_object_lock_configuration""
        object_lock.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""

  object_lock_enabled = true
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    default_retention {
      mode = ""COMPLIANCE""
      days = 5
    }
  }
}","Has an aws_s3_bucket resource
with object_lock_enabled = true

Has an aws_s3_object_lock resource
with bucket = bucket id OR name
and a correct rule"
"aws_s3_bucket, aws_s3_bucket_ownership_controls","Create a S3 bucket with bucket name = ""mybucket"". And create a bucket ownership controls resource to set object ownership to bucket owner preferred.","package s3_bucket_ownership_controls
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""

    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    bucket.address in ownership_controls.expressions.bucket.references
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""

    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    ownership_controls.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_ownership_controls
with bucket = bucket id OR name
and rule with object_ownership = ""BucketOwnerPreferred"""
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration",Create an AWS S3 bucket and apply KMS server side encryption that uses a defined KMS key resource.,"package aws_s3_bucket_sse
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.address in sse.expressions.bucket.references
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.expressions.bucket.constant_value == sse.expressions.bucket.constant_value
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
    
}",2,"resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_kms_key""

Has one resource ""aws_s3_bucket_server_side_encryption""
with bucket = bucket id OR name
with rule
    with apply_server_side_encryption_by_default
        with kms_master_key_id = kms key id
        and sse_algorithm = ""aws:kms"""
"aws_s3_bucket, aws_s3_bucket_cors_configuration",Create a S3 bucket and a bucket CORS configuration resource that attaches an example CORS rule to the bucket. Make sure the CORS rule has all of the required attributes.,"package aws_s3_bucket_cors
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
	some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

	some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    
    bucket.address in cors_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
	some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

	some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    
	cors_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_cors_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  cors_rule {
    allowed_headers = [""*""]
    allowed_methods = [""PUT"", ""POST""]
    allowed_origins = [""https://s3-website-test.hashicorp.com""]
    expose_headers  = [""ETag""]
    max_age_seconds = 3000
  }

  cors_rule {
    allowed_methods = [""GET""]
    allowed_origins = [""*""]
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_cors_configuration"
"aws_s3_bucket, aws_s3_bucket_cors_configuration","Create a S3 bucket with a CORS configuration that allows POSTs and GETs from my website ""https://domain.com"".","package aws_s3_bucket_cors
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    some rule in cors_config.expressions.cors_rule
    some method1 in rule.allowed_methods.constant_value
    method1 == ""POST""
    some method2 in rule.allowed_methods.constant_value
    method2 == ""GET""
    some origin in rule.allowed_origins.constant_value
    origin == ""https://domain.com""
    
    bucket.address in cors_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    some rule in cors_config.expressions.cors_rule
    some method1 in rule.allowed_methods.constant_value
    method1 == ""POST""
    some method2 in rule.allowed_methods.constant_value
    method2 == ""GET""
    some origin in rule.allowed_origins.constant_value
    origin == ""https://domain.com""
    
    cors_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_cors_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  cors_rule {
    allowed_headers = [""*""]
    allowed_methods = [""GET"", ""POST""]
    allowed_origins = [""https://domain.com""]
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_cors_configuration
with bucket = bucket id OR name
with cors rule {
    allowed_headers = [""*""]
    allowed_methods = [""GET"", ""POST""]
    allowed_origins = [""https://domain.com""]
  }"
"aws_s3_bucket, aws_s3_bucket_website_configuration",Create a S3 bucket and an example website configuration resource for the S3 bucket.,"package aws_s3_bucket_website
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""

    bucket.address in website.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""

    website.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}
",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
}","Has an aws_s3_bucket

Has an aws_s3_bucket_website_configuration
with bucket = bucket id OR name"
"aws_s3_bucket, aws_s3_bucket_website_configuration","Create a S3 bucket and host a static website. The website should use ""index.html"" in my bucket as the index page.","package aws_s3_bucket_website
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""
    some index in website.expressions.index_document
    index.suffix.constant_value == ""index.html""

    bucket.address in website.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""
    some index in website.expressions.index_document
    index.suffix.constant_value == ""index.html""

    website.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}
",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  index_document {
    suffix = ""index.html""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_website_configuration
with bucket = bucket id OR name
with index_document = { suffix = ""index.html"" }"
"aws_s3_bucket, aws_s3_bucket_public_access_block",Create a S3 bucket and an example public access block resource for the S3 bucket.,"package aws_s3_bucket_public_access_block

import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some public_access in input.configuration.root_module.resources
        public_access.type == ""aws_s3_bucket_public_access_block""

        bucket.address in public_access.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some public_access in input.configuration.root_module.resources
        public_access.type == ""aws_s3_bucket_public_access_block""

        public_access.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = aws_s3_bucket.example.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}","Has an aws_s3_bucket

Has an aws_s3_bucket_public_access
with bucket = bucket id OR name"
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name"
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket with the bucket owner paying for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner"""
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket where the bucket owner pays for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner"""
"aws_s3_bucket, aws_s3_bucket_versioning",Create a S3 bucket and an example versioning resource for the S3 bucket.,"package aws_s3_bucket_versioning
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""

    bucket.address in versioning.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""
    
    versioning.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_versioning"" ""versioning_example"" {
  bucket = aws_s3_bucket.example.id
  versioning_configuration {
    status = ""Enabled""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_versioning
with bucket = bucket id OR name"
"aws_s3_bucket, aws_s3_bucket_versioning",Create a S3 bucket with versioning disabled.,"package aws_s3_bucket_versioning
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""

    bucket.address in versioning.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""
    
    versioning.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_versioning"" ""versioning_example"" {
  bucket = aws_s3_bucket.example.id
  versioning_configuration {
    status = ""Disabled""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_versioning
with bucket = bucket id OR name
with versioning_configuration = { status = ""Disabled"" } "
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_logging","Create a S3 bucket and a second S3 bucket. Then create an example logging resource for the first S3 bucket that stores logs in the second bucket. Make sure the log object keys have a prefix of ""log/"".","package s3_bucket_logging
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""log_bucket"" {
  bucket = ""mylogbucket""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket = aws_s3_bucket.example.id

  target_bucket = aws_s3_bucket.log_bucket.id
  target_prefix = ""log/""
}","Has two aws_s3_bucket

Has an aws_s3_bucket_logging
with bucket = bucket id OR name
with target_bucket = second bucket id OR name
with target_prefix = ""log/"""
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_logging",Create a S3 bucket and a configuration that stores server access logs into a second S3 bucket.,"package s3_bucket_logging
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""log_bucket"" {
  bucket = ""mylogbucket""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket = aws_s3_bucket.example.id

  target_bucket = aws_s3_bucket.log_bucket.id
  target_prefix = ""log/""
}","Has two aws_s3_bucket

Has an aws_s3_bucket_logging
with bucket = bucket id OR name
with target_bucket = second bucket id OR name
with target_prefix = ""log/"""
aws_lightsail_instance,generate Basic Amazon Lightsail,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument"
aws_lightsail_instance,generate Basic Amazon Lightsail with user Data,"package aws_lightsail_instance

# aws_lightsail_instance attributes - optional

default lightsail_instance_valid := false

lightsail_instance_valid {
	instance := input.configuration.root_module.resources[_]
	instance.type == ""aws_lightsail_instance""
    expressions := instance.expressions

    # Validate the presence of required arguments
    expressions.name
    expressions.availability_zone
    expressions.blueprint_id
    expressions.bundle_id
    expressions.user_data
}

# You can add more specific validations here if needed.
# For example, if you want to check if the name follows a specific format,
# you could write a rule like this:

name_is_valid {
	instance := input.configuration.root_module.resources[_]
	instance.type == ""aws_lightsail_instance""
	regex.match(""^[a-zA-Z0-9-_]+$"", instance.expressions.name.constant_value)
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
  user_data         = ""sudo yum install -y httpd && sudo systemctl start httpd && sudo systemctl enable httpd && echo '<h1>Deployed via Terraform</h1>' | sudo tee /var/www/html/index.html""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with ""user_data"" argument"
aws_lightsail_instance,generate aws lightsail with auto snapshots enabled,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
    has_valid_add_on(resource.values.add_on)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}

has_valid_add_on(add_ons) {
    add_on := add_ons[_]
    add_on.type == ""AutoSnapshot""
    add_on.snapshot_time
    add_on.status == ""Enabled""
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""custom_instance""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
  add_on {
    type          = ""AutoSnapshot""
    snapshot_time = ""06:00""
    status        = ""Enabled""
  }
  tags = {
    foo = ""bar""
  }
}","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" argument
with ""bundle_id"" argument
with ""add_on"" argument
    with type = ""AutoSnapshot""
    with snapshot_time argument
    with status = ""Enabled"""
aws_lightsail_instance,create AWS Lightsail with default blueprint,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""gitlab_test"" {
  name              = ""custom_gitlab""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" argument
with ""bundle_id"" argument"
aws_lightsail_instance,create AWS Lightsail with WordPress blueprint,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id == ""wordpress""
    resource.values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
 }","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" = wordpress
with ""bundle_id"" argument"
"aws_lightsail_instance, aws_lightsail_static_ip, aws_lightsail_static_ip_attachment",create Amazon Lightsail with static ipv4 IP,"package terraform.validation

default has_required_resources = false

# Rule for aws_lightsail_instance resource
has_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
}

# Rule for aws_lightsail_static_ip resource
has_lightsail_static_ip {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_static_ip""
    resource.values.name
}

# Rule for aws_lightsail_static_ip_attachment resource considering expressions
has_lightsail_static_ip_attachment {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_lightsail_static_ip_attachment""
    static_ip_name_exists(resource)
    instance_name_exists(resource)
}

static_ip_name_exists(resource) {
    resource.expressions.static_ip_name.references[_]
}

instance_name_exists(resource) {
    resource.expressions.instance_name.references[_]
}

# Combined rule to ensure all conditions are met
has_required_resources {
    has_lightsail_instance
    has_lightsail_static_ip
    has_lightsail_static_ip_attachment
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_static_ip_attachment"" ""test"" {
  static_ip_name = aws_lightsail_static_ip.test.id
  instance_name  = aws_lightsail_instance.test.id
}

resource ""aws_lightsail_static_ip"" ""test"" {
  name = ""example""
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""custom_gitlab""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument

has one ""aws_lightsail_static_ip"" resource
    with ""name"" argument

has one ""aws_lightsail_static_ip_attachment"" resource
    with ""static_ip_name"" argument
    with ""instance_name"" argument"
aws_lightsail_instance,create Amazon Lightsail with dualstack IP,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance with specific arguments
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
    resource.values.ip_address_type == ""dualstack""
}",1,"resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
  ip_address_type = ""dualstack""
 }","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with ""ip_address_type"" = ""dualstack"""
"aws_lightsail_instance, aws_lightsail_key_pair",create Amazon Lightsail with a separate SSH key,"package terraform.validation

default has_all_required_resources = false

# Rule for aws_lightsail_instance resource with specific arguments
has_valid_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
    resource.values.key_pair_name
}

# Rule for aws_lightsail_key_pair resource with specific arguments
has_valid_lightsail_key_pair {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_key_pair""
    resource.values.name
    resource.values.public_key
}

# Combined rule to ensure both resources meet their respective conditions
has_all_required_resources {
    has_valid_lightsail_instance
    has_valid_lightsail_key_pair
}",2,"resource ""aws_lightsail_key_pair"" ""lg_key_pair"" {
  name       = ""importing""
  public_key = file(""~/.ssh/id_rsa.pub"")
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
  key_pair_name = aws_lightsail_key_pair.lg_key_pair.name
 }","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with key_pair_name argument

have one ""aws_lightsail_key_pair"" resource
    with ""name"" argument
    with ""public_key""argument"
aws_lightsail_database,create AWS Lightsail that creates a managed database,"package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id
    resource.values.bundle_id
}",1,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""mysql_8_0""
  bundle_id                = ""micro_1_0""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    "
aws_lightsail_database,create an AWS Lightsail instance that creates a mysql database,"package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""mysql_8_0""
    resource.values.bundle_id
}",1,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""mysql_8_0""
  bundle_id                = ""micro_1_0""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = mysql_8_0
    with bundle_id
    "
aws_lightsail_database,"create AWS Lightsail that creates a mysql database. It should allow daily backups to take place between 16:00 and 16:30 each day and  requires any maintiance tasks (anything that would cause an outage, including changing some attributes) to take place on Tuesdays between 17:00 and 17:30","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.bundle_id
    resource.values.preferred_backup_window == ""16:00-16:30""
    resource.values.preferred_maintenance_window == ""Tue:17:00-Tue:17:30""
    
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""mysql_8_0""
  bundle_id                    = ""micro_1_0""
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    with  preferred_backup_window    = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""

    "
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name"
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable applying changes immediately instead of waiting for a maintiance window","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.apply_immediately == true
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""postgres_12""
  bundle_id                = ""micro_1_0""
  apply_immediately        = true
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    apply_immediately  = true"
"aws_lightsail_disk, aws_availability_zones",create a Lightsail Disk resource,"package terraform.validation

default has_valid_lightsail_disk = false

# Rule for aws_lightsail_disk resource with specific arguments
has_valid_lightsail_disk {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_disk""
    resource.values.name
    resource.values.size_in_gb
    resource.values.availability_zone
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

resource ""aws_lightsail_disk"" ""test"" {
  name              = ""test""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}","have one aws_lightsail_disk resource
    with name
    with size_in_gb
    with availability_zone"
aws_lightsail_certificate,Provides a lightsail certificate.,"package terraform.validation

default has_valid_lightsail_certificate = false

# Rule for aws_lightsail_certificate resource with specific arguments
has_valid_lightsail_certificate {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_certificate""
    resource.values.name
    resource.values.domain_name
}",1,"resource ""aws_lightsail_certificate"" ""test"" {
  name                      = ""test""
  domain_name               = ""testdomain.com""
  subject_alternative_names = [""www.testdomain.com""]
}","have one aws_lightsail_certificate
    with name
    with domain_name"
aws_glacier_vault,an S3 Glacier vault for long-term data archiving,"package terraform.validation

default is_valid_glacier_vault = false

# Rule to validate if there is at least one aws_glacier_vault resource with a name attribute
is_valid_glacier_vault {
    # Find a resource that is of type aws_glacier_vault
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""

    # Check that the resource has a name attribute
    has_value(resource.values.name)
}

# Helper function to check if a value exists (not null)
has_value(value) {
    not is_null(value)
}",1,"provider ""aws"" {
  region = ""us-east-1""  # Replace with your desired AWS region
}

resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute"
aws_glacier_vault,generate a aws storage for long term data and backup,"package terraform.validation

default is_valid_glacier_vault = false

# Rule to validate if there is at least one aws_glacier_vault resource with a name attribute
is_valid_glacier_vault {
    # Find a resource that is of type aws_glacier_vault
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""

    # Check that the resource has a name attribute
    has_value(resource.values.name)
}

# Helper function to check if a value exists (not null)
has_value(value) {
    not is_null(value)
}",1,"provider ""aws"" {
  region = ""us-east-1""  # Replace with your desired AWS region
}

resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute"
"aws_glacier_vault, aws_sns_topic",generage an S3 Glacier vault that pushes notification when archive retrieval completed,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'notification' attributes
is_valid_glacier_vault {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""
    not is_null(resource.values.name)
    not is_null(resource.values.notification)
}

# Rule to check for the existence of an AWS SNS Topic resource with 'name' attribute
is_valid_sns_topic {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_sns_topic""
    not is_null(resource.values.name)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
    is_valid_glacier_vault
    is_valid_sns_topic
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

# Create an SNS topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-archive-retrieval-notifications""
}

# Create an S3 Glacier Vault
resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted""]
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with notification attribute
        with sns_topic attribute
        with events attribute

has one aws_sns_topic resources
    with name attribute"
aws_db_instance,create a basic AWS RDS instance,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  db_name              = ""mydb""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""
aws_db_instance,create a basic AWS SQL database,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine.constant_value == ""mysql""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""
aws_db_instance,Create a db.t4g.large AWS PostgresSQL,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage.constant_value >= 20
        expressions.engine.constant_value == ""postgres""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class.constant_value == ""db.t4g.large""
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""postgres""
  instance_class       = ""db.t4g.large""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage"" minimun value is 20
with one ""engine"" with value ""postgres""
with one ""instance_class"" with value ""db.t4g.large""
with one ""password""
with one ""username"""
aws_db_instance,create a memory optimized PostgresSQL,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage.constant_value >= 20
        expressions.engine.constant_value == ""postgres""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""postgres""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage"" minimun value is 20
with one ""engine"" with value ""postgres""
with one ""instance_class"" with valid memory-optimized class
with one ""password""
with one ""username"""
aws_db_instance,Create an AWS mySQL instance. Any changes on it will be applied immediately,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.apply_immediately.constant_value == true
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  apply_immediately = true
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" with value ""mysql""
with one ""instance_class"" 
with one ""password""
with one ""username""
with one ""apply_immediately"" set to true"
aws_db_instance,Create an AWS mySQL instance that skips the final snapshot,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.skip_final_snapshot.constant_value == true
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
        expressions.snapshot_identifier
}

requirement1(expressions) {
        expressions.replicate_source_db
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  skip_final_snapshot = true
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" with value ""mysql""
with one ""instance_class"" 
with one ""password""
with one ""username""
with one ""skip_final_snapshot"" set to true"
"aws_db_instance, aws_db_instance","create an aws sql, and make a replica of it","package terraform.validation

default is_valid_db_instance = false

has_valid_replica {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
    resource.expressions.instance_class
    resource.expressions.replicate_source_db
}

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
    has_valid_replica
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}

resource ""aws_db_instance"" ""replica"" {
  replicate_source_db = aws_db_instance.default.arn
  instance_class       = ""db.z1d.micro""
  password             = ""1234567""
}","Has two ""aws_db_instance"" resources

resource 1:
with one ""allocated_storage""
with one ""engine"" and a valid engine value
with one ""instance_class"" and a valid instance class type
with one ""password""
with one ""username""

resource 2:
with one ""instance_class""
with one ""replicate_source_db"" = ""aws_db_instance.resource1.identifier"""
aws_db_instance,create a aws relational database from a snapshot,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}


requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""replica"" {
  snapshot_identifier = ""your identifier""
  instance_class       = ""db.z1d.micro""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""
aws_db_instance,"create a basic AWS RDS instance, with gp3 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""gp3""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 20
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""gp3""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""
aws_db_instance,"create a basic AWS RDS instance, with io1 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""io1""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 100
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 100
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""io1""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""
aws_db_instance,create an aws database restored from s3,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.s3_import
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  s3_import {
    bucket_name           = ""mybucket""
    ingestion_role        = ""arn:aws:iam::1234567890:role/role-xtrabackup-rds-restore""
    source_engine         = ""mysql""
    source_engine_version = ""5.6""
  }
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one s3_import
        with one bucket_name
        with one ingestion_role
        with one source_engine
        with one source_engine_verison"
aws_db_instance,create an AWS database that enables storage autoscaling,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.max_allocated_storage.constant_value >  resource.expressions.allocated_storage.constant_value
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  max_allocated_storage = 100
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one max_allocated_storage"
aws_db_instance,"create an aws database that Managed Master Passwords via Secrets Manager, default KMS Key","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}


requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  manage_master_user_password = true
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""username""
    with one manage_master_user_password"
"aws_db_instance, aws_kms_key","create an aws database that Managed Master Passwords via Secrets Manager, with specific KMS Key","package terraform.validation

default is_valid_db_instance = false
default is_valid_kms_key = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

is_valid_kms_key {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_kms_key""
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  manage_master_user_password = true
  master_user_secret_kms_key_id = aws_kms_key.example.key_id
}

resource ""aws_kms_key"" ""example"" {
  description = ""Example KMS Key""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""username""
    with one manage_master_user_password
    with one ""master_user_secret_kms_key_id""  
 
has one ""aws_kms_key"""
"aws_db_instance, aws_db_snapshot",Create an AWS mySQL instance and a snapshot of the instance,"package terraform.validation

default is_valid_db_instance = false

default is_valid_db_snapshot = false

is_valid_db_snapshot {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_snapshot""
    resource.expressions.db_instance_identifier
    resource.expressions.db_snapshot_identifier
}

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}

resource ""aws_db_snapshot"" ""test"" {
  db_instance_identifier = aws_db_instance.default.identifier
  db_snapshot_identifier = ""testsnapshot1234""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" and a valid engine value
with one ""instance_class"" and a valid instance class type
with one ""password""
with one ""username""

Has one ""aws_db_snapshot"" instance
with one ""db_instance_identifier""
with one ""db_snapshot_identifier"""
aws_efs_file_system,create a AWS EFS File System with tags,"package terraform.validation

default is_valid_efs_file_system = false

# Rule to check if a valid aws_efs_file_system exists
is_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
    resource.expressions.tags
}
",1,"resource ""aws_efs_file_system"" ""foo"" {
  creation_token = ""my-product""

  tags = {
    Name = ""MyProduct""
  }
}","Have one ""aws_efs_file_system"" resource
    with one tags"
aws_efs_file_system,create a AWS EFS Using lifecycle policy,"package terraform.validation

default is_valid_efs_file_system = false

# Rule to check if a valid aws_efs_file_system exists
is_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
    resource.expressions.lifecycle_policy
}",1,"resource ""aws_efs_file_system"" ""foo_with_lifecyle_policy"" {
  creation_token = ""my-product""

  lifecycle_policy {
    transition_to_ia = ""AFTER_30_DAYS""
  }
}","Have one ""aws_efs_file_system"" resource"
"aws_efs_backup_policy, aws_efs_file_system",create a AWS EFS with automatic backups enabled,"package terraform.validation

default is_valid_efs_setup = false

# Rule to check if a valid aws_efs_file_system and aws_efs_file_system_policy exists
is_valid_efs_setup {
    has_valid_efs_file_system
    has_valid_aws_efs_backup_policy
}

# Helper rule to check if a valid aws_efs_file_system exists
has_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
}

# Helper rule to check if a valid aws_efs_file_system_policy exists with required arguments
has_valid_aws_efs_backup_policy {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_backup_policy""
    resource.expressions.backup_policy
    resource.expressions.file_system_id
}
",2,"resource ""aws_efs_file_system"" ""fs"" {
  creation_token = ""my-product""
}

resource ""aws_efs_backup_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.fs.id

  backup_policy {
    status = ""ENABLED""
  }
}","Have one ""aws_efs_file_system"" resource

Have one ""aws_efs_backup_policy"" resource
with one ""file_system_id""
with one ""backup_policy"""
aws_egress_only_internet_gateway,"creates an egress-only internet gateway named ""pike"" associated with a specified VPC, allowing IPv6-enabled instances to connect to the internet without allowing inbound internet traffic, and tags it with ""permissions"".","package terraform.validation

# Set default validation state for aws_egress_only_internet_gateway
default is_valid_egress_only_internet_gateway = false

# Validate aws_egress_only_internet_gateway resource
is_valid_egress_only_internet_gateway {
	some i
	resource := input.configuration.root_module.resources[i]
	resource.type == ""aws_egress_only_internet_gateway""
	resource.expressions.vpc_id.constant_value != null
	resource.expressions.tags.constant_value != null # Verifies it is tagged with ""permissions""
}",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_egress_only_internet_gateway"" ""pike"" {
  vpc_id = ""vpc-0c33dc8cd64f408c4""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_egress_only_internet_gateway"" resource:
Associated with a specified ""aws_vpc"" resource
Designed to allow IPv6-enabled instances within the VPC to connect to the internet while preventing inbound internet traffic
Tagged with ""permissions"" to categorize or specify its role or access levels within the infrastructure"
aws_egress_only_internet_gateway,"creates an egress-only internet gateway associated with a specified VPC, allowing IPv6-enabled instances to connect to the internet without allowing inbound internet traffic","package terraform.validation

# Set default validation state for aws_egress_only_internet_gateway
default is_valid_egress_only_internet_gateway = false

# Validate aws_egress_only_internet_gateway resource
is_valid_egress_only_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_egress_only_internet_gateway""
        resource.expressions.vpc_id.constant_value != null
        
}",1,"resource ""aws_egress_only_internet_gateway"" ""pike"" {
  vpc_id = ""vpc-0c33dc8cd64f408c4""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_egress_only_internet_gateway"" resource:
Associated with a specified ""aws_vpc"" resource
Designed to allow IPv6-enabled instances within the VPC to connect to the internet while preventing inbound internet traffic
"
aws_nat_gateway,creates a NAT Gateway associated with a specified subnet and Elastic IP allocation ID. The NAT Gateway is configured for public connectivity.,"package terraform.validation

# Set default validation state
default is_valid_nat_gateway = false

# Validate aws_nat_gateway resource
is_valid_nat_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_nat_gateway""

        # Ensure it is associated with a specified subnet
        resource.expressions.subnet_id != null

        # Ensure it uses a specific Elastic IP allocation ID
        resource.expressions.allocation_id != null
}
",2,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
}","Has one ""aws_nat_gateway"" resource:
Associated with a specified ""aws_subnet"" resource for hosting the NAT Gateway.
Utilizes an ""Elastic IP allocation ID"" to provide the NAT Gateway with a public IP address.
Configured for public connectivity, allowing resources within the private subnet to access the internet securely."
