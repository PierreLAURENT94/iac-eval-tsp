Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0
"aws_route53_record, aws_route53_zone",Set up a record that maps a domain name to an IPv4 address using Route 53 resources,"package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""A""
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""A""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example"" {
  name = ""example.com""
}

resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.example.zone_id
  name    = ""www""
  type    = ""A""
  ttl     = 300
  records = [""192.168.1.1""]
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_r53_record': False, 'is_valid_r53_zone': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_route53_record, aws_route53_zone",Set up a record that maps a domain name to an IPv6 address using Route 53 resources,"package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""AAAA""
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""AAAA""
  ttl     = ""300""
  records = [""2001:0db8:85a3:0000:0000:8a2e:0370:7334""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""AAAA""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example"" {
  name = ""example.com""
}

resource ""aws_route53_record"" ""ipv6"" {
  zone_id = aws_route53_zone.example.id
  name    = ""example.com""
  type    = ""AAAA""
  ttl     = 300
  records = [""2001:db8::1""]
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_r53_record': False, 'is_valid_r53_zone': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_route53_record, aws_route53_zone","Set up a Pointer record for reverse DNS using Route 53 resources. The domain name should be ""host.example53.com"" and name the zone ""reverse_zone""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""2.0.192.in-addr.arpa""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""PTR""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""host.example.com""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.reverse_zone.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

# Create Reverse DNS Hosted Zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""2.0.192.in-addr.arpa""
}

# Create a PTR Record for a specific IP address within that zone
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""53.2.0.192.in-addr.arpa""
  type    = ""PTR""
  ttl     = ""3600""
  records = [""host.example.com""]
}","Has one ""aws_route53_zone"" resource
    with ""name"" ending in "".in-addr.arpa""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""PTR""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""host.example53.com""
}

resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""10.0.0.1""
  type    = ""PTR""
  ttl     = 3600
  records = [""host.example53.com.""]
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_r53_record': False, 'is_valid_r53_zone': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_route53_record, aws_route53_zone","Set up a TXT recordfor domain ownership verification purposes using Route 53 resources. The verification string should be ""passwordpassword"" and the name of the zone should be ""example""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""TXT""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""passwordpassword""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example"" {
  name = ""example53.com""
}

# TXT Record for domain verification or other purposes
resource ""aws_route53_record"" ""example_txt"" {
  zone_id = aws_route53_zone.example.zone_id
  name    = ""sub.example53.com""
  type    = ""TXT""
  ttl     = ""300""
  records = [""passwordpassword""] 
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""TXT""
    with ""ttl""
    with ""records"" set to a string
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example"" {
  name = ""example""
}

resource ""aws_route53_record"" ""domain_verification"" {
  zone_id = aws_route53_zone.example.zone_id
  name    = ""_amazonses""
  type    = ""TXT""
  ttl     = 60
  records = [""passwordpassword""]
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_r53_record': True, 'is_valid_r53_zone': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_configuration_template",Create a template of an elastic beanstalk application,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false

default is_valid_eb_app_template = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}



is_valid_eb_app_template {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_configuration_template""
    resource.expressions.name
    resource.expressions.solution_stack_name
    startswith(resource.expressions.application.references[0], ""aws_elastic_beanstalk_application"")
}


is_configuration_valid {
    is_valid_eb_app
    is_valid_eb_app_template
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}

resource ""aws_elastic_beanstalk_configuration_template"" ""tf_template"" {
  name                = ""tf-test-template-config""
  application         = aws_elastic_beanstalk_application.tftest.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.0.9 running Python 3.11""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_configuration_template"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name"" "," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elastic_beanstalk_application"" ""example"" {
  name        = ""terraform-example-app""
  description = ""Terraform example of an Elastic Beanstalk application""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_eb_app': True, 'is_valid_eb_app_template': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_configuration_template",Create a template of an elastic beanstalk application that is running a version of Go,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false

default is_valid_eb_app_template = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}



is_valid_eb_app_template {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_configuration_template""
    resource.expressions.name
    endswith(resource.expressions.solution_stack_name.constant_value, ""Go 1"")
    startswith(resource.expressions.application.references[0], ""aws_elastic_beanstalk_application"")
}


is_configuration_valid {
    is_valid_eb_app
    is_valid_eb_app_template
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}

resource ""aws_elastic_beanstalk_configuration_template"" ""tf_template"" {
  name                = ""tf-test-template-config""
  application         = aws_elastic_beanstalk_application.tftest.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.10.1 running Go 1""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_configuration_template"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name"" that references a valid ""Go"" version"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elastic_beanstalk_application"" ""go_app"" {
  name        = ""go-app""
  description = ""An application running a version of Go""
}

resource ""aws_iam_role"" ""aws_elastic_beanstalk_service_role"" {
  name = ""aws_elastic_beanstalk_service_role""

  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""elasticbeanstalk.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""aws_elastic_beanstalk_service_role_policy"" {
  name = ""aws_elastic_beanstalk_service_role_policy""
  role = aws_iam_role.aws_elastic_beanstalk_service_role.name

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": [
        ""s3:*""
      ],
      ""Effect"": ""Allow"",
      ""Resource"": ""*""
    },
    {
      ""Action"": [
        ""cloudwatch:*"",
        ""logs:*""
      ],
      ""Effect"": ""Allow"",
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_elastic_beanstalk_environment"" ""go_app_env"" {
  name                = ""go-app-env""
  application         = aws_elastic_beanstalk_application.go_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.1.1 running Go 1.15""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""InstanceType""
    value     = ""t2.micro""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""LoadBalancerType""
    value     = ""application""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_eb_app': True, 'is_valid_eb_app_template': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_instance"" ""web_app"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Amazon Linux AMI ID for us-east-1
  instance_type = ""t2.micro""

  tags = {
    Name = ""WebAppInstance""
  }
}

resource ""aws_security_group"" ""web_app_sg"" {
  name_prefix = ""WebAppSG-""

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

resource ""aws_security_group_rule"" ""allow_ssh"" {
  type        = ""ingress""
  from_port   = 22
  to_port     = 22
  protocol    = ""tcp""
  cidr_blocks = [""0.0.0.0/0""]
  security_group_id = aws_security_group.web_app_sg.id
}

resource ""aws_network_interface_sg_attachment"" ""web_app_sg_attach"" {
  network_interface_id = aws_instance.web_app.primary_network_interface_id
  security_group_id    = aws_security_group.web_app_sg.id
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_eb_app': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_kinesis_stream,Generate a basic Amazon Kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_stream {
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_stream
}",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""drow1""
  shard_count      = 1
  retention_period = 48

  shard_level_metrics = [
    ""IncomingBytes"",
    ""OutgoingBytes"",
  ]

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""test""
  }
}","Has one ""aws_kinesis_stream"" resource"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""terraform-kinesis-stream""
  shard_count = 1

  tags = {
    Environment = ""dev""
  }
}
```",True,Success,No error,No error,
aws_kinesis_video_stream,Generate a basic Kinesis Video Stream resource,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_video_stream {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_video_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_video_stream
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_kinesis_video_stream"" ""default"" {
  name                    = ""terraform-kinesis-video-stream""
  data_retention_in_hours = 1
  device_name             = ""kinesis-video-device-name""
  media_type              = ""video/h264""

  tags = {
    Name = ""terraform-kinesis-video-stream""
  }
}","Has one ""aws_kinesis_video_stream"" resource"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_video_stream"" ""example"" {
  name        = ""example-stream""
  stream_mode = ""PRODUCE_AND_CONSUME""

  tags = {
    Environment = ""dev""
  }
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 7, in resource ""aws_kinesis_video_stream"" ""example"":
   7:   stream_mode = ""PRODUCE_AND_CONSUME""

An argument named ""stream_mode"" is not expected here.
",None,Terraform plan failed.
"aws_kinesis_stream, aws_kinesis_stream_consumer",Generate a resource to manage a Kinesis Stream Consumer,"package terraform.validation

import future.keywords.in

default has_valid_resources = false


has_valid_kinesis_stream {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_kinesis_stream_consumer {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream_consumer""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.stream_arn
}

has_valid_resources {
	has_valid_kinesis_stream
    has_valid_kinesis_stream_consumer
}
",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""example-stream""
  shard_count = 1
}

resource ""aws_kinesis_stream_consumer"" ""example"" {
  name       = ""example-consumer""
  stream_arn = aws_kinesis_stream.example.arn
}","Has one ""aws_kinesis_stream"" resource and one ""aws_kinesis_stream_consumer"" resource"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""example-stream""
  shard_count = 1
}

resource ""aws_kinesis_stream_consumer"" ""example_consumer"" {
  stream_arn = aws_kinesis_stream.example.arn
  name       = ""example-consumer""
}
```",True,Success,No error,No error,
aws_kinesis_analytics_application,Generate a basic Amazon Kinesis Analytics Application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
}

has_valid_resources {
    has_valid_kinesis_analytics_application
}",1,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_analytics_application"" ""test_application"" {
  name = ""kinesis-analytics-application-test""
}","Has one ""aws_kinesis_analytics_application"" resource"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_analytics_application"" ""example"" {
  name        = ""example-app""
  description = ""Example Kinesis Analytics Application""

  inputs {
    name_prefix       = ""example_stream_""
    stream_name       = aws_kinesis_stream.example.name
    role_arn          = aws_iam_role.example.arn
    input_schema {
      record_format_type = ""JSON""
    }
  }

  outputs {
    name_prefix     = ""example_output_""
    schema_stream_name = aws_kinesis_stream.example.name
    role_arn          = aws_iam_role.example.arn
    output_schema {
      record_format_type = ""JSON""
    }
  }
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""example-stream""
  shard_count = 1
}

resource ""aws_iam_role"" ""example"" {
  name = ""example-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}
```",False,No opa_result,"
Error: Insufficient schema blocks

  on main.tf line 9, in resource ""aws_kinesis_analytics_application"" ""example"":
   9:   inputs {

At least 1 ""schema"" blocks are required.

Error: Unsupported argument

  on main.tf line 11, in resource ""aws_kinesis_analytics_application"" ""example"":
  11:     stream_name       = aws_kinesis_stream.example.name

An argument named ""stream_name"" is not expected here. Did you mean
""stream_names""?

Error: Unsupported argument

  on main.tf line 12, in resource ""aws_kinesis_analytics_application"" ""example"":
  12:     role_arn          = aws_iam_role.example.arn

An argument named ""role_arn"" is not expected here.

Error: Unsupported block type

  on main.tf line 13, in resource ""aws_kinesis_analytics_application"" ""example"":
  13:     input_schema {

Blocks of type ""input_schema"" are not expected here.

Error: Missing required argument

  on main.tf line 18, in resource ""aws_kinesis_analytics_application"" ""example"":
  18:   outputs {

The argument ""name"" is required, but no definition was found.

Error: Insufficient schema blocks

  on main.tf line 18, in resource ""aws_kinesis_analytics_application"" ""example"":
  18:   outputs {

At least 1 ""schema"" blocks are required.

Error: Unsupported argument

  on main.tf line 19, in resource ""aws_kinesis_analytics_application"" ""example"":
  19:     name_prefix     = ""example_output_""

An argument named ""name_prefix"" is not expected here.

Error: Unsupported argument

  on main.tf line 20, in resource ""aws_kinesis_analytics_application"" ""example"":
  20:     schema_stream_name = aws_kinesis_stream.example.name

An argument named ""schema_stream_name"" is not expected here.

Error: Unsupported argument

  on main.tf line 21, in resource ""aws_kinesis_analytics_application"" ""example"":
  21:     role_arn          = aws_iam_role.example.arn

An argument named ""role_arn"" is not expected here.

Error: Unsupported block type

  on main.tf line 22, in resource ""aws_kinesis_analytics_application"" ""example"":
  22:     output_schema {

Blocks of type ""output_schema"" are not expected here.
",None,Terraform plan failed.
aws_route53_zone,"Set up an aws_route_53 zone named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false


# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
}
",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it to your desired region
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}
```",True,Success,No error,No error,
"aws_route53_record, aws_route53_zone","Set up a non-alias aws_route_53 record that is linked to an aws_route53_zone resource named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
	some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""alias_record"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""www""
  type    = ""A""

  alias {
    name                   = aws_lb.my_alb.dns_name
    zone_id                = aws_lb.my_alb.zone_id
    evaluate_target_health = true
  }
}

resource ""aws_lb"" ""my_alb"" {
  name               = ""my-alb""
  internal           = false
  load_balancer_type = ""application""
  subnets            = [""subnet-12345678"", ""subnet-87654321""]
  security_groups    = [""sg-12345678""]
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_configuration_valid': False, 'is_valid_r53_record': False, 'is_valid_r53_zone': True}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_metric, aws_s3_bucket_object","Set up an AWS S3 bucket named 'my_bucket' with forced destruction enabled for cleanup purposes. Create an S3 bucket metric named 'my_bucket_metric' to monitor the entire bucket's activity. Additionally, include an S3 bucket object named 'my_object' with a specific key and content to be stored in the bucket.","package terraform.validation

default valid_configuration = false
default has_s3_bucket = false
default has_s3_bucket_metrix = false
default has_s3_bucket_object = false


# Check for if any aws_s3_bucket with ""bucket""
has_s3_bucket {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.name == ""my_bucket""
    resource.expressions.bucket.constant_value != null
}

has_s3_bucket_metrix{
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket_metric""
    resource.name == ""my_bucket_matric""
    resource.expressions.bucket.references != null
    resource.expressions.name.constant_value != null
}

has_s3_bucket_object{
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket_object""
    resource.name == ""my_bucket_object""
    reference_check(resource)
    resource.expressions.key.constant_value != null
}

reference_check(object){
	some i
    object.expressions.bucket.references[i] == ""aws_s3_bucket.my_bucket.id""
}

valid_configuration{
    has_s3_bucket
    has_s3_bucket_metrix
    
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""test"" {
  bucket        = ""metricbeat-test-bucket""
  force_destroy = true // Required for cleanup
}

resource ""aws_s3_bucket_metric"" ""test"" {
  bucket = aws_s3_bucket.test.id
  name   = ""EntireBucket""
}

resource ""aws_s3_bucket_object"" ""test"" {
  key     = ""someobject""
  bucket  = aws_s3_bucket.test.id
  content = ""something""
}","has one ""aws_s3_bucket"" resource
    with one ""bucket""

has one ""aws_s3_bucket_metric""
    with one ""bucket""
    with one ""name

has one ""aws_s3_bucket_object""
    with one ""bucket""
    with one ""key""
"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Choose a valid AWS region
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my_bucket""

  lifecycle {
    prevent_destroy = true
  }
}

resource ""aws_s3_bucket_metric"" ""my_bucket_metric"" {
  bucket = aws_s3_bucket.my_bucket.id
  name   = ""my_bucket_metric""
}

resource ""aws_s3_bucket_object"" ""my_object"" {
  bucket = aws_s3_bucket.my_bucket.id
  key    = ""my_object""
  content = ""This is the content of my object.""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_s3_bucket': True, 'has_s3_bucket_metrix': False, 'has_s3_bucket_object': False, 'valid_configuration': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_versioning","Create a bucket ""sample"". Implement versioning resource for the AWS S3 bucket named 'sample' with the versioning status set to 'Enabled' and specify the expected_bucket_owner as '123456789012' to ensure ownership consistency.","package terraform.validation

default has_s3_bucket_versioning = false

default has_s3_bucket = false

default valid_configuration = false

has_s3_bucket {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.name == ""sample""
}

has_s3_bucket_versioning {
    versioning := input.planned_values.root_module.resources[_]
    versioning.type == ""aws_s3_bucket_versioning""
    versioning.values.bucket == ""sample""
    versioning.values.expected_bucket_owner == ""123456789012""
    versioning.values.versioning_configuration[_].status == ""Enabled""

}

valid_configuration {
        has_s3_bucket
    has_s3_bucket_versioning
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""sample"" {
  bucket = ""sample""
}

resource ""aws_s3_bucket_versioning"" ""sample"" {
  bucket = ""sample""
  versioning_configuration {
    status = ""Enabled""
  }
  expected_bucket_owner = ""123456789012""
}","has one aws_s3_bucket resource
with bucket

has one ""aws_s3_bucket_versioning"" resource
     with bucket
     with versionaning_configuration
          with status"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it to your desired region
}

resource ""aws_s3_bucket"" ""sample"" {
  bucket        = ""sample""
  expected_hash = ""123456789012""

  versioning {
    enabled = true
  }
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 7, in resource ""aws_s3_bucket"" ""sample"":
   7:   expected_hash = ""123456789012""

An argument named ""expected_hash"" is not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_logging","Create a bucket ""a"". Then configure logging for the bucket to send access logs to another bucket named 'logging-680235478471' with a prefix of 'log/' in the target bucket.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_logging = false

default valid_configuration = false

has_s3_bucket {
    bucket := input.configuration.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""a""
}

has_s3_bucket_logging {
    logging := input.configuration.root_module.resources[_]
    logging.type == ""aws_s3_bucket_logging""
    logging.expression.bucket.constant_value == ""a""
    logging.expression.target_bucket.constant_value == ""logging-680235478471""
    logging.expression.target_prefix.constant_value == ""log/""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_logging
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""testbucketineu-west2""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket        = ""testbucketineu-west2""
  target_bucket = ""logging-680235478471""
  target_prefix = ""log/""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_logging resource
with bucket
with target_bucket
with target_prefix"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""a""
}

resource ""aws_s3_bucket"" ""logging"" {
  bucket = ""logging-680235478471""
}

resource ""aws_s3_bucket_logging"" ""log"" {
  bucket        = aws_s3_bucket.a.bucket
  target_bucket = aws_s3_bucket.logging.bucket
  target_prefix = ""log/""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_s3_bucket': True, 'has_s3_bucket_logging': False, 'valid_configuration': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration",Create an AWS S3 bucket named 'example-bucket' with object lock enabled. Configure object lock governance mode with a retention period of 90 days for objects in the bucket.,"package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_object_lock_configuration = false

default valid_configuration = false


has_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example-bucket""
}

has_s3_bucket_object_lock_configuration {
    lock_config := input.planned_values.root_module.resources[_]
    lock_config.type == ""aws_s3_bucket_object_lock_configuration""
    lock_config.values.bucket == ""example-bucket""
    default_retention := lock_config.values.rule[_].default_retention[_]
    default_retention.mode == ""GOVERNANCE""
        default_retention.days == 90
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_object_lock_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    default_retention {
      mode  = ""GOVERNANCE""
      days  = 90
    }
  }
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_object_lock_configuration resource
with bucket
with rule
     with default_retention
          with mode (GOVERNANCE or COMPLIANCE)
          with days or years"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket        = ""example-bucket""
  force_destroy = true
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example_bucket_object_lock"" {
  bucket = aws_s3_bucket.example_bucket.id
  mode   = ""GOVERNANCE""
}

resource ""aws_s3_bucket_lifecycle_configuration"" ""example_bucket_lifecycle"" {
  bucket = aws_s3_bucket.example_bucket.id

  rule {
    id     = ""retention-policy""
    filter {}
    status = ""Enabled""

    expiration {
      days = 90
    }
  }
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 12, in resource ""aws_s3_bucket_object_lock_configuration"" ""example_bucket_object_lock"":
  12:   mode   = ""GOVERNANCE""

An argument named ""mode"" is not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration",Create an AWS S3 bucket named 'example-bucket' with object lock enabled. Configure object lock compliance mode with a retention period of 30 days for objects in the bucket.,"package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_object_lock_configuration = false

default valid_configuration = false


has_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example-bucket""
}

has_s3_bucket_object_lock_configuration {
    lock_config := input.planned_values.root_module.resources[_]
    lock_config.type == ""aws_s3_bucket_object_lock_configuration""
    lock_config.values.bucket == ""example-bucket""
    default_retention := lock_config.values.rule[_].default_retention[_]
    default_retention.mode == ""COMPLIANCE""
        default_retention.days == 30
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_object_lock_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    default_retention {
      mode  = ""COMPLIANCE""
      days  = 30
    }
  }
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_object_lock_configuration resource
with bucket
with rule
     with default_retention
          with mode (GOVERNANCE or COMPLIANCE)
          with days or years"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""example-bucket""
  object_lock_configuration {
    object_lock_enabled = ""Enabled""
  }
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example_bucket_object_lock"" {
  bucket = aws_s3_bucket.example_bucket.id
  object_lock_configuration {
    mode  = ""Compliance""
    days  = 30
  }
}
```",False,No opa_result,"
Error: Unsupported block type

  on main.tf line 14, in resource ""aws_s3_bucket_object_lock_configuration"" ""example_bucket_object_lock"":
  14:   object_lock_configuration {

Blocks of type ""object_lock_configuration"" are not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration","create a bucket 'pike-680235478471'. Set up request payment configuration for an AWS S3 bucket named 'pike-680235478471', specifying the payer as 'Requester'.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_request_payment_configuration = false

default valid_configuration = false

has_s3_bucket {
    some i
    bucket := input.planned_values.root_module.resources[i]
    bucket.type == ""aws_s3_bucket""
    bucket.values.bucket == ""pike-680235478471""
}

has_s3_bucket_request_payment_configuration {
    some i
    payment_config := input.planned_values.root_module.resources[i]
    payment_config.type == ""aws_s3_bucket_request_payment_configuration""
    payment_config.values.bucket == ""pike-680235478471""
    payment_config.values.payer == ""Requester""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_request_payment_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""pike"" {
  bucket = ""pike-680235478471""
  payer  = ""Requester""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_request_payment_configuration resource
with bucket
with payer (BucketOwner or Requester)"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""pike"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""requester_pays"" {
  bucket = aws_s3_bucket.pike.id
  request_payer = ""Requester""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 9, in resource ""aws_s3_bucket_request_payment_configuration"" ""requester_pays"":
   9: resource ""aws_s3_bucket_request_payment_configuration"" ""requester_pays"" {

The argument ""payer"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 11, in resource ""aws_s3_bucket_request_payment_configuration"" ""requester_pays"":
  11:   request_payer = ""Requester""

An argument named ""request_payer"" is not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_public_access_block","Implement public access block settings for an AWS S3 bucket named 'pike-680235478471' to block public ACLs, public policies, and restrict public buckets.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_public_access_block = false

default valid_configuration = false

has_s3_bucket {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_s3_bucket""
    resource.values.bucket == ""pike-680235478471""
}

has_s3_bucket_public_access_block {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket_public_access_block""
    resource.values.bucket ==  ""pike-680235478471""
    resource.values.block_public_acls == true
    resource.values.block_public_policy == true
    resource.values.ignore_public_acls == true
    resource.values.restrict_public_buckets == true

}


valid_configuration {
    has_s3_bucket
    has_s3_bucket_public_access_block
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = ""pike-680235478471""

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_public_access_block resource
with bucket
with block_public_acls (boolean)
with block_public_policy (boolean)
with ignore_public_acls (boolean)
with restrict_public_buckets (boolean)"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""pike_680235478471"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_public_access_block"" ""pike_680235478471"" {
  bucket = aws_s3_bucket.pike_680235478471.id

  block_public_acls   = true
  block_public_policy = true
  restrict_public_buckets = true
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_s3_bucket': True, 'has_s3_bucket_public_access_block': False, 'valid_configuration': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration","Create a Terraform configuration for an AWS environment. Start by defining a KMS key with a specific description and a set deletion window in days. Next, define an S3 bucket with a specified name. Finally, configure server-side encryption for the S3 bucket using the previously defined KMS key. The encryption should use the KMS algorithm specified in AWS. Include resource blocks for each component and ensure the bucket encryption references the KMS key's ARN and uses the KMS algorithm for encryption.","package terraform.validation

default has_aws_kms_key = false
default has_aws_s3_bucket = false
default has_aws_s3_bucket_server_side_encryption_configuration = false

has_aws_kms_key {
    key := input.planned_values.root_module.resources[_]
    key.type == ""aws_kms_key""
    key.name == ""mykey""
    key.values.description == ""This key is used to encrypt bucket objects""
    key.values.deletion_window_in_days == 10
}

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""mybucket""
    bucket.values.bucket == ""mybucket""
}

has_aws_s3_bucket_server_side_encryption_configuration {
    encryption_config := input.planned_values.root_module.resources[_]
    encryption_config.type == ""aws_s3_bucket_server_side_encryption_configuration""
    encryption_config.name == ""example""
    encryption_config.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    rule := encryption_config.values.rule.apply_server_side_encryption_by_default
    rule.kms_master_key_id == input.planned_values.root_module.resources[_].values.arn  # Correct KMS ARN reference
    rule.sse_algorithm == ""aws:kms""
}

valid_configuration {
    has_aws_kms_key
    has_aws_s3_bucket
    has_aws_s3_bucket_server_side_encryption_configuration
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Resource ""aws_kms_key""
has one ""description""
has one ""deletion_window_in_days""

Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_server_side_encryption_configuration""
has one ""bucket""
within ""rule"", it specifies:
""kms_master_key_id""
""sse_algorithm"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it to your preferred region
}

resource ""aws_kms_key"" ""example"" {
  description             = ""KMS key for S3 encryption""
  enable_key_rotation     = true
  deletion_window_in_days = 7
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""your-unique-bucket-name"" # Replace with your desired bucket name
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.example.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_aws_kms_key': False, 'has_aws_s3_bucket': False, 'has_aws_s3_bucket_server_side_encryption_configuration': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_ownership_controls","Generate a Terraform configuration for managing AWS S3 resources. Start by creating an S3 bucket with a specific name. Follow this with a configuration block for S3 bucket ownership controls, referencing the bucket's ID and setting the object ownership policy to a predefined setting. Then, define an S3 bucket ACL resource that depends on the successful application of the ownership controls. This ACL should set the bucket access to private and also reference the bucket's ID. Ensure each resource block is defined clearly and connected appropriately through references.","package terraform.validation

default has_aws_s3_bucket = false
default has_aws_s3_bucket_ownership_controls = false
default has_aws_s3_bucket_acl = false

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example""
    bucket.values.bucket == ""my-tf-example-bucket""
}

has_aws_s3_bucket_ownership_controls {
    ownership_controls := input.planned_values.root_module.resources[_]
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    ownership_controls.name == ""example""
    ownership_controls.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    ownership_controls.values.rule.object_ownership == ""BucketOwnerPreferred""
}

has_aws_s3_bucket_acl {
    acl := input.planned_values.root_module.resources[_]
    acl.type == ""aws_s3_bucket_acl""
    acl.name == ""example""
    acl.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    acl.values.acl == ""private""
    dependency := acl.depends_on[_]
    dependency == ""aws_s3_bucket_ownership_controls.example""
}

valid_configuration {
    has_aws_s3_bucket
    has_aws_s3_bucket_ownership_controls
    has_aws_s3_bucket_acl
}",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-tf-example-bucket""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]

  bucket = aws_s3_bucket.example.id
  acl    = ""private""
}","Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_ownership_controls""
has one ""bucket""
within ""rule"", it specifies:
""object_ownership""

Resource ""aws_s3_bucket_acl""
has a dependency on ""aws_s3_bucket_ownership_controls.example""
has one ""bucket""
has one ""acl""
"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it as needed
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-unique-bucket-name"" # Replace with your desired bucket name
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]
  bucket     = aws_s3_bucket.example.id
  acl        = ""private""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_aws_s3_bucket': False, 'has_aws_s3_bucket_acl': False, 'has_aws_s3_bucket_ownership_controls': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_s3_bucket_object,"Craft a Terraform configuration for creating an AWS S3 bucket object. Define an S3 bucket object (with value ""object"") with specific attributes including the bucket name (with value ""your_bucket_name""), object key (with value ""new_object_key""), and the path to the source file (with value ""path/to/file""). ","package terraform.validation

default has_aws_s3_bucket_object = false

has_aws_s3_bucket_object {
    object := input.planned_values.root_module.resources[_]
    object.type == ""aws_s3_bucket_object""
    object.name == ""object""
    object.values.bucket == ""your_bucket_name""
    object.values.key == ""new_object_key""
    object.values.source == ""path/to/file""
}

valid_configuration {
    has_aws_s3_bucket_object
}",1,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket_object"" ""object"" {
  bucket = ""your_bucket_name""
  key    = ""new_object_key""
  source = ""path/to/file""
}","Resource ""aws_s3_bucket_object""
has one ""bucket""
has one ""key""
has one ""source""
has one ""etag"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""your_bucket_name""
}

resource ""aws_s3_object"" ""example"" {
  bucket = aws_s3_bucket.example.id
  key    = ""new_object_key""
  source = ""path/to/file""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_aws_s3_bucket_object': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_ownership_controls","Generate a Terraform configuration for creating an AWS S3 bucket and setting its ownership controls. Define a resource for an S3 bucket with a specified name. Follow this by configuring ownership controls for this bucket, referencing its ID. Within the ownership controls, set a rule to specify the object ownership as 'BucketOwnerPreferred'. Ensure that the resources are properly linked using references, and the configuration is clearly laid out to reflect these settings.","package terraform.validation

default has_aws_s3_bucket_example = false
default has_aws_s3_bucket_ownership_controls_example = false

has_aws_s3_bucket_example {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""example""
    bucket.values.bucket == ""example""
}

has_aws_s3_bucket_ownership_controls_example {
    controls := input.planned_values.root_module.resources[_]
    controls.type == ""aws_s3_bucket_ownership_controls""
    controls.name == ""example""
    controls.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensures it's the correct reference to ""example"" bucket
    controls.values.rule.object_ownership == ""BucketOwnerPreferred""
}

valid_configuration {
    has_aws_s3_bucket_example
    has_aws_s3_bucket_ownership_controls_example
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}","Resource ""aws_s3_bucket"" 
has one ""bucket""

Resource ""aws_s3_bucket_ownership_controls""
has one ""bucket""
within the ""rule"", it specifies:
""object_ownership"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-unique-bucket-name""
}

resource ""aws_s3_bucket_ownership_controls"" ""example_ownership_controls"" {
  bucket = aws_s3_bucket.example_bucket.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_aws_s3_bucket_example': False, 'has_aws_s3_bucket_ownership_controls_example': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_ami,Create an AWS aws_ami resource for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-*-gp2""]
  }
}

resource ""aws_instance"" ""example"" {
  ami           = data.aws_ami.latest_amazon_linux_2.id
  instance_type = ""t2.micro""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'ami_latest_amazon_linux_2': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_ami,Create an AWS aws_ami for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""137112412989""] # Amazon AWS official account

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm*""]
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'ami_latest_amazon_linux_2': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_security_group"" ""memcached"" {
  name_prefix = ""memcached-""

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

resource ""aws_instance"" ""memcached"" {
  count         = 3
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with the actual AMI ID for your region
  instance_type = ""t2.micro""
  key_name      = ""your-key-pair-name""

  security_groups = [aws_security_group.memcached.name]

  tags = {
    Name = ""memcached-${count.index}""
  }
}

output ""memcached_endpoints"" {
  value = aws_instance.memcached[*].private_ip
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'main': {'allow': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elasticache_cluster,Provide a Redis Instance,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""redis""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""redis""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 1
  parameter_group_name = ""default.redis3.2""
  engine_version       = ""3.2.10""
  port                 = 6379
}","requires cluster id, engine=redis, node_type, num_cache_nodes, parameter_group_name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_cluster"" ""redis"" {
  engine               = ""redis""
  cluster_id           = ""redis-cluster""
  node_type            = ""cache.m5.large""
  num_cache_nodes      = 1
  parameter_group_name = ""default.redis3.2""
  security_group_ids   = [aws_security_group.redis_sg.id]
}

resource ""aws_security_group"" ""redis_sg"" {
  name_prefix = ""redis-sg-""

  ingress {
    from_port        = 6379
    to_port          = 6379
    protocol         = ""tcp""
    cidr_blocks      = [""0.0.0.0/0""]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = ""-1""
    cidr_blocks      = [""0.0.0.0/0""]
  }
}
```",True,Success,No error,No error,
aws_elasticache_user,Provide an ElastiCache user resource with redis,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.engine == ""REDIS""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}","requires access string, engine = REDIS, user id and user name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user"" ""example"" {
  user_id        = ""my-user""
  access_string  = ""on ~* +@all""
  engine         = ""redis""
  passwords      = [""password1"", ""password2""]
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_elasticache_user"" ""example"":
   5: resource ""aws_elasticache_user"" ""example"" {

The argument ""user_name"" is required, but no definition was found.
",None,Terraform plan failed.
aws_elasticache_user,Provide an ElastiCache user resource with iam,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""iam""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type = ""iam""
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type = ""iam""}"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user"" ""example"" {
  user_id        = ""exampleUserId""
  access_string  = ""on ~* +@all""
  engine         = ""redis""
  password       = ""password123""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_elasticache_user"" ""example"":
   5: resource ""aws_elasticache_user"" ""example"" {

The argument ""user_name"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 9, in resource ""aws_elasticache_user"" ""example"":
   9:   password       = ""password123""

An argument named ""password"" is not expected here. Did you mean ""passwords""?
",None,Terraform plan failed.
aws_elasticache_user,Provide an ElastiCache user resource with password.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""password""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type      = ""password""
    passwords = [""password1"", ""password2""]
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type= ""password"", passwords = [""password1"", ""password2""]}"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user"" ""example"" {
  username       = ""testuser""
  access_string  = ""on ~* &* +@all""
  engine         = ""redis""
  user_id        = ""testuser""
  password       = ""password123""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_elasticache_user"" ""example"":
   5: resource ""aws_elasticache_user"" ""example"" {

The argument ""user_name"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 6, in resource ""aws_elasticache_user"" ""example"":
   6:   username       = ""testuser""

An argument named ""username"" is not expected here. Did you mean ""user_name""?

Error: Unsupported argument

  on main.tf line 10, in resource ""aws_elasticache_user"" ""example"":
  10:   password       = ""password123""

An argument named ""password"" is not expected here. Did you mean ""passwords""?
",None,Terraform plan failed.
"aws_elasticache_user, aws_elasticache_user_group",Provide an ElastiCache user group resource.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}","creates elasticache user resources(requires access string, engine = REDIS, user id and user name) and sets engine , creates group resource using the user ids"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine         = ""redis""
  user_group_id  = ""my-user-group""
  user_ids       = [""user1"", ""user2""]
}
```",True,Success,No error,No error,
aws_redshift_cluster,Create a RedShift cluster resource with a single node,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 1
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
}
",1,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""tf-redshift-cluster""
  database_name      = ""mydb""
  master_username    = ""exampleuser""
  master_password    = ""Mustbe8characters""
  node_type          = ""dc2.large""
  cluster_type       = ""single-node""
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 1"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""example-redshift-cluster""
  database_name      = ""mydb""
  master_username    = ""admin""
  master_password    = ""Password123!""
  node_type          = ""dc1.large""
  number_of_nodes    = 1

  tags = {
    Environment = ""dev""
  }
}
```",True,Success,No error,No error,
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier   = ""example-redshift-cluster""
  database_name        = ""mydb""
  master_username      = ""admin""
  master_password      = ""password123!""
  node_type            = ""dc2.large""
  number_of_nodes      = 2
  cluster_type         = ""multi-node""
  skip_final_snapshot  = true
  enable_logging       = false
}

resource ""aws_iam_role"" ""example"" {
  name = ""redshift_cluster_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
        Effect = ""Allow""
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""example"" {
  name = ""redshift_cluster_policy""
  role = aws_iam_role.example.name

  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = [
          ""s3:GetBucketLocation"",
          ""s3:ListBucket"",
          ""s3:Read*""
        ],
        Effect   = ""Allow"",
        Resource = [""*""]
      }
    ]
  })
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 14, in resource ""aws_redshift_cluster"" ""example"":
  14:   enable_logging       = false

An argument named ""enable_logging"" is not expected here.
",None,Terraform plan failed.
"aws_redshift_cluster, aws_redshift_endpoint_authorization",Create a 2 node RedShift cluster and create a new Amazon Redshift endpoint authorization for an account with AWS id 012345678910,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

# Function to check if Redshift snapshot schedule is every 12 hours
aws_redshift_endpoint_authorization_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_endpoint_authorization""
    resource.change.after.account == ""012345678910""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_endpoint_authorization_valid(input.resource_changes)
}",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_endpoint_authorization"" ""example"" {
  account            = ""012345678910""
  cluster_identifier = aws_redshift_cluster.example.cluster_identifier
}",Has an aws_redshift_resouce with 2 nodes and creates a aws_redshift_endpoint_authorization resource with account =012345678910," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, choose one appropriate for your use case
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier   = ""redshift-cluster-1""
  database_name        = ""mydb""
  master_username      = ""admin""
  master_password      = ""Passw0rd!"" # Replace with a strong password
  node_type            = ""dc2.large""
  cluster_type         = ""multi-node""
  number_of_nodes      = 2
  skip_final_snapshot  = true
}

resource ""aws_redshift_endpoint_authorization"" ""example"" {
  account_id    = ""012345678910""
  cluster_identifier = aws_redshift_cluster.example.cluster_identifier
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 16, in resource ""aws_redshift_endpoint_authorization"" ""example"":
  16: resource ""aws_redshift_endpoint_authorization"" ""example"" {

The argument ""account"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 17, in resource ""aws_redshift_endpoint_authorization"" ""example"":
  17:   account_id    = ""012345678910""

An argument named ""account_id"" is not expected here.
",None,Terraform plan failed.
"aws_redshift_cluster, aws_redshift_snapshot_schedule, aws_redshift_snapshot_schedule_association",Create a 1 node RedShift cluster and automatically create a snapshot every 12 hours,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 1
}

# Function to check if Redshift snapshot schedule is every 12 hours
aws_redshift_snapshot_schedule_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_snapshot_schedule""
    resource.change.after.definitions[_] == ""rate(12 hours)""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_snapshot_schedule_valid(input.resource_changes)
}",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 1

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_snapshot_schedule"" ""default"" {
  identifier = ""tf-redshift-snapshot-schedule""
  definitions = [
    ""rate(12 hours)"",
  ]
}

resource ""aws_redshift_snapshot_schedule_association"" ""default"" {
  cluster_identifier  = aws_redshift_cluster.example.id
  schedule_identifier = aws_redshift_snapshot_schedule.default.id
}",Has an aws_redshift resouce with 1 node and aws_redshift_snapshot_schedule resource with rate set to 12 hours. Then have a aws_redshift_snapshot_schedle_association to conect the 2," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier   = ""redshift-cluster-1""
  database_name        = ""mydb""
  master_username      = ""admin""
  master_password      = ""Passw0rd!""
  node_type            = ""dc2.large""
  number_of_nodes      = 1
  skip_final_snapshot  = false
  snapshot_identifier  = aws_redshift_cluster.example.id
}

resource ""aws_iam_role"" ""example"" {
  name = ""redshift-snapshot-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
        Effect = ""Allow"",
        Sid = """"
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""example"" {
  name = ""redshift-snapshot-policy""
  role = aws_iam_role.example.id

  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ],
        Effect   = ""Allow"",
        Resource = ""${aws_s3_bucket.example.arn}/*""
      }
    ]
  })
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""redshift-snapshot-bucket""
}

resource ""aws_redshift_cluster_snapshot"" ""every_12_hours"" {
  cluster_identifier = aws_redshift_cluster.example.id
  snapshot_identifier = ""${aws_redshift_cluster.example.id}-snapshot""
  force_delete        = true
}

schedule ""cron"" ""take_snapshot"" {
  expression    = ""0 */12 * * *""
  timezone      = ""UTC""
  action {
    type        = ""aws_lambda_invocation""
    function_name = aws_lambda_function.take_snapshot.arn
  }
}

resource ""aws_lambda_function"" ""take_snapshot"" {
  filename         = ""snapshot_lambda_function.zip""
  source_code_hash = filebase64sha256(""snapshot_lambda_function.zip"")
  function_name    = ""TakeSnapshot""
  role             = aws_iam_role.example.arn
  handler          = ""index.handler""
  runtime          = ""nodejs14.x""
}
```",False,No opa_result,"
Error: Unsupported block type

  on main.tf line 63:
  63: schedule ""cron"" ""take_snapshot"" {

Blocks of type ""schedule"" are not expected here.
",None,Terraform plan failed.
aws_redshift_cluster,Setup a 2 node RedShift cluster in us-east-1 and replicate it to us-east-2.,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
    resource.change.after.snapshot_copy[_].destination_region == ""us-east-2""
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
}
",2,"provider ""aws"" {
  region = ""us-east-1""
  alias  = ""useast1""
}

# Redshift Cluster in us-east-1
resource ""aws_redshift_cluster"" ""primary_cluster"" {
  provider = aws.useast1

  cluster_identifier  = ""primary-cluster""
  database_name       = ""mydatabase""
  master_username     = ""myusername""
  master_password     = ""Mypassword1""
  node_type           = ""dc2.large""
  cluster_type        = ""multi-node""
  number_of_nodes     = 2
  skip_final_snapshot = true

  snapshot_copy {
    destination_region = ""us-east-2""
  }

}
",Creates a redshift_clluster in us-east-1 and a snapshot_copy in us-east-2," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_redshift_cluster"" ""redshift_cluster"" {
  provider                 = aws
  cluster_identifier       = ""redshift-cluster""
  database_name            = ""mydb""
  master_username          = ""admin""
  master_password          = ""password123""
  node_type                = ""dc2.large""
  number_of_nodes          = 2
  cluster_type             = ""multi-node""
}

provider ""aws"" {
  alias   = ""us-east-2""
  region  = ""us-east-2""
}

resource ""aws_redshift_cluster"" ""redshift_cluster_replica"" {
  provider                 = aws.us-east-2
  cluster_identifier       = ""redshift-cluster-replica""
  database_name            = ""mydb""
  master_username          = ""admin""
  master_password          = ""password123""
  node_type                = ""dc2.large""
  number_of_nodes          = 2
  cluster_type             = ""multi-node""
}
```",False,No opa_result,"
Error: invalid value for master_password (must contain at least one uppercase letter)

  with aws_redshift_cluster.redshift_cluster,
  on main.tf line 10, in resource ""aws_redshift_cluster"" ""redshift_cluster"":
  10:   master_password          = ""password123""


Error: invalid value for master_password (must contain at least one uppercase letter)

  with aws_redshift_cluster.redshift_cluster_replica,
  on main.tf line 26, in resource ""aws_redshift_cluster"" ""redshift_cluster_replica"":
  26:   master_password          = ""password123""

",None,Terraform plan failed.
aws_lambda_layer_version,"Create a Lambda Layer Version resource from ""lambda_layer_payload.zip""","package main

import future.keywords.in

default allow = false

# Check if the AWS Lambda Layer Version is valid
aws_lambda_layer_version_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_layer_version""
    resource.change.after.filename == ""lambda_layer_payload.zip""
}

# Aggregate all checks
allow {
    aws_lambda_layer_version_valid(input.resource_changes)
}",1,"resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename   = ""lambda_layer_payload.zip""
  layer_name = ""lambda_layer_name""

  compatible_runtimes = [""nodejs16.x""]
}","Create a aws_lambda_layer_version by using file_name = ""lambda_layer_payload.zip"". Layer name can be any."," ```hcl
# AWS provider configuration with region 'us-west-2'
provider ""aws"" {
  region = ""us-west-2""
}

# Define a variable for the Lambda layer zip file path
variable ""lambda_layer_zip"" {
  description = ""Path to the Lambda layer ZIP file""
  default     = ""lambda_layer_payload.zip""
}

# Create an IAM role for the Lambda function
resource ""aws_iam_role"" ""lambda_execution_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Principal = {
          Service = ""lambda.amazonaws.com""
        },
        Effect = ""Allow"",
        Sid = """"
      }
    ]
  })
}

# Create a Lambda Layer Version resource from the specified zip file
resource ""aws_lambda_layer_version"" ""example_lambda_layer"" {
  filename            = var.lambda_layer_zip
  source_code_hash    = filebase64sha256(var.lambda_layer_zip)
  compatible_runtimes = [""python3.8"", ""python3.9""]
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 32, in resource ""aws_lambda_layer_version"" ""example_lambda_layer"":
  32: resource ""aws_lambda_layer_version"" ""example_lambda_layer"" {

The argument ""layer_name"" is required, but no definition was found.
",None,Terraform plan failed.
"aws_cloudwatch_composite_alarm, aws_cloudwatch_metric_alarm",Create a CloudWatch Composite Alarm resource.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_composite_alarm""
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}",2,"resource ""aws_cloudwatch_composite_alarm"" ""example"" {
  alarm_name        = ""example-composite-alarm""

  alarm_rule = <<EOF
ALARM(${aws_cloudwatch_metric_alarm.foobar.alarm_name})
EOF

}

resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = ""2""
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = ""120""
  statistic                 = ""Average""
  threshold                 = ""80""
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",Creates an aws_cloudwatch_composite_alarm which requires at least one other aws_cloudwatch_metric_alarm," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_cloudwatch_composite_alarm"" ""example"" {
  alarm_name                = ""example-composite-alarm""
  comparison_operator       = ""GreaterThanThreshold""
  evaluation_periods        = 1
  threshold                 = 80
  treat_missing_data        = ""breaching""
  metric_query {
    id          = ""m1""
    expression  = ""ANOMALY_DETECTION_BAND(m2, 3)""
    label       = ""CPUUtilization (Expected Range: 0-80)""
    return_data = false
  }
  metric_query {
    id                = ""m2""
    metric_name       = ""CPUUtilization""
    namespace         = ""AWS/EC2""
    period            = 300
    statistic         = ""Average""
    unit              = ""Percent""
  }
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
   5: resource ""aws_cloudwatch_composite_alarm"" ""example"" {

The argument ""alarm_rule"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 7, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
   7:   comparison_operator       = ""GreaterThanThreshold""

An argument named ""comparison_operator"" is not expected here.

Error: Unsupported argument

  on main.tf line 8, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
   8:   evaluation_periods        = 1

An argument named ""evaluation_periods"" is not expected here.

Error: Unsupported argument

  on main.tf line 9, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
   9:   threshold                 = 80

An argument named ""threshold"" is not expected here.

Error: Unsupported argument

  on main.tf line 10, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
  10:   treat_missing_data        = ""breaching""

An argument named ""treat_missing_data"" is not expected here.

Error: Unsupported block type

  on main.tf line 11, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
  11:   metric_query {

Blocks of type ""metric_query"" are not expected here.

Error: Unsupported block type

  on main.tf line 17, in resource ""aws_cloudwatch_composite_alarm"" ""example"":
  17:   metric_query {

Blocks of type ""metric_query"" are not expected here.
",None,Terraform plan failed.
aws_cloudwatch_metric_alarm,Create a CloudWatch Metric Alarm resource.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_metric_alarm""
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}",2,"resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 120
  statistic                 = ""Average""
  threshold                 = 80
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",Createsr a aws_cloudwatch_metric_alarm," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_cloudwatch_metric_alarm"" ""example"" {
  alarm_name                = ""cpu-utilization-high""
  comparison_operator       = ""GreaterThanThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 300
  statistic                 = ""Average""
  threshold                 = 85.0
  alarm_actions             = [aws_sns_topic.example.arn]
}

resource ""aws_sns_topic"" ""example"" {
  name = ""cpu-utilization-alarms""
}
```",True,Success,No error,No error,
aws_cloudwatch_metric_alarm,Generate a Terraform HCL code snippet to define an AWS CloudWatch metric alarm named 'foobar' that monitors the 'CPUUtilization' metric of AWS EC2 instances. The alarm should trigger when the average CPU utilization is greater than or equal to 80% for two consecutive 2-minute periods. Ensure that no actions are taken when there's insufficient data.,"package main

import future.keywords.in

default allow = false

# Check if AWS CloudWatch Composite Alarm is being created
aws_cloudwatch_composite_alarm_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_metric_alarm""
    resource.change.after.comparison_operator == ""GreaterThanOrEqualToThreshold""
    resource.change.after.threshold == 80
    resource.change.after.evaluation_periods == 2
}

# Aggregate all checks
allow {
    aws_cloudwatch_composite_alarm_valid(input.resource_changes)
}
",2,"resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""terraform-test-foobar5""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 120
  statistic                 = ""Average""
  threshold                 = 80
  alarm_description         = ""This metric monitors ec2 cpu utilization""
  insufficient_data_actions = []
}",," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_cloudwatch_metric_alarm"" ""foobar"" {
  alarm_name                = ""foobar""
  comparison_operator       = ""GreaterThanOrEqualToThreshold""
  evaluation_periods        = 2
  metric_name               = ""CPUUtilization""
  namespace                 = ""AWS/EC2""
  period                    = 120
  statistic                 = ""Average""
  threshold                 = 80.0
  alarm_actions             = [aws_sns_topic.foobar_alarm.arn]
}

resource ""aws_sns_topic"" ""foobar_alarm"" {
  name = ""foobar_alarm""
}
```",True,Success,No error,No error,
aws_dynamodb_contributor_insights,Create a DynamoDB Contributor Insights resource for a specific table with custom settings,"package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Contributor Insights is being created for a specific table
aws_dynamodb_contributor_insights_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_contributor_insights""
}

# Aggregate all checks
allow {
    aws_dynamodb_contributor_insights_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_contributor_insights"" ""test"" {
  table_name = ""ExampleTableName""
}",Create a DynamoDB table and enable contributor insights," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dynamodb_table_item"" ""contributor_insights"" {
  table_name = var.table_name

  item = jsonencode({
    ""PartitionKey"": {""S"": ""UserId""},
    ""SortKey"": {""S"": ""MessageTimestamp""},
    ""Message"": {""S"": ""Hello, DynamoDB!""},
    ""Sender"": {""S"": ""User123""}
  })

  provider = aws.contributor_insights
}

variable ""table_name"" {
  description = ""The name of the DynamoDB table""
  default     = ""example-table""
}
```",False,No opa_result,"
Error: Provider configuration not present

To work with aws_dynamodb_table_item.contributor_insights its original
provider configuration at
provider[""registry.terraform.io/hashicorp/aws""].contributor_insights is
required, but it has been removed. This occurs when a provider configuration
is removed while objects created by that provider still exist in the state.
Re-add the provider configuration to destroy
aws_dynamodb_table_item.contributor_insights, after which you can remove the
provider configuration again.
",None,Terraform plan failed.
"aws_dynamodb_kinesis_streaming_destination, aws_dynamodb_table, aws_kinesis_stream",Create a Kinesis Streaming Destination for a specific DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Kinesis Streaming Destination is being created for a specific table
aws_dynamodb_kinesis_streaming_destination_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_kinesis_streaming_destination""
}

# Aggregate all checks
allow {
    aws_dynamodb_kinesis_streaming_destination_valid(input.resource_changes)
}",2,"resource ""aws_dynamodb_table"" ""example"" {
  name     = ""orders""
  hash_key = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

resource ""aws_kinesis_stream"" ""example"" {
  name        = ""order_item_changes""
  shard_count = 1
}

resource ""aws_dynamodb_kinesis_streaming_destination"" ""example"" {
  stream_arn = aws_kinesis_stream.example.arn
  table_name = aws_dynamodb_table.example.name
}","create a stream_arn (name), dynamodb table(hash_key, name, attribute (name, type)), create the kineses streaming"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PROVISIONED""
  read_capacity  = 5
  write_capacity = 5
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name        = ""example-stream""
  shard_count = 1
}

resource ""aws_kinesis_analytics_application"" ""example_app"" {
  name           = ""example-app""
  runtime        = ""FLINK-1_8""
  service_execution_role = aws_iam_role.example_role.arn

  inputs {
    name_prefix       = ""example-input-""
    kinesis_stream {
      stream_name     = aws_kinesis_stream.example_stream.name
      role_arn        = aws_iam_role.example_role.arn
    }
    schema {
      record_format_type = ""JSON""
    }
  }

  outputs {
    name       = ""example-output""
    kinesis_stream {
      stream_name     = aws_kinesis_stream.example_stream.name
      role_arn        = aws_iam_role.example_role.arn
    }
    schema {
      record_format_type = ""JSON""
    }
  }
}

resource ""aws_iam_role"" ""example_role"" {
  name = ""example-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 25, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  25:   runtime        = ""FLINK-1_8""

An argument named ""runtime"" is not expected here.

Error: Unsupported argument

  on main.tf line 26, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  26:   service_execution_role = aws_iam_role.example_role.arn

An argument named ""service_execution_role"" is not expected here.

Error: Missing required argument

  on main.tf line 30, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  30:     kinesis_stream {

The argument ""resource_arn"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 31, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  31:       stream_name     = aws_kinesis_stream.example_stream.name

An argument named ""stream_name"" is not expected here.

Error: Insufficient record_columns blocks

  on main.tf line 34, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  34:     schema {

At least 1 ""record_columns"" blocks are required.

Error: Insufficient record_format blocks

  on main.tf line 34, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  34:     schema {

At least 1 ""record_format"" blocks are required.

Error: Unsupported argument

  on main.tf line 35, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  35:       record_format_type = ""JSON""

An argument named ""record_format_type"" is not expected here.

Error: Missing required argument

  on main.tf line 41, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  41:     kinesis_stream {

The argument ""resource_arn"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 42, in resource ""aws_kinesis_analytics_application"" ""example_app"":
  42:       stream_name     = aws_kinesis_stream.example_stream.name

An argument named ""stream_name"" is not expected here.
",None,Terraform plan failed.
aws_dax_parameter_group,Create a custom DAX parameter group,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_parameter_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_parameter_group""
}

# Aggregate all checks
allow {
    aws_dax_parameter_group_valid(input.resource_changes)
}",2,"resource ""aws_dax_parameter_group"" ""example"" {
  name = ""example""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""100000""
  }
}","require name of the group, list of parameters (key, value) pairs"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dax_parameter_group"" ""example"" {
  name = ""custom-dax-parameters""

  parameters {
    name  = ""query-ttl""
    value = ""60""
  }

  parameters {
    name  = ""record-ttl""
    value = ""30""
  }
}
```",True,Success,No error,No error,
aws_dynamodb_table,Configure on-demand capacity mode for a DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dynamodb_table_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_table""
    resource.change.after.billing_mode == ""PAY_PER_REQUEST""
}

# Aggregate all checks
allow {
    aws_dynamodb_table_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_table"" ""basic-dynamodb-table"" {
  name           = ""example""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""UserId""

  attribute {
    name = ""UserId""
    type = ""S""
  }
}","dynamodb table -> hash_key, name, attribute (name, type), billing mode pay per read"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""terraform-lock-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""LockID""

  attribute {
    name = ""LockID""
    type = ""S""
  }
}
```",True,Success,No error,No error,
aws_sagemaker_code_repository,"Create a sagemaker_code_repository to aws_sagemaker_code_repository to ""https://github.com/hashicorp/terraform-provider-aws.git""","package main

import future.keywords.in

default allow = false


aws_sagemaker_code_repository_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_code_repository""
    resource.change.after.git_config[0].repository_url ==  ""https://github.com/hashicorp/terraform-provider-aws.git""
}

# Aggregate all checks
allow {
    aws_sagemaker_code_repository_valid(input.resource_changes)
}",1,"resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""example""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}",," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""hashicorp-terraform-provider-aws""
  git_repo {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}
```",False,No opa_result,"
Error: Insufficient git_config blocks

  on main.tf line 5, in resource ""aws_sagemaker_code_repository"" ""example"":
   5: resource ""aws_sagemaker_code_repository"" ""example"" {

At least 1 ""git_config"" blocks are required.

Error: Unsupported block type

  on main.tf line 7, in resource ""aws_sagemaker_code_repository"" ""example"":
   7:   git_repo {

Blocks of type ""git_repo"" are not expected here.
",None,Terraform plan failed.
"aws_iam_role, aws_sagemaker_image, aws_iam_policy_document",Create a aws_sagemaker_image,"package main

import future.keywords.in

default allow = false

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_sagemaker_image_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_image""
}


# Aggregate all checks
allow {
    aws_sagemaker_image_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
}",2,"resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}

resource ""aws_sagemaker_image"" ""example"" {
  image_name = ""example""
  role_arn   = aws_iam_role.example.arn
}",," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_sagemaker_image"" ""example"" {
  image_name      = ""example-image""
  role_arn        = aws_iam_role.sagemaker_execution_role.arn
  container_image = ""123456789012.dkr.ecr.us-west-2.amazonaws.com/my-repo:latest""
}

resource ""aws_iam_role"" ""sagemaker_execution_role"" {
  name = ""sagemaker_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 8, in resource ""aws_sagemaker_image"" ""example"":
   8:   container_image = ""123456789012.dkr.ecr.us-west-2.amazonaws.com/my-repo:latest""

An argument named ""container_image"" is not expected here.
",None,Terraform plan failed.
aws_sagemaker_human_task_ui,"Create a SageMaker Human Task UI resource from sagemaker-human-task-ui-template.html containing ""<h1>
    TEST
</h1>""","package main

import future.keywords.in

default allow = false

aws_sagemaker_human_task_ui_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_human_task_ui""
    resource.change.after.ui_template[0].content == ""\u003ch1\u003e\n    TEST\n\u003c/h1\u003e""

}

# Aggregate all checks
allow {
    aws_sagemaker_human_task_ui_valid(input.resource_changes)
}",1,"resource ""aws_sagemaker_human_task_ui"" ""example"" {
  human_task_ui_name = ""example""

  ui_template {
    content = file(""sagemaker-human-task-ui-template.html"")
  }
}",," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_sagemaker_human_task_ui"" ""test_human_task_ui"" {
  human_task_ui_name = ""test-human-task-ui""
  ui_template {
    content = file(""sagemaker-human-task-ui-template.html"")
  }
}
```",False,No opa_result,"
Error: Invalid function argument

  on main.tf line 8, in resource ""aws_sagemaker_human_task_ui"" ""test_human_task_ui"":
   8:     content = file(""sagemaker-human-task-ui-template.html"")
    
     while calling file(path)

Invalid value for ""path"" parameter: no file exists at
""sagemaker-human-task-ui-template.html""; this function works only with files
that are distributed as part of the configuration source code, so if this
file will be created by a resource in this configuration you must instead
obtain this result from an attribute of that resource.
",None,Terraform plan failed.
aws_ami,Create the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",1,"data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""137112412989""] # Amazon Linux 2 AMI owner

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*""]
  }
}

resource ""aws_instance"" ""example"" {
  ami           = data.aws_ami.latest_amazon_linux_2.id
  instance_type = ""t2.micro""

  tags = {
    Name = ""terraform-example""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'ami_latest_amazon_linux_2': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elasticache_cluster,Provide a resource to reduce the number of database calls.,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_cluster"" ""redis"" {
  cluster_id           = ""terraform-redis""
  engine               = ""redis""
  node_type            = ""cache.m5.large""
  num_cache_nodes      = 1
  parameter_group_name = ""default.redis3.2""
  port                 = 6379
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'main': {'allow': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elasticache_user,authenticate a elasticache user with redis,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.engine == ""REDIS""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}","requires access string, engine = REDIS, user id and user name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user"" ""redis_user"" {
  engine       = ""redis""
  user_id      = ""my-user""
  access_string = ""+@all""
  no_password_required = false
  passwords    = [""password1"", ""password2""]
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_elasticache_user"" ""redis_user"":
   5: resource ""aws_elasticache_user"" ""redis_user"" {

The argument ""user_name"" is required, but no definition was found.
",None,Terraform plan failed.
aws_elasticache_user,authenticate a elasticache user with iam,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""iam""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",1,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type = ""iam""
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type = ""iam""}"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_iam_user"" ""elasticache_user"" {
  name = ""elasticache_user""
}

resource ""aws_iam_access_key"" ""elasticache_user_key"" {
  user = aws_iam_user.elasticache_user.name
}

output ""elasticache_user_access_key_id"" {
  value = aws_iam_access_key.elasticache_user_key.id
}

output ""elasticache_user_secret_access_key"" {
  value     = aws_iam_access_key.elasticache_user_key.secret
  sensitive = true
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'main': {'allow': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_elasticache_user,authenticate a elasticache user with passwords,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.after.authentication_mode[_].type == ""password""
}

# Aggregate all checks
allow {
    aws_elasticache_user_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""testUserName""
  access_string = ""on ~* +@all""
  engine        = ""REDIS""

  authentication_mode {
    type      = ""password""
    passwords = [""password1"", ""password2""]
  }
}","requires access string, engine = REDIS, user id and user name, authentication_mode {type= ""password"", passwords = [""password1"", ""password2""]}"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user"" ""example"" {
  user_id        = ""my-user""
  access_string  = ""on ~* &* +@all""
  engine         = ""redis""
  passwords      = [""password1"", ""password2""]
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_elasticache_user"" ""example"":
   5: resource ""aws_elasticache_user"" ""example"" {

The argument ""user_name"" is required, but no definition was found.
",None,Terraform plan failed.
"aws_elasticache_user, aws_elasticache_user_group",Provides an ElastiCache user group resource with 3 users,"package main

import future.keywords.in

default allow = false

# Check if AWS ElastiCache user group exists
aws_elasticache_user_group_exists(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
    resource.change.actions[_] == ""create""
    count(resource.change.after[user_ids])==3
}

# Check if AWS ElastiCache user exists
aws_elasticache_user_exists(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user""
    resource.change.actions[_] == ""create""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_exists(input.resource_changes)
    aws_elasticache_user_exists(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}",," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine         = ""redis""
  user_group_id  = ""default""
  user_ids       = [""user1"", ""user2"", ""user3""]
}

resource ""aws_elasticache_user"" ""user1"" {
  user_id   = ""user1""
  username  = ""user1""
  password  = ""password123""
  access_string = ""+@all""
}

resource ""aws_elasticache_user"" ""user2"" {
  user_id   = ""user2""
  username  = ""user2""
  password  = ""password456""
  access_string = ""+@read""
}

resource ""aws_elasticache_user"" ""user3"" {
  user_id   = ""user3""
  username  = ""user3""
  password  = ""password789""
  access_string = ""+@all""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 11, in resource ""aws_elasticache_user"" ""user1"":
  11: resource ""aws_elasticache_user"" ""user1"" {

The argument ""user_name"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 11, in resource ""aws_elasticache_user"" ""user1"":
  11: resource ""aws_elasticache_user"" ""user1"" {

The argument ""engine"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 13, in resource ""aws_elasticache_user"" ""user1"":
  13:   username  = ""user1""

An argument named ""username"" is not expected here. Did you mean ""user_name""?

Error: Unsupported argument

  on main.tf line 14, in resource ""aws_elasticache_user"" ""user1"":
  14:   password  = ""password123""

An argument named ""password"" is not expected here. Did you mean ""passwords""?

Error: Missing required argument

  on main.tf line 18, in resource ""aws_elasticache_user"" ""user2"":
  18: resource ""aws_elasticache_user"" ""user2"" {

The argument ""user_name"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 18, in resource ""aws_elasticache_user"" ""user2"":
  18: resource ""aws_elasticache_user"" ""user2"" {

The argument ""engine"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 20, in resource ""aws_elasticache_user"" ""user2"":
  20:   username  = ""user2""

An argument named ""username"" is not expected here. Did you mean ""user_name""?

Error: Unsupported argument

  on main.tf line 21, in resource ""aws_elasticache_user"" ""user2"":
  21:   password  = ""password456""

An argument named ""password"" is not expected here. Did you mean ""passwords""?

Error: Missing required argument

  on main.tf line 25, in resource ""aws_elasticache_user"" ""user3"":
  25: resource ""aws_elasticache_user"" ""user3"" {

The argument ""user_name"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 25, in resource ""aws_elasticache_user"" ""user3"":
  25: resource ""aws_elasticache_user"" ""user3"" {

The argument ""engine"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 27, in resource ""aws_elasticache_user"" ""user3"":
  27:   username  = ""user3""

An argument named ""username"" is not expected here. Did you mean ""user_name""?

Error: Unsupported argument

  on main.tf line 28, in resource ""aws_elasticache_user"" ""user3"":
  28:   password  = ""password789""

An argument named ""password"" is not expected here. Did you mean ""passwords""?
",None,Terraform plan failed.
aws_dynamodb_contributor_insights,"Enable contributor insights on a table named ""ExampleTableName""","package main

import future.keywords.in

default allow = false

# Check if AWS DynamoDB Contributor Insights is being created for a specific table
aws_dynamodb_contributor_insights_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_contributor_insights""
}

# Aggregate all checks
allow {
    aws_dynamodb_contributor_insights_valid(input.resource_changes)
}
",1,"resource ""aws_dynamodb_contributor_insights"" ""test"" {
  table_name = ""ExampleTableName""
}",Create a DynamoDB table and enable contributor insights," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""ExampleTableName""
  billing_mode   = ""PROVISIONED""
  read_capacity  = 5
  write_capacity = 5
  hash_key       = ""Id""

  attribute {
    name = ""Id""
    type = ""N""
  }
}

resource ""aws_dynamodb_contributor_insights"" ""example"" {
  table_name = aws_dynamodb_table.example.name
  enable     = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 20, in resource ""aws_dynamodb_contributor_insights"" ""example"":
  20:   enable     = true

An argument named ""enable"" is not expected here.
",None,Terraform plan failed.
aws_dax_parameter_group,Create a DAX parameter group.,"package main

import future.keywords.in

default allow = false

# Check for DAX parameter group creation
dax_parameter_group_created(resources) {
some resource in resources
resource.type == ""aws_dax_parameter_group""
resource.change.actions[_] == ""create""
}

# Allow DAX parameter group creation with specific parameters
allow {
dax_parameter_group_created(input.resource_changes)
}",2,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

resource ""aws_dax_parameter_group"" ""example"" {
name = ""example""

parameters {
name = ""query-ttl-millis""
value = ""100000""
}

parameters {
name = ""record-ttl-millis""
value = ""100000""
}
}","Has one ""aws_dax_parameter_group"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dax_parameter_group"" ""example"" {
  name = ""example-dax-param-group""

  parameters {
    name  = ""query-ttl""
    value = ""10000""
  }

  parameters {
    name  = ""consistent-read""
    value = ""true""
  }
}
```",True,Success,No error,No error,
aws_dynamodb_table,Enable point-in-time recovery for the DynamoDB table.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if point-in-time recovery is enabled
point_in_time_recovery_enabled(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.point_in_time_recovery[_].enabled == true
}

# Allow if DynamoDB table is modified and point-in-time recovery is enabled
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
point_in_time_recovery_enabled(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

point_in_time_recovery {
enabled = true # Set to true to enable point-in-time recovery
}

# Define other table settings as needed
}","Has one ""aws_dynamodb_table"" resource
with one ""point_in_time_recovery"" with ""enabled"" = true"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""terraform-lock-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""LockID""

  attribute {
    name = ""LockID""
    type = ""S""
  }
}

resource ""aws_dynamodb_table_item"" ""lock_item"" {
  table_name = aws_dynamodb_table.example.name
  hash_key   = aws_dynamodb_table.example.hash_key
  item       = <<EOF
{
  ""LockID"": {""S"": ""terraform-lock""}
}
EOF
}

resource ""aws_dynamodb_point_in_time_recovery"" ""example"" {
  table_name = aws_dynamodb_table.example.name
  enabled    = true
}
```",False,No opa_result,"
Error: Invalid resource type

  on main.tf line 26, in resource ""aws_dynamodb_point_in_time_recovery"" ""example"":
  26: resource ""aws_dynamodb_point_in_time_recovery"" ""example"" {

The provider hashicorp/aws does not support resource type
""aws_dynamodb_point_in_time_recovery"".
",None,Terraform plan failed.
aws_dynamodb_table,Configure a custom Time to Live (TTL) attribute for data expiration.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if TTL is enabled with the correct attribute name
ttl_configured(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.ttl[_].enabled == true
resource.change.after.ttl[_].attribute_name == ""custom_ttl_attribute""
}

# Allow if DynamoDB table is modified and TTL is configured correctly
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
ttl_configured(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

ttl {
attribute_name = ""custom_ttl_attribute"" # Replace with your custom TTL attribute name
enabled = true # Set to true to enable TTL on the table
}
}","Has one ""aws_dynamodb_table"" resource
with one ""ttl""
with one ""attribute_name"" = ""custom_ttl_attribute""
with one ""enabled"" = true"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""terraform_example""
  billing_mode   = ""PROVISIONED""
  read_capacity  = 5
  write_capacity = 5
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

resource ""aws_dynamodb_table_item"" ""example"" {
  table_name = aws_dynamodb_table.example.name
  hash_key   = aws_dynamodb_table.example.hash_key
  item       = <<EOF
{
  ""id"": {""S"": ""12345""},
  ""ttl"": {""N"": ""86400""} # TTL set to one day (in seconds)
}
EOF
}
```",False,No opa_result,"
Error: Invalid format of ""item"": invalid character '#' after object key:value pair

  with aws_dynamodb_table_item.example,
  on main.tf line 21, in resource ""aws_dynamodb_table_item"" ""example"":
  21:   item       = <<EOF
  22: {
  23:   ""id"": {""S"": ""12345""},
  24:   ""ttl"": {""N"": ""86400""} # TTL set to one day (in seconds)
  25: }
  26: EOF

",None,Terraform plan failed.
aws_dynamodb_table,Configure a DynamoDB table with server-side encryption enabled.,"package main

import future.keywords.in

default allow = false

dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

encryption_at_rest_enabled(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.server_side_encryption[_].enabled == true
}

allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
encryption_at_rest_enabled(resource)
}",2,"resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

server_side_encryption {
enabled = true
}
}","Has one ""aws_dynamodb_table"" resource
with one ""server_side_encryption""
with one ""enabled"" = true"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Replace with your preferred region
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""terraform-lock-table""
  billing_mode   = ""PROVISIONED""
  read_capacity  = 5
  write_capacity = 5
  hash_key       = ""LockID""

  attribute {
    name = ""LockID""
    type = ""S""
  }

  server_side_encryption {
    enabled = true
  }
}
```",True,Success,No error,No error,
aws_dynamodb_table,Create a DynamoDB table with data encryption at rest enabled for the table.,"package main

import future.keywords.in

default allow = false

# Check for DynamoDB table creation or update
dynamodb_table_modified(resources) {
some resource in resources
resource.type == ""aws_dynamodb_table""
resource.change.actions[_] == ""create""
}

# Check if stream is enabled with new and old images
stream_configured(resource) {
resource.type == ""aws_dynamodb_table""
resource.change.after.stream_enabled == true
resource.change.after.stream_view_type == ""NEW_AND_OLD_IMAGES"" 
}

# Allow if DynamoDB table is modified and stream is configured correctly
allow {
dynamodb_table_modified(input.resource_changes)
some resource in input.resource_changes
stream_configured(resource)
}",2,"provider ""aws"" {
region = ""us-east-1"" # Change this to your desired AWS region
}

resource ""aws_dynamodb_table"" ""example"" {
name = ""your-dynamodb-table-name""
billing_mode = ""PAY_PER_REQUEST"" # Or ""PROVISIONED"" if you prefer provisioned capacity
hash_key = ""id""
attribute {
name = ""id""
type = ""S""
}

# Define other table settings as needed

stream_enabled = true # Enable streams
stream_view_type = ""NEW_AND_OLD_IMAGES"" # Choose the appropriate stream view type
}","Has one ""aws_dynamodb_table"" resource
with one ""stream_enabled"" = true
with one ""stream_view_type"" = ""NEW_AND_OLD_IMAGES"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_dynamodb_table"" ""example"" {
  name           = ""terraform-lock-table""
  billing_mode   = ""PROVISIONED""
  read_capacity  = 5
  write_capacity = 5
  hash_key       = ""LockID""

  attribute {
    name = ""LockID""
    type = ""S""
  }

  server_side_encryption {
    enabled = true
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'main': {'allow': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_vpc,Create an AWS VPC resource with an example CIDR block and IPv6 enabled,"package vpc_ipv6
import future.keywords.in

default valid := false

valid {
	some changed_resource in input.resource_changes
	changed_resource.type == ""aws_vpc""
	""create"" in changed_resource.change.actions
	args := changed_resource.change.after
	net.cidr_is_valid(args.cidr_block)
	args.assign_generated_ipv6_cidr_block == true
}
",1,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  assign_generated_ipv6_cidr_block = true
}
","Has one resource ""aws_vpc""        
with one ""cidr_block"" with any valid IPv4 address
with one ""assign_generated_ipv6_cidr_block"" with value ""true"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, change as needed
}

resource ""aws_vpc"" ""example"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true
  assign_generated_ipv6_cidr_block = true
}
```",True,Success,No error,No error,
"aws_internet_gateway, aws_vpc",Create an AWS VPC resource with an Internet Gateway attached to it,"package vpc_netgate_basic
import future.keywords.in

default valid := false

valid {
	some ig_resource in input.configuration.root_module.resources
	ig_resource.type == ""aws_internet_gateway""

	some vpc_resource in input.configuration.root_module.resources
	vpc_resource.type == ""aws_vpc""
	vpc_resource.address in ig_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_internet_gateway"" ""example_igw"" {
  vpc_id = aws_vpc.main.id
}","Has one resource ""aws_vpc""

Has one resource ""aws_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # You can change this region to your preferred one
}

resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""

  tags = {
    Name = ""terraform-example-vpc""
  }
}

resource ""aws_internet_gateway"" ""gw"" {
  vpc_id = aws_vpc.example.id

  tags = {
    Name = ""terraform-example-igw""
  }
}
```",True,Success,No error,No error,
"aws_egress_only_internet_gateway, aws_vpc",Create an AWS VPC with an Egress-Only Internet Gateway attached to it,"package vpc_egress_netgate
import future.keywords.in

default valid := false

valid {
	some eg_ig_resource in input.configuration.root_module.resources
	eg_ig_resource.type == ""aws_egress_only_internet_gateway""

	some vpc_resource in input.configuration.root_module.resources
	vpc_resource.type == ""aws_vpc""
	vpc_resource.address in eg_ig_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
}

resource ""aws_egress_only_internet_gateway"" ""example_egress_igw"" {
  vpc_id = aws_vpc.main.id
}","Has one resource ""aws_vpc""

Has one resource ""aws_egress_only_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""terraform-example""
  }
}

resource ""aws_egress_only_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id
}
```",True,Success,No error,No error,
"aws_subnet, aws_subnet, aws_vpc",Create an AWS VPC resource with one public subnet and one private subnet,"package vpc_two_subnet
import future.keywords.in

default valid := false

valid {
        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""

        some subnet1 in input.configuration.root_module.resources
        subnet1.type == ""aws_subnet""
        vpc_resource.address in subnet1.expressions.vpc_id.references

        some subnet2 in input.configuration.root_module.resources
        subnet2.type == ""aws_subnet""
        vpc_resource.address in subnet2.expressions.vpc_id.references

        not subnet1 == subnet2
}
",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""my_public_subnet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""my_public_subnet""
  }
}

resource ""aws_subnet"" ""my_private_subnet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  tags = {
    Name = ""my_private_subnet""
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has another resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it to your desired region
}

resource ""aws_vpc"" ""example"" {
  cidr_block       = ""10.0.0.0/16""
  instance_tenancy = ""default""

  tags = {
    Name = ""terraform-example-vpc""
  }
}

resource ""aws_subnet"" ""public"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""
  map_public_ip_on_launch = true

  tags = {
    Name = ""terraform-example-public-subnet""
  }
}

resource ""aws_subnet"" ""private"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.2.0/24""
  map_public_ip_on_launch = false

  tags = {
    Name = ""terraform-example-private-subnet""
  }
}
```",True,Success,No error,No error,
"aws_vpc, aws_vpc, aws_vpc_peering_connection",Create two AWS VPCs and establish a peering connection between them,"package vpc_peer_connect
import future.keywords.in

default valid := false        

valid {
        some vpc_resource1 in input.configuration.root_module.resources
        vpc_resource1.type == ""aws_vpc""
    
    some vpc_resource2 in input.configuration.root_module.resources
        vpc_resource2.type == ""aws_vpc""

    not vpc_resource1 == vpc_resource2
    
        some peer_connection_resource in input.configuration.root_module.resources
        peer_connection_resource.type == ""aws_vpc_peering_connection""
    vpc_resource1.address in peer_connection_resource.expressions.vpc_id.references
    vpc_resource2.address in peer_connection_resource.expressions.peer_vpc_id.references
}",2,"resource ""aws_vpc"" ""example_vpc1"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_vpc"" ""example_vpc2"" {
  cidr_block = ""10.1.0.0/16""
}

resource ""aws_vpc_peering_connection"" ""example_peer"" {
  vpc_id        = aws_vpc.example_vpc1.id
  peer_vpc_id   = aws_vpc.example_vpc2.id
  auto_accept   = true
}","Has two resources ""aws_vpc""

Has one resource ""aws_vpc_peering_connection""
with one ""vpc_id"" with value of one aws_vpc
with one ""peer_vpc_id"" with value of the other aws_vpc"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Choose an appropriate region
}

# First VPC
resource ""aws_vpc"" ""first_vpc"" {
  cidr_block = ""10.0.0.0/16""

  tags = {
    Name = ""FirstVPC""
  }
}

# Second VPC
resource ""aws_vpc"" ""second_vpc"" {
  cidr_block = ""10.1.0.0/16""

  tags = {
    Name = ""SecondVPC""
  }
}

# Peer the two VPCs together
resource ""aws_vpc_peering_connection"" ""peer_connection"" {
  vpc_id        = aws_vpc.first_vpc.id
  peer_vpc_id   = aws_vpc.second_vpc.id
  peer_region   = ""us-west-2"" # Assuming both VPCs are in the same region for simplicity
  auto_accept   = true         # Automatically accept the peering request
}
```",True,Success,No error,No error,
"aws_internet_gateway, aws_route_table, aws_vpc",Create an AWS VPC resource with an Internet Gateway attached and a route to a custom route table directing traffic for a specific CIDR block through the Internet Gateway,"package vpc_gateway_route
import future.keywords.in

default valid := false

valid {
    some route_resource in input.configuration.root_module.resources
        route_resource.type == ""aws_route_table""

        some ig_resource in input.configuration.root_module.resources
        ig_resource.type == ""aws_internet_gateway""   
    ig_resource.address in route_resource.expressions.route.references

        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""
        vpc_resource.address in ig_resource.expressions.vpc_id.references
        vpc_resource.address in route_resource.expressions.vpc_id.references
}",2,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_internet_gateway"" ""example_igw"" {
  vpc_id = aws_vpc.main.id
}

resource ""aws_route_table"" ""example_public_rt"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""10.0.1.0/24""
    gateway_id = aws_internet_gateway.example_igw.id
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_internet_gateway""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has one resource ""aws_route_table""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""
with one route"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_vpc"" ""example"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true
}

resource ""aws_internet_gateway"" ""gw"" {
  vpc_id = aws_vpc.example.id
}

resource ""aws_route_table"" ""custom_route_table"" {
  vpc_id = aws_vpc.example.id

  route {
    cidr_block = ""10.0.0.0/16""
    gateway_id = aws_internet_gateway.gw.id
  }
}
```",True,Success,No error,No error,
"aws_neptune_cluster, aws_neptune_cluster_parameter_group",Set up a basic AWS Neptune cluster with a custom parameter group,"package aws_neptune_cluster
import future.keywords.in

default neptune_cluster_valid := false
default cluster_parameter_group_valid := false

neptune_cluster_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""
    cluster_parameter_group_valid
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""

    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references

    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    cluster.expressions.engine_version.constant_value < ""1.2.0.0""
    cluster_parameter_group.expressions.family.constant_value == ""neptune1""   
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""

    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references

    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    cluster.expressions.engine_version.constant_value >= ""1.2.0.0""
    cluster_parameter_group.expressions.family.constant_value == ""neptune1.2""   
}

cluster_parameter_group_valid {
    some cluster in input.configuration.root_module.resources
    cluster.type == ""aws_neptune_cluster""
    
    some cluster_parameter_group in input.configuration.root_module.resources
    cluster_parameter_group.type == ""aws_neptune_cluster_parameter_group""
    cluster_parameter_group.address in cluster.expressions.neptune_cluster_parameter_group_name.references
    
    # See for more info: https://docs.aws.amazon.com/neptune/latest/userguide/parameter-groups.html
    not cluster.expressions.engine_version.constant_value # defaults as latest version
    cluster_parameter_group.expressions.family.constant_value == ""neptune1.2""   
}",2,"resource ""aws_neptune_cluster_parameter_group"" ""example"" {
  family      = ""neptune1.2""
  name        = ""example""
  description = ""neptune cluster parameter group""

  parameter {
    name  = ""neptune_enable_audit_log""
    value = 1
  }
}

resource ""aws_neptune_cluster"" ""default"" {
  cluster_identifier                  = ""neptune-cluster-demo""
  engine                              = ""neptune""
  backup_retention_period             = 5
  preferred_backup_window             = ""07:00-09:00""
  skip_final_snapshot                 = true
  iam_database_authentication_enabled = true
  apply_immediately                   = true
  neptune_cluster_parameter_group_name = aws_neptune_cluster_parameter_group.example.id
}","Has ""aws_neptune_cluster""
with ""neptune_cluster_parameter_group_name""

Has ""aws_neptune_cluster_parameter_group""
with family = ""neptune1.2"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_neptune_cluster"" ""example"" {
  cluster_identifier      = ""neptune-cluster-1""
  engine                  = ""neptune""
  database_name           = ""neptune_db""
  master_username         = ""admin""
  master_password         = ""password123!""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
}

resource ""aws_neptune_parameter_group"" ""example"" {
  name   = ""custom-neptune-params""
  family = ""neptune1""

  parameter {
    name  = "" NeptuneMaxResultSetSize""
    value = ""2048""
  }
}

resource ""aws_neptune_cluster_instance"" ""example"" {
  cluster_identifier = aws_neptune_cluster.example.id
  instance_class     = ""db.r5.large""
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 8, in resource ""aws_neptune_cluster"" ""example"":
   8:   database_name           = ""neptune_db""

An argument named ""database_name"" is not expected here.

Error: Unsupported argument

  on main.tf line 9, in resource ""aws_neptune_cluster"" ""example"":
   9:   master_username         = ""admin""

An argument named ""master_username"" is not expected here.

Error: Unsupported argument

  on main.tf line 10, in resource ""aws_neptune_cluster"" ""example"":
  10:   master_password         = ""password123!""

An argument named ""master_password"" is not expected here.
",None,Terraform plan failed.
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector. Then create an AWS Chime Voice Connector Logging resource with enable_sip_logs disabled and enable_media_metric_logs enabled,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_sip_logs.constant_value == false
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_sip_logs false
with enable_media_metric_logs true"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""example"" {
  name                = ""example-voice-connector""
  aws_region          = ""us-west-2""
  require_encryption  = true
  outgoing_caller_id = ""1234567890""
}

resource ""aws_chime_voice_connector_logging"" ""example"" {
  voice_connector_id       = aws_chime_voice_connector.example.id
  enable_sip_logs          = false
  enable_media_metric_logs = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 9, in resource ""aws_chime_voice_connector"" ""example"":
   9:   outgoing_caller_id = ""1234567890""

An argument named ""outgoing_caller_id"" is not expected here.
",None,Terraform plan failed.
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector. Then create an AWS Chime Voice Connector Logging resource with logging of ONLY media metrics,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_sip_logs.constant_value == false
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_sip_logs false
with enable_media_metric_logs true"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_chime_voice_connector"" ""example"" {
  name                = ""terraform-chime-vc""
  aws_region          = ""us-east-1""
  require_encryption  = true
}

resource ""aws_chime_voice_connector_logging"" ""media_metrics_log"" {
  voice_connector_id = aws_chime_voice_connector.example.id
  log_mode           = ""MediaMetrics""
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 13, in resource ""aws_chime_voice_connector_logging"" ""media_metrics_log"":
  13:   log_mode           = ""MediaMetrics""

An argument named ""log_mode"" is not expected here.
",None,Terraform plan failed.
"aws_chime_voice_connector, aws_chime_voice_connector_logging",Create an AWS Chime Voice Connector with encryption and log media metrics.,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_logging in input.configuration.root_module.resources
    vc_logging.type = ""aws_chime_voice_connector_logging""
    vc_logging.expressions.enable_media_metric_logs.constant_value == true
    vc.address in vc_logging.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_logging"" ""default"" {
  enable_sip_logs          = false
  enable_media_metric_logs = true
  voice_connector_id       = aws_chime_voice_connector.default.id
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=true

Has one resource ""aws_chime_voice_connector_logging""
with voice_connector_id equal to the id of the voice connector
with enable_media_metric_logs true"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""example"" {
  name               = ""example-voice-connector""
  require_encryption = true
  media_metrics      = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 8, in resource ""aws_chime_voice_connector"" ""example"":
   8:   media_metrics      = true

An argument named ""media_metrics"" is not expected here.
",None,Terraform plan failed.
"aws_chime_voice_connector, aws_chime_voice_connector_streaming","Create an AWS Chime Voice Connector with encryption. Then configure the streaming of the voice connector with retention period of 5 hours, notifications sent to SNS, and disable streaming to Amazon Kinesis.","package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_streaming in input.configuration.root_module.resources
    vc_streaming.type = ""aws_chime_voice_connector_streaming""
    vc_streaming.expressions.data_retention.constant_value == 5
    vc_streaming.expressions.disabled.constant_value == false
    ""SNS"" in vc_streaming.expressions.streaming_notification_targets.constant_value
    vc.address in vc_streaming.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_streaming"" ""default"" {
  disabled = false
  voice_connector_id  = aws_chime_voice_connector.default.id
  data_retention = 5
  streaming_notification_targets = [""SNS""]
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=true

Has one resource ""aws_chime_voice_connector_streaming""
with voice_connector_id equal to the id of the voice connector
with streaming_enabled = false
with data_retention = 5
streaming_notification_targets = [""SNS""]"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""example"" {
  name               = ""example-voice-connector""
  require_encryption = true
}

resource ""aws_chime_streaming_configuration"" ""example"" {
  voice_connector_id    = aws_chime_voice_connector.example.id
  streaming_notifications_endpoint = aws_sns_topic.chime_stream_notification.arn
  data_retention_in_hours = 5
  disabled                = true
}

resource ""aws_sns_topic"" ""chime_stream_notification"" {
  name = ""chime-stream-notification""
}
```",False,No opa_result,"
Error: Invalid resource type

  on main.tf line 10, in resource ""aws_chime_streaming_configuration"" ""example"":
  10: resource ""aws_chime_streaming_configuration"" ""example"" {

The provider hashicorp/aws does not support resource type
""aws_chime_streaming_configuration"".
",None,Terraform plan failed.
"aws_chime_voice_connector, aws_chime_voice_connector_streaming",Create an AWS Chime Voice Connector. Then set up a configuration to stream media to Amazon Kinesis.,"package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc in input.configuration.root_module.resources
    vc.type == ""aws_chime_voice_connector""
    vc.expressions.require_encryption.constant_value == true

    some vc_streaming in input.configuration.root_module.resources
    vc_streaming.type = ""aws_chime_voice_connector_streaming""
    vc.address in vc_streaming.expressions.voice_connector_id.references
}",2,"resource ""aws_chime_voice_connector"" ""default"" {
  name               = ""vc-name-test""
  require_encryption = true
}

resource ""aws_chime_voice_connector_streaming"" ""default"" {
  voice_connector_id  = aws_chime_voice_connector.default.id
  data_retention = 5
}","Has one resource ""aws_chime_voice_connector""

Has one resource ""aws_chime_voice_connector_streaming""
with voice_connector_id equal to the id of the voice connector
with data_retention to some number"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""example"" {
  name               = ""example-vc""
  aws_region         = ""us-west-2""
  require_encryption = true
}

data ""aws_iam_policy_document"" ""kinesis_access"" {
  statement {
    actions   = [""kinesis:PutRecord"", ""kinesis:PutRecords"", ""kinesis:DescribeStream"", ""kinesis:GetShardIterator"", ""kinesis:GetRecords""]
    resources = [""*""]
  }
}

resource ""aws_iam_policy"" ""kinesis_access"" {
  name   = ""KinesisAccessPolicy""
  policy = data.aws_iam_policy_document.kinesis_access.json
}

resource ""aws_iam_role"" ""kinesis_streamer"" {
  name = ""kinesis-streamer-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""kinesis.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""kinesis_access_attachment"" {
  role       = aws_iam_role.kinesis_streamer.name
  policy_arn = aws_iam_policy.kinesis_access.arn
}

resource ""aws_kinesis_stream"" ""media_stream"" {
  name        = ""media-stream""
  shard_count = 1

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_chime_voice_connector': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_iam_user, aws_iam_user_ssh_key",Create a basic AWS IAM user with a basic SSH key attached.,"package iam_user_ssh
import future.keywords.in

default valid := false

valid {
    some user_ssh_resource in input.configuration.root_module.resources
    user_ssh_resource.type == ""aws_iam_user_ssh_key""

    some user_resource in input.configuration.root_module.resources
    user_resource.type == ""aws_iam_user""
    user_resource.address in user_ssh_resource.expressions.username.references
}",2,"resource ""aws_iam_user"" ""user"" {
  name = ""test-user""
  path = ""/""
}

resource ""aws_iam_user_ssh_key"" ""user"" {
  username   = aws_iam_user.user.name
  encoding   = ""SSH""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 mytest@mydomain.com""
}","Has one ""aws_iam_user"" resource
with one ""name"" with any value

Has one ""aws_iam_user_ssh_key"" resource
with one ""username"" with value ""aws_iam_user.{USER NAME}.name""
with one ""encoding"" with value ""SSH""
with one ""public_key"" with any value"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_iam_user"" ""basic_user"" {
  name = ""basic_user""
}

resource ""aws_iam_user_ssh_key"" ""basic_key"" {
  username   = aws_iam_user.basic_user.name
  encoding   = ""SSH""
  public_key = file(""~/.ssh/id_rsa.pub"")
}
```",True,Success,No error,No error,
aws_iam_group,Create a basic AWS IAM group example,"package iam_group_basic
import future.keywords.in

default valid := false

valid {
    some group_resource in input.configuration.root_module.resources
    group_resource.type == ""aws_iam_group""
}",1,"resource ""aws_iam_group"" ""group"" {
  name = ""my-group""
  path = ""/users/""
}","Has one ""aws_iam_group"" resource
with one ""name"" with any value"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_iam_group"" ""example"" {
  name = ""example-group""
}
```",True,Success,No error,No error,
aws_iam_user,Create a basic AWS IAM user example,"package iam_user_basic
import future.keywords.in

default valid := false

valid {
	some user_resource in input.configuration.root_module.resources
	user_resource.type == ""aws_iam_user""
}",1,"resource ""aws_iam_user"" ""lb"" {
  name = ""loadbalancer""
  path = ""/system/""

  tags = {
    tag-key = ""tag-value""
  }
}","Has one ""aws_iam_user"" resource
with one ""name"" with any value"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_iam_user"" ""example"" {
  name = ""example_user""
}
```",True,Success,No error,No error,
aws_iam_virtual_mfa_device,Create a basic AWS IAM Virtual MFA Device resource,"package iam_mfa_basic
import future.keywords.in

default valid := false

valid {
	some mfa in input.configuration.root_module.resources
	mfa.type == ""aws_iam_virtual_mfa_device""
}",1,"resource ""aws_iam_virtual_mfa_device"" ""example"" {
  virtual_mfa_device_name = ""example""
}","Has one ""aws_iam_virtual_mfa_device"" resource
with one ""virtual_mfa_device_name"" with any string"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Replace with your desired region
}

resource ""aws_iam_virtual_mfa_device"" ""example"" {
  virtual_mfa_device_name = ""example-mfa""
  path                    = ""/""
}
```",True,Success,No error,No error,
aws_s3_bucket,Create a S3 bucket with an example name,"package aws_bucket
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
}",1,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-tf-test-bucket""
}",Has an aws_s3_bucket resource," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""example-bucket-${random_string.suffix.result}""
}

resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}
```",True,Success,No error,No error,
aws_s3_bucket,Create an AWS resource to help me store images that I want to display on my website,"package aws_s3
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
}",1,"resource ""aws_s3_bucket"" ""example-bucket"" {
  bucket = ""test-bucket""
}",Has an aws_s3_bucket resource," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""image_storage"" {
  bucket = ""your-unique-bucket-name""
  acl    = ""private""

  tags = {
    Name        = ""Image Storage Bucket""
    Environment = ""Production""
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_accelerate_configuration","Create S3 bucket with bucket name = ""mybucket"" and attach an acclerate configuration resource for the bucket with enabled status.","package s3_bucket_accelerate
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some accelerate_config in input.configuration.root_module.resources
        accelerate_config.type == ""aws_s3_bucket_accelerate_configuration""
        bucket.address in accelerate_config.expressions.bucket.references
        accelerate_config.expressions.status.constant_value == ""Enabled""
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some accelerate_config in input.configuration.root_module.resources
        accelerate_config.type == ""aws_s3_bucket_accelerate_configuration""
        accelerate_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        accelerate_config.expressions.status.constant_value == ""Enabled""
}
",2,"resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_accelerate_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id
  status = ""Enabled""
}","Has an aws_s3_bucket resource

Has an aws_bucket_accelerate_configuration resource
with bucket = bucked id OR name
with status = ""Enabled"" (case sensitive)"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
  acl    = ""private""

  tags = {
    Name        = ""mybucket""
    Environment = ""Development""
  }
}

resource ""aws_s3_bucket_accelerate_configuration"" ""mybucket_accelerate"" {
  bucket = aws_s3_bucket.mybucket.id
  status = ""Enabled""
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_ownership_controls","Create S3 bucket with bucket name = ""mybucket"" and set the ownership control of the S3 bucket to be bucket owner preferred.
Then create an ACL resource that depends on ownership control and makes bucket private. Use bucket references, NOT the name of the bucket.","package s3_bucket_acl
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    bucket.address in ownership_controls.expressions.bucket.references
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""

    some acl in input.configuration.root_module.resources
    acl.type == ""aws_s3_bucket_acl""
    bucket.address in acl.expressions.bucket.references
    acl.expressions.acl.constant_value == ""private""
    ownership_controls.address in acl.depends_on
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]

  bucket = aws_s3_bucket.example.id
  acl    = ""private""
}
","Has an aws_s3_bucket resource

Has an aws_s3_bucket_ownership_controls
with bucket = bucket id
and rule with object_ownership = ""BucketOwnerPreferred""

Has an aws_s3_bucket_acl"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
  acl    = ""private""

  versioning {
    enabled = true
  }

  lifecycle_rule {
    id      = ""log-expiration""
    enabled = true

    transition {
      days          = 30
      storage_class = ""STANDARD_IA""
    }

    expiration {
      days = 90
    }
  }
}

resource ""aws_s3_bucket_ownership_controls"" ""mybucket"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""mybucket"" {
  depends_on = [aws_s3_bucket_ownership_controls.mybucket]
  bucket     = aws_s3_bucket.mybucket.id
  acl        = ""private""
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_analytics_configuration","Create S3 bucket with bucket name = ""mybucket"". Add analytics configuration for entire S3 bucket and export results to a second S3 bucket. Use bucket references, NOT the names of the buckets.","package s3_bucket_analytics
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some other_bucket in input.configuration.root_module.resources
        other_bucket.type == ""aws_s3_bucket""
        not bucket == other_bucket

        some analytics in input.configuration.root_module.resources
        analytics.type == ""aws_s3_bucket_analytics_configuration""
        bucket.address in analytics.expressions.bucket.references
    some export in analytics.expressions.storage_class_analysis
    some dest in export.data_export
    some bucket_dest in dest.destination
    some arn in bucket_dest.s3_bucket_destination
    other_bucket.address in arn.bucket_arn.references
}",2,"resource ""aws_s3_bucket_analytics_configuration"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""

  storage_class_analysis {
    data_export {
      destination {
        s3_bucket_destination {
          bucket_arn = aws_s3_bucket.analytics.arn
        }
      }
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""analytics"" {
  bucket = ""analytics destination""
}","Has an aws_s3_bucket resource

Has an aws_bucket_analytics_configuration resource
with bucket = bucked id
and storage_class_analysis with (refer to desired output)"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_analytics_configuration"" ""analytics"" {
  bucket = aws_s3_bucket.mybucket.id
  name   = ""analytics""

  storage_class_analysis {
    data_export_details {
      destination {
        s3_bucket_arn = aws_s3_bucket.mybucket.arn
      }
    }
  }
}
```",False,No opa_result,"
Error: Insufficient data_export blocks

  on main.tf line 13, in resource ""aws_s3_bucket_analytics_configuration"" ""analytics"":
  13:   storage_class_analysis {

At least 1 ""data_export"" blocks are required.

Error: Unsupported block type

  on main.tf line 14, in resource ""aws_s3_bucket_analytics_configuration"" ""analytics"":
  14:     data_export_details {

Blocks of type ""data_export_details"" are not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_intelligent_tiering_configuration","Create S3 bucket with bucket name = ""mybucket"". Add intelligent tiering configuration resource for the bucket.","package s3_bucket_intelligent_tiering
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some intelligent_tiering in input.configuration.root_module.resources
    intelligent_tiering.type == ""aws_s3_bucket_intelligent_tiering_configuration""
    bucket.address in intelligent_tiering.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some intelligent_tiering in input.configuration.root_module.resources
    intelligent_tiering.type == ""aws_s3_bucket_intelligent_tiering_configuration""
    intelligent_tiering.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket_intelligent_tiering_configuration"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""

  tiering {
    access_tier = ""DEEP_ARCHIVE_ACCESS""
    days        = 180
  }
  tiering {
    access_tier = ""ARCHIVE_ACCESS""
    days        = 125
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_intelligent_tiering
with bucket = bucked id OR name
and correct tiering attributes"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_intelligent_tiering_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id
  name   = ""intelligent-tiering-config""

  tiering {
    days        = 30
    access_tier = ""ARCHIVE_ACCESS""
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_metric","Create a S3 bucket with bucket name = ""mybucket"" and a bucket metric resource that adds metrics configuration for the entire bucket.","package s3_bucket_metric
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    bucket.address in metrics.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    metrics.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_metric"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_metric
with bucket = bucked id OR name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""

  versioning {
    enabled = true
  }

  metric {
    name = ""AllMetrics""
    filter {
      prefix = """"
      tags  = {}
    }
  }
}
```",False,No opa_result,"
Error: Unsupported block type

  on main.tf line 12, in resource ""aws_s3_bucket"" ""mybucket"":
  12:   metric {

Blocks of type ""metric"" are not expected here.
",None,Terraform plan failed.
"aws_s3_bucket, aws_s3_bucket_notification, aws_sns_topic",Create an AWS S3 bucket and send a SNS notification whenever a .log object is created in the bucket. Do not include policies required for this. ,"package aws_s3_bucket_notification
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    
    some sns_topic in input.configuration.root_module.resources
    sns_topic.type == ""aws_sns_topic""

    some notification in input.configuration.root_module.resources
    notification.type = ""aws_s3_bucket_notification""
    bucket.address in notification.expressions.bucket.references
    some topic in notification.expressions.topic
    some event in topic.events.constant_value
    event == ""s3:ObjectCreated:*""
    topic.filter_suffix.constant_value == "".log""
    sns_topic.address in topic.topic_arn.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some sns_topic in input.configuration.root_module.resources
    sns_topic.type == ""aws_sns_topic""

    some notification in input.configuration.root_module.resources
    notification.type = ""aws_s3_bucket_notification""
    bucket.expressions.bucket.constant_value == notification.expressions.bucket.constant_value
    some topic in notification.expressions.topic
    some event in topic.events.constant_value
    event == ""s3:ObjectCreated:*""
    topic.filter_suffix.constant_value == "".log""
    sns_topic.address in topic.topic_arn.references
}",2,"resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
}

resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_sns_topic""

Has one resource ""aws_s3_bucket_notification""
with bucket = bucket id OR name
with topic
    with topic arn = sns topic arn 
    and events = [""s3:ObjectCreated:*""]
    and filter_suffix = "".log"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-log-bucket""
}

resource ""aws_sns_topic"" ""log_notification"" {
  name = ""log-notification-topic""
}

resource ""aws_s3_bucket_notification"" ""log_notification"" {
  bucket = aws_s3_bucket.example.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.process_logs.arn
    events              = [""s3:ObjectCreated:*""]
  }
}

resource ""aws_lambda_function"" ""process_logs"" {
  filename      = ""lambda_function.zip""
  function_name = ""process_log_files""
  role          = aws_iam_role.lambda_exec.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

resource ""aws_iam_role"" ""lambda_exec"" {
  name = ""lambda-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_s3_bucket_notification': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_object","Create S3 bucket with bucket name = ""mybucket"". Then create a bucket object resource for a file at source location ""path/to/file"" and upload the object to the bucket.","package s3_bucket_object
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_bucket_object""
        bucket.address in object.expressions.bucket.references
        object.expressions.key
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_bucket_object""
        object.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        object.expressions.key
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}
resource ""aws_s3_bucket_object"" ""object"" {
  bucket = ""your_bucket_name""
  key    = ""new_object_key""
  source = ""path/to/file""
}","Has an aws_s3_bucket resource

Has an aws_s3_object resource
with bucket = bucket id OR name
with source = ""path/to/file"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_object"" ""file"" {
  bucket = aws_s3_bucket.mybucket.id
  key    = ""path/to/file""
  source = ""path/to/file""
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_object","I have a PDF with path ""assets/test.pdf"". Make an AWS resource to store this PDF and upload it.","package aws_s3_object
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_object""
        object.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
        object.expressions.source.constant_value == ""assets/test.pdf""
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some object in input.configuration.root_module.resources
        object.type == ""aws_s3_object""
        bucket.address in object.expressions.bucket.references
        object.expressions.source.constant_value == ""assets/test.pdf""
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""test-bucket""
}

resource ""aws_s3_object"" ""object"" {
  bucket = ""my-bucket""
  key    = ""new_object_key""
  source = ""assets/test.pdf""
}","Has an aws_s3_bucket resource

Has an aws_s3_object resource
with bucket = bucket id OR name
with source = ""assets/test.pdf"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it as needed
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""test-pdf-storage-bucket""
}

resource ""aws_s3_object"" ""test_pdf"" {
  bucket = aws_s3_bucket.example.id
  key    = ""assets/test.pdf""
  source = ""assets/test.pdf""
  acl    = ""private""
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_object_lock_configuration","Create a S3 bucket with bucket name = ""mybucket"" and object lock enabled. Then, add an object lock configuration resource for the new bucket.","package s3_bucket_object_lock
import future.keywords.in

default valid := false

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""
        bucket.expressions.object_lock_enabled.constant_value == true

        some object_lock in input.configuration.root_module.resources
        object_lock.type == ""aws_s3_bucket_object_lock_configuration""
        bucket.address in object_lock.expressions.bucket.references
}

valid {
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""
        bucket.expressions.bucket.constant_value == ""mybucket""
        bucket.expressions.object_lock_enabled.constant_value == true

        some object_lock in input.configuration.root_module.resources
        object_lock.type == ""aws_s3_bucket_object_lock_configuration""
        object_lock.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""

  object_lock_enabled = true
}

resource ""aws_s3_bucket_object_lock_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    default_retention {
      mode = ""COMPLIANCE""
      days = 5
    }
  }
}","Has an aws_s3_bucket resource
with object_lock_enabled = true

Has an aws_s3_object_lock resource
with bucket = bucket id OR name
and a correct rule"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""

  object_lock_configuration {
    object_lock_enabled = ""Enabled""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'s3_bucket_object_lock': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_ownership_controls","Create a S3 bucket with bucket name = ""mybucket"". And create a bucket ownership controls resource to set object ownership to bucket owner preferred.","package s3_bucket_ownership_controls
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""

    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    bucket.address in ownership_controls.expressions.bucket.references
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""

    some ownership_controls in input.configuration.root_module.resources
    ownership_controls.type == ""aws_s3_bucket_ownership_controls""
    ownership_controls.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    some rule in ownership_controls.expressions.rule
    rule.object_ownership.constant_value == ""BucketOwnerPreferred""
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_ownership_controls
with bucket = bucket id OR name
and rule with object_ownership = ""BucketOwnerPreferred"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, you can change it as needed
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```",True,Success,No error,No error,
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration",Create an AWS S3 bucket and apply KMS server side encryption that uses a defined KMS key resource.,"package aws_s3_bucket_sse
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.address in sse.expressions.bucket.references
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.expressions.bucket.constant_value == sse.expressions.bucket.constant_value
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
    
}",2,"resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_kms_key""

Has one resource ""aws_s3_bucket_server_side_encryption""
with bucket = bucket id OR name
with rule
    with apply_server_side_encryption_by_default
        with kms_master_key_id = kms key id
        and sse_algorithm = ""aws:kms"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, adjust as needed
}

resource ""aws_kms_key"" ""s3_encryption_key"" {
  description             = ""KMS Key for S3 Encryption""
  deletion_window_in_days = 7
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""terraform-example-bucket""

  tags = {
    Name        = ""Terraform Example Bucket""
    Environment = ""dev""
  }
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_encryption_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_cors_configuration",Create a S3 bucket and a bucket CORS configuration resource that attaches an example CORS rule to the bucket. Make sure the CORS rule has all of the required attributes.,"package aws_s3_bucket_cors
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
	some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

	some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    
    bucket.address in cors_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
	some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

	some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    
	cors_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_cors_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  cors_rule {
    allowed_headers = [""*""]
    allowed_methods = [""PUT"", ""POST""]
    allowed_origins = [""https://s3-website-test.hashicorp.com""]
    expose_headers  = [""ETag""]
    max_age_seconds = 3000
  }

  cors_rule {
    allowed_methods = [""GET""]
    allowed_origins = [""*""]
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_cors_configuration"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_cors_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  cors_rule {
    allowed_methods = [""GET"", ""POST""]
    allowed_origins = [""https://example.com""]
    allowed_headers = [""*""]
    expose_headers  = [""ETag""]
    max_age_seconds = 3600
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_cors_configuration","Create a S3 bucket with a CORS configuration that allows POSTs and GETs from my website ""https://domain.com"".","package aws_s3_bucket_cors
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    some rule in cors_config.expressions.cors_rule
    some method1 in rule.allowed_methods.constant_value
    method1 == ""POST""
    some method2 in rule.allowed_methods.constant_value
    method2 == ""GET""
    some origin in rule.allowed_origins.constant_value
    origin == ""https://domain.com""
    
    bucket.address in cors_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some cors_config in input.configuration.root_module.resources
    cors_config.type == ""aws_s3_bucket_cors_configuration""
    some rule in cors_config.expressions.cors_rule
    some method1 in rule.allowed_methods.constant_value
    method1 == ""POST""
    some method2 in rule.allowed_methods.constant_value
    method2 == ""GET""
    some origin in rule.allowed_origins.constant_value
    origin == ""https://domain.com""
    
    cors_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_cors_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  cors_rule {
    allowed_headers = [""*""]
    allowed_methods = [""GET"", ""POST""]
    allowed_origins = [""https://domain.com""]
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_cors_configuration
with bucket = bucket id OR name
with cors rule {
    allowed_headers = [""*""]
    allowed_methods = [""GET"", ""POST""]
    allowed_origins = [""https://domain.com""]
  }"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my-website-bucket""

  tags = {
    Name        = ""My Website Bucket""
    Environment = ""Production""
  }
}

resource ""aws_s3_bucket_cors_configuration"" ""my_cors_config"" {
  bucket = aws_s3_bucket.my_bucket.id

  cors_rule {
    allowed_headers = [""Authorization"", ""Content-Length""]
    allowed_methods = [""POST"", ""GET""]
    allowed_origins = [""https://domain.com""]
    expose_headers  = [""ETag""]
    max_age_seconds = 3600
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_website_configuration",Create a S3 bucket and an example website configuration resource for the S3 bucket.,"package aws_s3_bucket_website
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""

    bucket.address in website.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""

    website.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}
",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
}","Has an aws_s3_bucket

Has an aws_s3_bucket_website_configuration
with bucket = bucket id OR name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-website-bucket""

  tags = {
    Name        = ""Example Website Bucket""
    Environment = ""Dev""
  }
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.bucket

  index_document {
    suffix = ""index.html""
  }

  error_document {
    key = ""error.html""
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_website_configuration","Create a S3 bucket and host a static website. The website should use ""index.html"" in my bucket as the index page.","package aws_s3_bucket_website
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""
    some index in website.expressions.index_document
    index.suffix.constant_value == ""index.html""

    bucket.address in website.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some website in input.configuration.root_module.resources
    website.type == ""aws_s3_bucket_website_configuration""
    some index in website.expressions.index_document
    index.suffix.constant_value == ""index.html""

    website.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}
",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  index_document {
    suffix = ""index.html""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_website_configuration
with bucket = bucket id OR name
with index_document = { suffix = ""index.html"" }"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Replace with your desired region
}

resource ""aws_s3_bucket"" ""example"" {
  bucket        = ""your-unique-bucket-name"" # Replace with a unique bucket name
  force_destroy = true
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  index_document {
    suffix = ""index.html""
  }
}

resource ""aws_s3_bucket_policy"" ""allow_access_from_another_account"" {
  bucket = aws_s3_bucket.example.id
  policy = data.aws_iam_policy_document.allow_access.json
}

data ""aws_iam_policy_document"" ""allow_access"" {
  statement {
    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }
    actions   = [""s3:GetObject""]
    resources = [""${aws_s3_bucket.example.arn}/*""]
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_public_access_block",Create a S3 bucket and an example public access block resource for the S3 bucket.,"package aws_s3_bucket_public_access_block

import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some public_access in input.configuration.root_module.resources
        public_access.type == ""aws_s3_bucket_public_access_block""

        bucket.address in public_access.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some public_access in input.configuration.root_module.resources
        public_access.type == ""aws_s3_bucket_public_access_block""

        public_access.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = aws_s3_bucket.example.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}","Has an aws_s3_bucket

Has an aws_s3_bucket_public_access
with bucket = bucket id OR name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""your-unique-bucket-name""

  tags = {
    Name        = ""example-s3-bucket""
    Environment = ""development""
  }
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = aws_s3_bucket.example.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket-terraformai""

  tags = {
    Name        = ""Example Bucket""
    Environment = ""Dev""
  }
}

resource ""aws_s3_bucket_website_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id

  index_document {
    suffix = ""index.html""
  }

  error_document {
    key = ""error.html""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_s3_bucket_payment_config': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket with the bucket owner paying for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket-tf""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_s3_bucket_payment_config': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket where the bucket owner pays for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, adjust as necessary
}

resource ""aws_s3_bucket"" ""example"" {
  bucket        = ""terraform-example-bucket""
  force_destroy = true

  tags = {
    Name        = ""TerraformExampleBucket""
    Environment = ""Dev""
  }
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_s3_bucket_payment_config': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket_versioning",Create a S3 bucket and an example versioning resource for the S3 bucket.,"package aws_s3_bucket_versioning
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""

    bucket.address in versioning.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""
    
    versioning.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_versioning"" ""versioning_example"" {
  bucket = aws_s3_bucket.example.id
  versioning_configuration {
    status = ""Enabled""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_versioning
with bucket = bucket id OR name"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""your-unique-bucket-name""

  tags = {
    Name        = ""example-s3-bucket""
    Environment = ""dev""
  }
}

resource ""aws_s3_bucket_versioning"" ""example"" {
  bucket = aws_s3_bucket.example.id
  versioning_configuration {
    status = ""Enabled""
  }
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket_versioning",Create a S3 bucket with versioning disabled.,"package aws_s3_bucket_versioning
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""

    bucket.address in versioning.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some versioning in input.configuration.root_module.resources
    versioning.type == ""aws_s3_bucket_versioning""
    
    versioning.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example-bucket""
}

resource ""aws_s3_bucket_versioning"" ""versioning_example"" {
  bucket = aws_s3_bucket.example.id
  versioning_configuration {
    status = ""Disabled""
  }
}","Has an aws_s3_bucket

Has an aws_s3_bucket_versioning
with bucket = bucket id OR name
with versioning_configuration = { status = ""Disabled"" } "," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-unique-bucket-name""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'aws_s3_bucket_versioning': {'valid': False}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_logging","Create a S3 bucket and a second S3 bucket. Then create an example logging resource for the first S3 bucket that stores logs in the second bucket. Make sure the log object keys have a prefix of ""log/"".","package s3_bucket_logging
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""log_bucket"" {
  bucket = ""mylogbucket""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket = aws_s3_bucket.example.id

  target_bucket = aws_s3_bucket.log_bucket.id
  target_prefix = ""log/""
}","Has two aws_s3_bucket

Has an aws_s3_bucket_logging
with bucket = bucket id OR name
with target_bucket = second bucket id OR name
with target_prefix = ""log/"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Replace with your desired region
}

resource ""aws_s3_bucket"" ""first_bucket"" {
  bucket = ""my-first-s3-bucket""
}

resource ""aws_s3_bucket"" ""second_bucket"" {
  bucket = ""my-second-s3-bucket""
}

resource ""aws_s3_bucket_logging"" ""log_bucket"" {
  bucket        = aws_s3_bucket.first_bucket.id
  target_bucket = aws_s3_bucket.second_bucket.id
  target_prefix = ""log/""
}
```",True,Success,No error,No error,
"aws_s3_bucket, aws_s3_bucket, aws_s3_bucket_logging",Create a S3 bucket and a configuration that stores server access logs into a second S3 bucket.,"package s3_bucket_logging
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    other_bucket.address in logging.expressions.target_bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    bucket.address in logging.expressions.bucket.references
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""

    some other_bucket in input.configuration.root_module.resources
    other_bucket.type == ""aws_s3_bucket""
    not bucket == other_bucket

    some logging in input.configuration.root_module.resources
    logging.type == ""aws_s3_bucket_logging""
    logging.expressions.target_prefix.constant_value == ""log/""
    
    logging.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
    logging.expressions.target_bucket.constant_value == other_bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket"" ""log_bucket"" {
  bucket = ""mylogbucket""
}

resource ""aws_s3_bucket_logging"" ""example"" {
  bucket = aws_s3_bucket.example.id

  target_bucket = aws_s3_bucket.log_bucket.id
  target_prefix = ""log/""
}","Has two aws_s3_bucket

Has an aws_s3_bucket_logging
with bucket = bucket id OR name
with target_bucket = second bucket id OR name
with target_prefix = ""log/"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, change as needed
}

resource ""aws_s3_bucket"" ""main"" {
  bucket = ""your-unique-bucket-name""

  tags = {
    Environment = ""production""
  }
}

resource ""aws_s3_bucket"" ""logs"" {
  bucket = ""your-logs-bucket-name""

  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = ""AES256""
      }
    }
  }
}

resource ""aws_s3_bucket_policy"" ""logs"" {
  bucket = aws_s3_bucket.logs.id

  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Sid       = ""AllowLogBucket"",
        Effect    = ""Allow"",
        Principal = ""*"",
        Action    = [""s3:PutObject""],
        Resource  = ""${aws_s3_bucket.logs.arn}/*""
      },
      {
        Sid       = ""DenyIncorrectEncryptionHeader"",
        Effect    = ""Deny"",
        Principal = ""*"",
        Action    = [""s3:PutObject""],
        Condition = {
          StringNotEquals = {
            ""s3:x-amz-server-side-encryption"" : ""AES256""
          }
        },
        Resource  = ""${aws_s3_bucket.logs.arn}/*""
      },
      {
        Sid       = ""DenyUnencryptedRequests"",
        Effect    = ""Deny"",
        Principal = ""*"",
        Action    = [""s3:PutObject""],
        Condition = {
          Bool = {
            ""aws:SecureTransport"" : ""false""
          }
        },
        Resource  = ""${aws_s3_bucket.logs.arn}/*""
      }
    ]
  })
}

resource ""aws_s3_bucket_logging"" ""main"" {
  bucket = aws_s3_bucket.main.id
  target_bucket = aws_s3_bucket.logs.id
  target_prefix = ""log/""
}
```",True,Success,No error,No error,
aws_lightsail_instance,generate Basic Amazon Lightsail,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""basic_lamp"" {
  name              = ""basic-lamp""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux""
  bundle_id         = ""nano_2_0""

  user_data = <<-EOF
              #!/bin/bash
              sudo yum update -y
              sudo yum install -y httpd
              sudo systemctl start httpd
              sudo systemctl enable httpd
              EOF
}
```
",True,Success,No error,No error,
aws_lightsail_instance,generate Basic Amazon Lightsail with user Data,"package aws_lightsail_instance

# aws_lightsail_instance attributes - optional

default lightsail_instance_valid := false

lightsail_instance_valid {
	instance := input.configuration.root_module.resources[_]
	instance.type == ""aws_lightsail_instance""
    expressions := instance.expressions

    # Validate the presence of required arguments
    expressions.name
    expressions.availability_zone
    expressions.blueprint_id
    expressions.bundle_id
    expressions.user_data
}

# You can add more specific validations here if needed.
# For example, if you want to check if the name follows a specific format,
# you could write a rule like this:

name_is_valid {
	instance := input.configuration.root_module.resources[_]
	instance.type == ""aws_lightsail_instance""
	regex.match(""^[a-zA-Z0-9-_]+$"", instance.expressions.name.constant_value)
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
  user_data         = ""sudo yum install -y httpd && sudo systemctl start httpd && sudo systemctl enable httpd && echo '<h1>Deployed via Terraform</h1>' | sudo tee /var/www/html/index.html""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with ""user_data"" argument"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""basic_instance"" {
  name        = ""basic-lightsail-instance""
  availability_zone = ""us-east-1a""
  blueprint_id    = ""ubuntu_20_04""
  bundle_id       = ""nano_500mb""
  user_data       = <<EOF
#!/bin/bash
sudo apt update -y
sudo apt install -y apache2
sudo systemctl enable apache2
sudo systemctl start apache2
echo '<html><body><h1>Hello from Terraform on Lightsail!</h1></body></html>' | sudo tee /var/www/html/index.html
EOF
}
```",True,Success,No error,No error,
aws_lightsail_instance,generate aws lightsail with auto snapshots enabled,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
    has_valid_add_on(resource.values.add_on)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}

has_valid_add_on(add_ons) {
    add_on := add_ons[_]
    add_on.type == ""AutoSnapshot""
    add_on.snapshot_time
    add_on.status == ""Enabled""
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""custom_instance""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
  add_on {
    type          = ""AutoSnapshot""
    snapshot_time = ""06:00""
    status        = ""Enabled""
  }
  tags = {
    foo = ""bar""
  }
}","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" argument
with ""bundle_id"" argument
with ""add_on"" argument
    with type = ""AutoSnapshot""
    with snapshot_time argument
    with status = ""Enabled"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""example"" {
  name        = ""terraform-lightsail-instance""
  availability_zone = ""us-east-1a""
  blueprint_id = ""ubuntu_18_04""
  bundle_id   = ""nano_2_0""
}

resource ""aws_lightsail_disk"" ""auto_snapshot"" {
  name        = ""${aws_lightsail_instance.example.name}-auto-snapshot""
  size_in_gb  = 32
  attached_to = aws_lightsail_instance.example.id
}

resource ""aws_lightsail_disk_attachment"" ""auto_snapshot"" {
  disk_name   = aws_lightsail_disk.auto_snapshot.name
  instance_name = aws_lightsail_instance.example.name
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_disk"" ""auto_snapshot"":
  12: resource ""aws_lightsail_disk"" ""auto_snapshot"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 15, in resource ""aws_lightsail_disk"" ""auto_snapshot"":
  15:   attached_to = aws_lightsail_instance.example.id

An argument named ""attached_to"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_instance,create AWS Lightsail with default blueprint,"package terraform.validation

default is_valid_lightsail_instance = false

is_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    has_required_arguments(resource.values)
}

has_required_arguments(values) {
    values.name
    values.availability_zone
    values.blueprint_id
    values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""gitlab_test"" {
  name              = ""custom_gitlab""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" argument
with ""bundle_id"" argument"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""default"" {
  name              = ""default-lightsail-instance""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux""
  bundle_id         = ""nano_1_0""
}
```",True,Success,No error,No error,
aws_lightsail_instance,create AWS Lightsail with WordPress blueprint,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id == ""wordpress""
    resource.values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
 }","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" = wordpress
with ""bundle_id"" argument"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""wordpress"" {
  name              = ""wordpress-blueprint""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon-linux-2""
  bundle_id         = ""nano_1_0""
}

resource ""aws_lightsail_static_ip"" ""wordpress_ip"" {
  name = ""wordpress-blueprint-ip""
}

resource ""aws_lightsail_static_ip_attachment"" ""attach_ip"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_ip.id
  instance_name  = aws_lightsail_instance.wordpress.id
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_valid_lightsail_instance': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_lightsail_instance, aws_lightsail_static_ip, aws_lightsail_static_ip_attachment",create Amazon Lightsail with static ipv4 IP,"package terraform.validation

default has_required_resources = false

# Rule for aws_lightsail_instance resource
has_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
}

# Rule for aws_lightsail_static_ip resource
has_lightsail_static_ip {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_static_ip""
    resource.values.name
}

# Rule for aws_lightsail_static_ip_attachment resource considering expressions
has_lightsail_static_ip_attachment {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_lightsail_static_ip_attachment""
    static_ip_name_exists(resource)
    instance_name_exists(resource)
}

static_ip_name_exists(resource) {
    resource.expressions.static_ip_name.references[_]
}

instance_name_exists(resource) {
    resource.expressions.instance_name.references[_]
}

# Combined rule to ensure all conditions are met
has_required_resources {
    has_lightsail_instance
    has_lightsail_static_ip
    has_lightsail_static_ip_attachment
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_static_ip_attachment"" ""test"" {
  static_ip_name = aws_lightsail_static_ip.test.id
  instance_name  = aws_lightsail_instance.test.id
}

resource ""aws_lightsail_static_ip"" ""test"" {
  name = ""example""
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""custom_gitlab""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""
}","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument

has one ""aws_lightsail_static_ip"" resource
    with ""name"" argument

has one ""aws_lightsail_static_ip_attachment"" resource
    with ""static_ip_name"" argument
    with ""instance_name"" argument"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_lightsail_instance"" ""static_ipv4"" {
  name        = ""static-ipv4-instance""
  bundle_id   = ""nano_1_0""
  user_data   = ""sudo apt update && sudo apt install -y nginx""
  key_pair_name = aws_lightsail_key_pair.my_key_pair.name
}

resource ""aws_lightsail_key_pair"" ""my_key_pair"" {
  name       = ""my-key-pair""
  public_key = file(""~/.ssh/id_rsa.pub"")
}

output ""static_ipv4"" {
  value = aws_lightsail_instance.static_ipv4.public_ip_address
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""static_ipv4"":
   5: resource ""aws_lightsail_instance"" ""static_ipv4"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""static_ipv4"":
   5: resource ""aws_lightsail_instance"" ""static_ipv4"" {

The argument ""blueprint_id"" is required, but no definition was found.
",None,Terraform plan failed.
aws_lightsail_instance,create Amazon Lightsail with dualstack IP,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance with specific arguments
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
    resource.values.ip_address_type == ""dualstack""
}",1,"resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
  ip_address_type = ""dualstack""
 }","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with ""ip_address_type"" = ""dualstack"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""dualstack_ip"" {
  name        = ""dualstack-ip-instance""
  bundle_id   = ""nano_2_0""
  blueprint_id = ""amazon_linux_2""

  add_ons {
    httpd = {}
  }
}

resource ""aws_lightsail_static_ip"" ""dualstack_ip"" {
  name = ""dualstack-ip-static-ip""
}

resource ""aws_lightsail_static_ip_attachment"" ""attach_static_ip"" {
  static_ip_name = aws_lightsail_static_ip.dualstack_ip.id
  instance_name  = aws_lightsail_instance.dualstack_ip.id
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""dualstack_ip"":
   5: resource ""aws_lightsail_instance"" ""dualstack_ip"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Unsupported block type

  on main.tf line 10, in resource ""aws_lightsail_instance"" ""dualstack_ip"":
  10:   add_ons {

Blocks of type ""add_ons"" are not expected here. Did you mean ""add_on""?
",None,Terraform plan failed.
"aws_lightsail_instance, aws_lightsail_key_pair",create Amazon Lightsail with a separate SSH key,"package terraform.validation

default has_all_required_resources = false

# Rule for aws_lightsail_instance resource with specific arguments
has_valid_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
    resource.values.key_pair_name
}

# Rule for aws_lightsail_key_pair resource with specific arguments
has_valid_lightsail_key_pair {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_key_pair""
    resource.values.name
    resource.values.public_key
}

# Combined rule to ensure both resources meet their respective conditions
has_all_required_resources {
    has_valid_lightsail_instance
    has_valid_lightsail_key_pair
}",2,"resource ""aws_lightsail_key_pair"" ""lg_key_pair"" {
  name       = ""importing""
  public_key = file(""~/.ssh/id_rsa.pub"")
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
  key_pair_name = aws_lightsail_key_pair.lg_key_pair.name
 }","have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument
    with key_pair_name argument

have one ""aws_lightsail_key_pair"" resource
    with ""name"" argument
    with ""public_key""argument"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_key_pair"" ""default"" {
  name       = ""terraform-key""
  public_key = file(""~/.ssh/id_rsa.pub"")
}

resource ""aws_lightsail_instance"" ""default"" {
  name        = ""terraform-instance""
  zone        = ""us-east-1a""
  bundle_id   = ""nano_2_0""
  key_pair_name = aws_lightsail_key_pair.default.name
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 10, in resource ""aws_lightsail_instance"" ""default"":
  10: resource ""aws_lightsail_instance"" ""default"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 10, in resource ""aws_lightsail_instance"" ""default"":
  10: resource ""aws_lightsail_instance"" ""default"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 12, in resource ""aws_lightsail_instance"" ""default"":
  12:   zone        = ""us-east-1a""

An argument named ""zone"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_database,create AWS Lightsail that creates a managed database,"package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id
    resource.values.bundle_id
}",1,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""mysql_8_0""
  bundle_id                = ""micro_1_0""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    "," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""example"" {
  name        = ""terraform-managed-database""
  bundle_id   = ""nano_1_0""
  user_data   = file(""${path.module}/bootstrap.sh"")
  key_pair_name = aws_lightsail_key_pair.example.name
}

resource ""aws_lightsail_key_pair"" ""example"" {
  name       = ""terraform-key-pair""
  public_key = file(""${path.module}/id_rsa.pub"")
}

resource ""aws_lightsail_database"" ""example"" {
  instance_name   = aws_lightsail_instance.example.name
  database_name   = ""mydb""
  engine          = ""MySQL""
  engine_version  = ""5.7""
  master_username = ""admin""
  master_password = ""password123!""
}
```",False,No opa_result,"
Error: Invalid function argument

  on main.tf line 14, in resource ""aws_lightsail_key_pair"" ""example"":
  14:   public_key = file(""${path.module}/id_rsa.pub"")
    
     while calling file(path)
     path.module is "".""

Invalid value for ""path"" parameter: no file exists at ""./id_rsa.pub""; this
function works only with files that are distributed as part of the
configuration source code, so if this file will be created by a resource in
this configuration you must instead obtain this result from an attribute of
that resource.

Error: Missing required argument

  on main.tf line 17, in resource ""aws_lightsail_database"" ""example"":
  17: resource ""aws_lightsail_database"" ""example"" {

The argument ""master_database_name"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 17, in resource ""aws_lightsail_database"" ""example"":
  17: resource ""aws_lightsail_database"" ""example"" {

The argument ""bundle_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 17, in resource ""aws_lightsail_database"" ""example"":
  17: resource ""aws_lightsail_database"" ""example"" {

The argument ""relational_database_name"" is required, but no definition was
found.

Error: Missing required argument

  on main.tf line 17, in resource ""aws_lightsail_database"" ""example"":
  17: resource ""aws_lightsail_database"" ""example"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 18, in resource ""aws_lightsail_database"" ""example"":
  18:   instance_name   = aws_lightsail_instance.example.name

An argument named ""instance_name"" is not expected here.

Error: Unsupported argument

  on main.tf line 19, in resource ""aws_lightsail_database"" ""example"":
  19:   database_name   = ""mydb""

An argument named ""database_name"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_database,create an AWS Lightsail instance that creates a mysql database,"package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""mysql_8_0""
    resource.values.bundle_id
}",1,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""mysql_8_0""
  bundle_id                = ""micro_1_0""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = mysql_8_0
    with bundle_id
    "," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""mysql_instance"" {
  name              = ""mysql-instance""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""

  user_data = <<-EOF
    #!/bin/bash
    sudo yum update -y
    sudo yum install -y mysql
    sudo systemctl start mysqld
    sudo systemctl enable mysqld
  EOF
}

resource ""aws_lightsail_disk"" ""mysql_disk"" {
  name        = ""mysql-data""
  size_in_gb  = 30
  instance_id = aws_lightsail_instance.mysql_instance.id
}

output ""mysql_endpoint"" {
  value = ""${aws_lightsail_instance.mysql_instance.public_ip_address}:3306""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 20, in resource ""aws_lightsail_disk"" ""mysql_disk"":
  20: resource ""aws_lightsail_disk"" ""mysql_disk"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 23, in resource ""aws_lightsail_disk"" ""mysql_disk"":
  23:   instance_id = aws_lightsail_instance.mysql_instance.id

An argument named ""instance_id"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_database,"create AWS Lightsail that creates a mysql database. It should allow daily backups to take place between 16:00 and 16:30 each day and  requires any maintiance tasks (anything that would cause an outage, including changing some attributes) to take place on Tuesdays between 17:00 and 17:30","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.bundle_id
    resource.values.preferred_backup_window == ""16:00-16:30""
    resource.values.preferred_maintenance_window == ""Tue:17:00-Tue:17:30""
    
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""mysql_8_0""
  bundle_id                    = ""micro_1_0""
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    with  preferred_backup_window    = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""

    "," ```hcl
provider ""aws"" {
  region = ""us-east-2"" # Ensure this is a valid AWS region supporting Lightsail.
}

resource ""aws_lightsail_instance"" ""mysql_server"" {
  name        = ""mysql-server""
  bundle_id   = ""nano_1_0"" # Choose an appropriate instance size and plan.
  user_data   = <<EOF
              #!/bin/bash
              sudo apt-get update
              sudo apt-get install -y mysql-server
              sudo systemctl start mysql
              EOF
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  name        = ""mysql-db""
  instance_name = aws_lightsail_instance.mysql_server.id
  engine      = ""MySQL""
  engine_version = ""5.7"" # Choose a valid MySQL version supported by Lightsail.
  master_username = ""admin""
  master_user_password = ""yourStrong(!)Password""
}

resource ""aws_lightsail_alarm"" ""backup_alarm"" {
  name        = ""mysql-backup-alarm""
  instance_name = aws_lightsail_instance.mysql_server.id
  metric_name = ""CPUUtilization""
  statistic   = ""Average""
  period      = 300 # 5 minutes, which is shorter than the default Lightsail monitoring period.
  threshold   = 70 # Example threshold for CPU utilization. Adjust as needed.
  comparison_operator = ""GreaterThanThreshold""
}

resource ""aws_lightsail_alarm"" ""maintenance_window_alarm"" {
  name        = ""mysql-maintence-window-alarm""
  instance_name = aws_lightsail_instance.mysql_server.id
  metric_name = ""CPUUtilization""
  statistic   = ""Average""
  period      = 300 # Same as above, since the maintenance task will be scheduled in a similar monitoring interval.
  threshold   = 70 # Again, example threshold. Adjust based on expected usage and performance during maintenance tasks.
  comparison_operator = ""GreaterThanThreshold""
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""mysql_server"":
   5: resource ""aws_lightsail_instance"" ""mysql_server"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""mysql_server"":
   5: resource ""aws_lightsail_instance"" ""mysql_server"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""mysql_db"":
  16: resource ""aws_lightsail_database"" ""mysql_db"" {

The argument ""bundle_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""mysql_db"":
  16: resource ""aws_lightsail_database"" ""mysql_db"" {

The argument ""relational_database_name"" is required, but no definition was
found.

Error: Missing required argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""mysql_db"":
  16: resource ""aws_lightsail_database"" ""mysql_db"" {

The argument ""master_password"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""mysql_db"":
  16: resource ""aws_lightsail_database"" ""mysql_db"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""mysql_db"":
  16: resource ""aws_lightsail_database"" ""mysql_db"" {

The argument ""master_database_name"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 17, in resource ""aws_lightsail_database"" ""mysql_db"":
  17:   name        = ""mysql-db""

An argument named ""name"" is not expected here.

Error: Unsupported argument

  on main.tf line 18, in resource ""aws_lightsail_database"" ""mysql_db"":
  18:   instance_name = aws_lightsail_instance.mysql_server.id

An argument named ""instance_name"" is not expected here.

Error: Unsupported argument

  on main.tf line 22, in resource ""aws_lightsail_database"" ""mysql_db"":
  22:   master_user_password = ""yourStrong(!)Password""

An argument named ""master_user_password"" is not expected here.

Error: Invalid resource type

  on main.tf line 25, in resource ""aws_lightsail_alarm"" ""backup_alarm"":
  25: resource ""aws_lightsail_alarm"" ""backup_alarm"" {

The provider hashicorp/aws does not support resource type
""aws_lightsail_alarm"".

Error: Invalid resource type

  on main.tf line 35, in resource ""aws_lightsail_alarm"" ""maintenance_window_alarm"":
  35: resource ""aws_lightsail_alarm"" ""maintenance_window_alarm"" {

The provider hashicorp/aws does not support resource type
""aws_lightsail_alarm"".
",None,Terraform plan failed.
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""example"" {
  name        = ""postgres-instance""
  blueprint   = ""amazon_linux_2""
  bundle      = ""nano_2_0""
  key_pair    = aws_lightsail_key_pair.example.name
}

resource ""aws_lightsail_database"" ""example"" {
  name        = ""postgres-db""
  instance_arn = aws_lightsail_instance.example.arn
  engine      = ""PostgreSQL""
  version     = ""12""
  master_username = ""admin""
  master_user_password = ""yourStrong(!)Password""
}

resource ""aws_lightsail_key_pair"" ""example"" {
  name       = ""terraform-key-pair""
  public_key = file(""~/.ssh/id_rsa.pub"")
}

resource ""aws_lightsail_static_ip"" ""example"" {
  name = ""static-ip""
}

resource ""aws_lightsail_static_ip_attachment"" ""example"" {
  static_ip_name = aws_lightsail_static_ip.example.id
  instance_name  = aws_lightsail_instance.example.name
}

resource ""aws_backup_plan"" ""db_backup"" {
  name                = ""postgres-db-backup-plan""
  resource_type       = ""AWS::EC2::Instance""
  backup_region       = ""us-east-1""
  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_backup_selection"" ""db_backup_selection"" {
  plan_id     = aws_backup_plan.db_backup.id
  selection_tag {
    type  = ""AWS::EC2::Instance""
    key   = ""Name""
    value = aws_lightsail_instance.example.name
  }
}

resource ""aws_backup_vault"" ""db_backup_vault"" {
  name        = ""postgres-db-backup-vault""
  encryption_key_arn = aws_kms_key.db_encryption_key.arn
  lifecycle {
    prevent_destroy = true
  }
}

resource ""aws_kms_key"" ""db_encryption_key"" {
  description             = ""Encryption key for PostgreSQL database backups""
  enable_key_rotation     = true
}

resource ""aws_backup_vault_lock_configuration"" ""db_backup_vault_lock"" {
  backup_vault_name = aws_backup_vault.db_backup_vault.name
  change_able         = false
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""example"":
   5: resource ""aws_lightsail_instance"" ""example"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""example"":
   5: resource ""aws_lightsail_instance"" ""example"" {

The argument ""bundle_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_instance"" ""example"":
   5: resource ""aws_lightsail_instance"" ""example"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 7, in resource ""aws_lightsail_instance"" ""example"":
   7:   blueprint   = ""amazon_linux_2""

An argument named ""blueprint"" is not expected here.

Error: Unsupported argument

  on main.tf line 8, in resource ""aws_lightsail_instance"" ""example"":
   8:   bundle      = ""nano_2_0""

An argument named ""bundle"" is not expected here.

Error: Unsupported argument

  on main.tf line 9, in resource ""aws_lightsail_instance"" ""example"":
   9:   key_pair    = aws_lightsail_key_pair.example.name

An argument named ""key_pair"" is not expected here.

Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_database"" ""example"":
  12: resource ""aws_lightsail_database"" ""example"" {

The argument ""blueprint_id"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_database"" ""example"":
  12: resource ""aws_lightsail_database"" ""example"" {

The argument ""relational_database_name"" is required, but no definition was
found.

Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_database"" ""example"":
  12: resource ""aws_lightsail_database"" ""example"" {

The argument ""master_password"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_database"" ""example"":
  12: resource ""aws_lightsail_database"" ""example"" {

The argument ""master_database_name"" is required, but no definition was found.

Error: Missing required argument

  on main.tf line 12, in resource ""aws_lightsail_database"" ""example"":
  12: resource ""aws_lightsail_database"" ""example"" {

The argument ""bundle_id"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 13, in resource ""aws_lightsail_database"" ""example"":
  13:   name        = ""postgres-db""

An argument named ""name"" is not expected here.

Error: Unsupported argument

  on main.tf line 14, in resource ""aws_lightsail_database"" ""example"":
  14:   instance_arn = aws_lightsail_instance.example.arn

An argument named ""instance_arn"" is not expected here.

Error: Unsupported argument

  on main.tf line 16, in resource ""aws_lightsail_database"" ""example"":
  16:   version     = ""12""

An argument named ""version"" is not expected here.

Error: Unsupported argument

  on main.tf line 18, in resource ""aws_lightsail_database"" ""example"":
  18:   master_user_password = ""yourStrong(!)Password""

An argument named ""master_user_password"" is not expected here.

Error: Insufficient rule blocks

  on main.tf line 35, in resource ""aws_backup_plan"" ""db_backup"":
  35: resource ""aws_backup_plan"" ""db_backup"" {

At least 1 ""rule"" blocks are required.

Error: Unsupported argument

  on main.tf line 37, in resource ""aws_backup_plan"" ""db_backup"":
  37:   resource_type       = ""AWS::EC2::Instance""

An argument named ""resource_type"" is not expected here.

Error: Unsupported argument

  on main.tf line 38, in resource ""aws_backup_plan"" ""db_backup"":
  38:   backup_region       = ""us-east-1""

An argument named ""backup_region"" is not expected here.

Error: Unsupported argument

  on main.tf line 55, in resource ""aws_backup_vault"" ""db_backup_vault"":
  55:   encryption_key_arn = aws_kms_key.db_encryption_key.arn

An argument named ""encryption_key_arn"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable applying changes immediately instead of waiting for a maintiance window","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.apply_immediately == true
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name = ""test""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""testdatabasename""
  master_password          = ""testdatabasepassword""
  master_username          = ""test""
  blueprint_id             = ""postgres_12""
  bundle_id                = ""micro_1_0""
  apply_immediately        = true
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    apply_immediately  = true"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""example"" {
  name        = ""postgres-instance""
  availability_zone = ""us-east-1b""
  blueprint_id = ""amazon_linux_2""
  bundle_id   = ""nano_1_0""

  user_data = <<EOF
#!/bin/bash
sudo yum update -y
sudo amazon-linux-extras install postgresql13 -y
sudo systemctl enable postgresql-13
sudo systemctl start postgresql-13
sudo -u postgres psql -c ""CREATE DATABASE mydatabase;""
sudo -u postgres psql -c ""CREATE USER myuser WITH PASSWORD 'mypassword';""
sudo -u postgres psql -c ""GRANT ALL PRIVILEGES ON DATABASE mydatabase TO myuser;""
EOF
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'has_valid_lightsail_database': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_lightsail_disk, aws_availability_zones",create a Lightsail Disk resource,"package terraform.validation

default has_valid_lightsail_disk = false

# Rule for aws_lightsail_disk resource with specific arguments
has_valid_lightsail_disk {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_disk""
    resource.values.name
    resource.values.size_in_gb
    resource.values.availability_zone
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

resource ""aws_lightsail_disk"" ""test"" {
  name              = ""test""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}","have one aws_lightsail_disk resource
    with name
    with size_in_gb
    with availability_zone"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_disk"" ""example"" {
  name        = ""example-disk""
  size_in_gb  = 32
  snapshot_id = """" # You can specify a snapshot ID if needed, otherwise leave it empty.
}
```",False,No opa_result,"
Error: Missing required argument

  on main.tf line 5, in resource ""aws_lightsail_disk"" ""example"":
   5: resource ""aws_lightsail_disk"" ""example"" {

The argument ""availability_zone"" is required, but no definition was found.

Error: Unsupported argument

  on main.tf line 8, in resource ""aws_lightsail_disk"" ""example"":
   8:   snapshot_id = """" # You can specify a snapshot ID if needed, otherwise leave it empty.

An argument named ""snapshot_id"" is not expected here.
",None,Terraform plan failed.
aws_lightsail_certificate,Provides a lightsail certificate.,"package terraform.validation

default has_valid_lightsail_certificate = false

# Rule for aws_lightsail_certificate resource with specific arguments
has_valid_lightsail_certificate {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_certificate""
    resource.values.name
    resource.values.domain_name
}",1,"resource ""aws_lightsail_certificate"" ""test"" {
  name                      = ""test""
  domain_name               = ""testdomain.com""
  subject_alternative_names = [""www.testdomain.com""]
}","have one aws_lightsail_certificate
    with name
    with domain_name"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_certificate"" ""example"" {
  name        = ""example-cert""
  domain_name = ""example.com""
  certificate_chain = file(""${path.module}/certificate_chain.pem"")
  private_key   = file(""${path.module}/private_key.pem"")
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 8, in resource ""aws_lightsail_certificate"" ""example"":
   8:   certificate_chain = file(""${path.module}/certificate_chain.pem"")

An argument named ""certificate_chain"" is not expected here.

Error: Unsupported argument

  on main.tf line 9, in resource ""aws_lightsail_certificate"" ""example"":
   9:   private_key   = file(""${path.module}/private_key.pem"")

An argument named ""private_key"" is not expected here.
",None,Terraform plan failed.
aws_glacier_vault,an S3 Glacier vault for long-term data archiving,"package terraform.validation

default is_valid_glacier_vault = false

# Rule to validate if there is at least one aws_glacier_vault resource with a name attribute
is_valid_glacier_vault {
    # Find a resource that is of type aws_glacier_vault
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""

    # Check that the resource has a name attribute
    has_value(resource.values.name)
}

# Helper function to check if a value exists (not null)
has_value(value) {
    not is_null(value)
}",1,"provider ""aws"" {
  region = ""us-east-1""  # Replace with your desired AWS region
}

resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""archive_bucket"" {
  bucket = ""long-term-data-archive""
  force_destroy = true

  lifecycle_rule {
    id      = ""archive-to-glacier""
    enabled = true

    transition {
      days          = 365 * 4
      storage_class = ""GLACIER""
    }
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_glacier_vault': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_glacier_vault,generate a aws storage for long term data and backup,"package terraform.validation

default is_valid_glacier_vault = false

# Rule to validate if there is at least one aws_glacier_vault resource with a name attribute
is_valid_glacier_vault {
    # Find a resource that is of type aws_glacier_vault
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""

    # Check that the resource has a name attribute
    has_value(resource.values.name)
}

# Helper function to check if a value exists (not null)
has_value(value) {
    not is_null(value)
}",1,"provider ""aws"" {
  region = ""us-east-1""  # Replace with your desired AWS region
}

resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_s3_bucket"" ""long_term_backup"" {
  bucket = ""long-term-backup-bucket""
  acl    = ""private""

  tags = {
    Name        = ""Long Term Backup Bucket""
    Environment = ""Production""
  }
}

resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""s3.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""backup_policy"" {
  name = ""backup_policy""
  role = aws_iam_role.backup_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [""s3:GetBucketLocation"", ""s3:ListBucket""]
        Effect   = ""Allow""
        Resource = aws_s3_bucket.long_term_backup.arn
      },
      {
        Action = [""s3:PutObject"", ""s3:GetObject"", ""s3:DeleteObject""]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.long_term_backup.arn}/*""
      }
    ]
  })
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_glacier_vault': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_glacier_vault, aws_sns_topic",generage an S3 Glacier vault that pushes notification when archive retrieval completed,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'notification' attributes
is_valid_glacier_vault {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_glacier_vault""
    not is_null(resource.values.name)
    not is_null(resource.values.notification)
}

# Rule to check for the existence of an AWS SNS Topic resource with 'name' attribute
is_valid_sns_topic {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_sns_topic""
    not is_null(resource.values.name)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
    is_valid_glacier_vault
    is_valid_sns_topic
}",2,"provider ""aws"" {
  region     = ""us-east-1""
}

# Create an SNS topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-archive-retrieval-notifications""
}

# Create an S3 Glacier Vault
resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted""]
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with notification attribute
        with sns_topic attribute
        with events attribute

has one aws_sns_topic resources
    with name attribute"," ```hcl
provider ""aws"" {
  region = ""us-east-1"" # You can change this to your desired region
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""your-unique-bucket-name""
}

resource ""aws_sns_topic"" ""archive_notification"" {
  name = ""archive-retrieval-notification""
}

resource ""aws_s3_bucket_notification"" ""example"" {
  bucket = aws_s3_bucket.example.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.example.arn
    events              = [""s3:ObjectCreated:*""]
  }
}

resource ""aws_iam_role"" ""lambda_exec"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Principal = {
          Service = ""lambda.amazonaws.com""
        },
        Effect = ""Allow""
      }
    ]
  })
}

resource ""aws_lambda_function"" ""example"" {
  filename         = ""your-lambda-function.zip""
  function_name    = ""S3EventFunction""
  role             = aws_iam_role.lambda_exec.arn
  handler          = ""index.lambda_handler""
  runtime          = ""python3.8""
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_sns_topic': True, 'is_valid_terraform_plan': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_db_instance,create a basic AWS RDS instance,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  db_name              = ""mydb""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 20
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t2.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password123!""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,create a basic AWS SQL database,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine.constant_value == ""mysql""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region
}

resource ""aws_db_instance"" ""example"" {
  identifier             = ""terraform-example""
  engine                 = ""mysql""
  engine_version         = ""5.7""
  instance_class         = ""db.t2.micro""
  allocated_storage      = 20
  storage_type           = ""gp2""
  username               = ""admin""
  password               = ""password""
  parameter_group_name   = ""default.mysql5.7""
  skip_final_snapshot    = true
}
```",True,Success,No error,No error,
aws_db_instance,Create a db.t4g.large AWS PostgresSQL,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage.constant_value >= 20
        expressions.engine.constant_value == ""postgres""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class.constant_value == ""db.t4g.large""
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""postgres""
  instance_class       = ""db.t4g.large""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage"" minimun value is 20
with one ""engine"" with value ""postgres""
with one ""instance_class"" with value ""db.t4g.large""
with one ""password""
with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 10
  engine               = ""postgres""
  engine_version       = ""14.5""
  instance_class       = ""db.t4g.large""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,create a memory optimized PostgresSQL,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage.constant_value >= 20
        expressions.engine.constant_value == ""postgres""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""postgres""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage"" minimun value is 20
with one ""engine"" with value ""postgres""
with one ""instance_class"" with valid memory-optimized class
with one ""password""
with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 10 # Adjust based on your needs
  engine               = ""postgres""
  engine_version       = ""13.4""
  instance_class       = ""db.r5.large"" # Memory optimized instance class
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.postgres13""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,Create an AWS mySQL instance. Any changes on it will be applied immediately,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.apply_immediately.constant_value == true
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  apply_immediately = true
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" with value ""mysql""
with one ""instance_class"" 
with one ""password""
with one ""username""
with one ""apply_immediately"" set to true"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 10
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t2.micro""
  name                 = ""mydb""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydb""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,Create an AWS mySQL instance that skips the final snapshot,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.skip_final_snapshot.constant_value == true
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
        expressions.snapshot_identifier
}

requirement1(expressions) {
        expressions.replicate_source_db
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  skip_final_snapshot = true
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" with value ""mysql""
with one ""instance_class"" 
with one ""password""
with one ""username""
with one ""skip_final_snapshot"" set to true"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""my_sql_instance"" {
  allocated_storage    = 20
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password123!""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""my_sql_instance"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
"aws_db_instance, aws_db_instance","create an aws sql, and make a replica of it","package terraform.validation

default is_valid_db_instance = false

has_valid_replica {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
    resource.expressions.instance_class
    resource.expressions.replicate_source_db
}

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
    has_valid_replica
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.z1d.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}

resource ""aws_db_instance"" ""replica"" {
  replicate_source_db = aws_db_instance.default.arn
  instance_class       = ""db.z1d.micro""
  password             = ""1234567""
}","Has two ""aws_db_instance"" resources

resource 1:
with one ""allocated_storage""
with one ""engine"" and a valid engine value
with one ""instance_class"" and a valid instance class type
with one ""password""
with one ""username""

resource 2:
with one ""instance_class""
with one ""replicate_source_db"" = ""aws_db_instance.resource1.identifier"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""primary"" {
  allocated_storage    = 10
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t2.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}

resource ""aws_db_instance"" ""replica"" {
  identifier         = ""mydatabase-replica""
  replicate_source_db = aws_db_instance.primary.id
  instance_class     = ""db.t2.micro""
  publicly_accessible = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""primary"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,create a aws relational database from a snapshot,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}


requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""replica"" {
  snapshot_identifier = ""your identifier""
  instance_class       = ""db.z1d.micro""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 10
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  db_subnet_group_name = aws_db_subnet_group.example.name
  vpc_security_group_ids = [aws_security_group.rds.id]
}

resource ""aws_db_subnet_group"" ""example"" {
  name       = ""example-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]
}

resource ""aws_security_group"" ""rds"" {
  name_prefix = ""rds-sg-""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,"create a basic AWS RDS instance, with gp3 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""gp3""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 20
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""gp3""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 20
  storage_type         = ""gp3""
  engine               = ""postgres""
  engine_version       = ""14.5""
  instance_class       = ""db.t3.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  skip_final_snapshot  = true
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 11, in resource ""aws_db_instance"" ""example"":
  11:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,"create a basic AWS RDS instance, with io1 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""io1""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 100
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 100
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""io1""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""example"" {
  identifier              = ""terraform-rds-instance""
  engine                  = ""mysql""
  instance_class          = ""db.r5.large""
  allocated_storage       = 20
  storage_type            = ""io1""
  iops                    = 4000
  vpc_security_group_ids  = [""sg-xxxxxxxx""] # Replace with your security group ID
  db_subnet_group_name    = aws_db_subnet_group.example.name
  depends_on              = [aws_db_subnet_group.example]
}

resource ""aws_db_subnet_group"" ""example"" {
  name       = ""rds-subnet-group""
  subnet_ids = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy""] # Replace with your subnet IDs
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_db_instance': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_db_instance,create an aws database restored from s3,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.s3_import
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  s3_import {
    bucket_name           = ""mybucket""
    ingestion_role        = ""arn:aws:iam::1234567890:role/role-xtrabackup-rds-restore""
    source_engine         = ""mysql""
    source_engine_version = ""5.6""
  }
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one s3_import
        with one bucket_name
        with one ingestion_role
        with one source_engine
        with one source_engine_verison"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""backup"" {
  bucket = ""my-backup-bucket""
}

data ""aws_kms_key"" ""default"" {
  key_id = ""alias/aws/s3""
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""aurora-cluster-demo""
  engine                  = ""aurora-mysql""
  engine_version          = ""5.7.mysql_aurora.2.07.2""
  database_name           = ""mydb""
  master_username         = ""admin""
  master_password         = ""mypassword123""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
}

resource ""aws_rds_cluster_instance"" ""example"" {
  cluster_identifier = aws_rds_cluster.example.id
  instance_class     = ""db.r4.large""
  engine             = ""aurora-mysql""
  engine_version     = ""5.7.mysql_aurora.2.07.2""
}

resource ""aws_rds_cluster_snapshot"" ""example"" {
  db_cluster_identifier = aws_rds_cluster.example.id
  snapshot_name         = ""my-snapshot""
}

data ""aws_s3_bucket"" ""backup_bucket"" {
  bucket = ""my-backup-bucket""
}

resource ""aws_db_snapshot"" ""from_s3"" {
  db_instance_identifier = aws_rds_cluster.example.endpoint
  db_snapshot_identifier = ""restored-from-s3""
  source_region          = data.aws_s3_bucket.backup_bucket.region
  source_db_snapshot_arn = aws_rds_cluster_snapshot.example.db_cluster_snapshot_arn
}
```",False,No opa_result,"
Error: Invalid resource type

  on main.tf line 31, in resource ""aws_rds_cluster_snapshot"" ""example"":
  31: resource ""aws_rds_cluster_snapshot"" ""example"" {

The provider hashicorp/aws does not support resource type
""aws_rds_cluster_snapshot"". Did you mean ""aws_db_cluster_snapshot""?

Error: Unsupported argument

  on main.tf line 44, in resource ""aws_db_snapshot"" ""from_s3"":
  44:   source_db_snapshot_arn = aws_rds_cluster_snapshot.example.db_cluster_snapshot_arn

An argument named ""source_db_snapshot_arn"" is not expected here.
",None,Terraform plan failed.
aws_db_instance,create an AWS database that enables storage autoscaling,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.max_allocated_storage.constant_value >  resource.expressions.allocated_storage.constant_value
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  max_allocated_storage = 100
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one max_allocated_storage"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""example"" {
  allocated_storage    = 10
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t2.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  storage_autoscaling {
    enabled              = true
    target_storage_ratio = 75
  }
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""example"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.

Error: Unsupported block type

  on main.tf line 15, in resource ""aws_db_instance"" ""example"":
  15:   storage_autoscaling {

Blocks of type ""storage_autoscaling"" are not expected here.
",None,Terraform plan failed.
aws_db_instance,"create an aws database that Managed Master Passwords via Secrets Manager, default KMS Key","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}


requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  manage_master_user_password = true
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""username""
    with one manage_master_user_password"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_secretsmanager_secret"" ""rds_master_password"" {
  name                    = ""rds_master_password""
  recovery_window_in_days = 0
}

resource ""aws_kms_key"" ""default_kms_key"" {
  description             = ""Default KMS Key for RDS Master Password""
  enable_key_rotation     = true
}

resource ""aws_secretsmanager_secret_version"" ""rds_master_password_version"" {
  secret_id     = aws_secretsmanager_secret.rds_master_password.id
  secret_string = jsonencode({ username = ""admin"", password = var.rds_master_password })
}

variable ""rds_master_password"" {
  description = ""The master password for the RDS instance""
  type        = string
  default     = """"
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_db_instance': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
"aws_db_instance, aws_kms_key","create an aws database that Managed Master Passwords via Secrets Manager, with specific KMS Key","package terraform.validation

default is_valid_db_instance = false
default is_valid_kms_key = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

is_valid_kms_key {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_kms_key""
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  manage_master_user_password = true
  master_user_secret_kms_key_id = aws_kms_key.example.key_id
}

resource ""aws_kms_key"" ""example"" {
  description = ""Example KMS Key""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""username""
    with one manage_master_user_password
    with one ""master_user_secret_kms_key_id""  
 
has one ""aws_kms_key"""," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_secretsmanager_secret"" ""db_password"" {
  name                    = ""rds_master_password""
  recovery_window_in_days = 0
}

resource ""aws_secretsmanager_secret_version"" ""db_password_version"" {
  secret_id     = aws_secretsmanager_secret.db_password.id
  secret_string = jsonencode({
    username = ""admin""
    password = random_password.rds_password.result
  })
}

resource ""random_password"" ""rds_password"" {
  length           = 16
  special          = true
  override_special = ""_%@""
}

resource ""aws_kms_key"" ""rds_kms_key"" {
  description             = ""KMS key for RDS Master Password""
  enable_key_rotation     = true
  deletion_window_in_days = 7
}

output ""db_password"" {
  value = aws_secretsmanager_secret_version.db_password_version.secret_string
}
```",False,No opa_result,"
Error: Output refers to sensitive values

  on main.tf line 30:
  30: output ""db_password"" {

To reduce the risk of accidentally exporting sensitive data that was intended
to be only internal, Terraform requires that any root module output
containing sensitive data be explicitly marked as sensitive, to confirm your
intent.

If you do intend to export this data, annotate the output value as sensitive
by adding the following argument:
    sensitive = true
",None,Terraform plan failed.
"aws_db_instance, aws_db_snapshot",Create an AWS mySQL instance and a snapshot of the instance,"package terraform.validation

default is_valid_db_instance = false

default is_valid_db_snapshot = false

is_valid_db_snapshot {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_snapshot""
    resource.expressions.db_instance_identifier
    resource.expressions.db_snapshot_identifier
}

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
	expressions.snapshot_identifier
}

requirement1(expressions) {
	expressions.replicate_source_db
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",2,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}

resource ""aws_db_snapshot"" ""test"" {
  db_instance_identifier = aws_db_instance.default.identifier
  db_snapshot_identifier = ""testsnapshot1234""
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" and a valid engine value
with one ""instance_class"" and a valid instance class type
with one ""password""
with one ""username""

Has one ""aws_db_snapshot"" instance
with one ""db_instance_identifier""
with one ""db_snapshot_identifier"""," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""mysql_instance"" {
  allocated_storage    = 10
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  name                 = ""mydatabase""
  username             = ""admin""
  password             = ""password""
  parameter_group_name = ""default.mysql5.7""
}

resource ""aws_db_snapshot"" ""snapshot"" {
  db_instance_identifier = aws_db_instance.mysql_instance.id
  db_snapshot_identifier = ""${aws_db_instance.mysql_instance.id}-snapshot""
}
```",False,No opa_result,"
Error: Unsupported argument

  on main.tf line 10, in resource ""aws_db_instance"" ""mysql_instance"":
  10:   name                 = ""mydatabase""

An argument named ""name"" is not expected here.
",None,Terraform plan failed.
aws_efs_file_system,create a AWS EFS File System with tags,"package terraform.validation

default is_valid_efs_file_system = false

# Rule to check if a valid aws_efs_file_system exists
is_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
    resource.expressions.tags
}
",1,"resource ""aws_efs_file_system"" ""foo"" {
  creation_token = ""my-product""

  tags = {
    Name = ""MyProduct""
  }
}","Have one ""aws_efs_file_system"" resource
    with one tags"," ```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_efs_file_system"" ""example"" {
  creation_token   = ""terraform-example""
  performance_mode = ""generalPurpose""
  throughput_mode  = ""bursting""
  encrypted        = false

  tags = {
    Name        = ""Terraform Example""
    Environment = ""dev""
  }
}
```",True,Success,No error,No error,
aws_efs_file_system,create a AWS EFS Using lifecycle policy,"package terraform.validation

default is_valid_efs_file_system = false

# Rule to check if a valid aws_efs_file_system exists
is_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
    resource.expressions.lifecycle_policy
}",1,"resource ""aws_efs_file_system"" ""foo_with_lifecyle_policy"" {
  creation_token = ""my-product""

  lifecycle_policy {
    transition_to_ia = ""AFTER_30_DAYS""
  }
}","Have one ""aws_efs_file_system"" resource"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_efs_file_system"" ""example"" {
  creation_token = ""terraform-example""

  lifecycle_policy {
    transition_to_ia = ""AFTER_30_DAYS""
  }
}
```",True,Success,No error,No error,
"aws_efs_backup_policy, aws_efs_file_system",create a AWS EFS with automatic backups enabled,"package terraform.validation

default is_valid_efs_setup = false

# Rule to check if a valid aws_efs_file_system and aws_efs_file_system_policy exists
is_valid_efs_setup {
    has_valid_efs_file_system
    has_valid_aws_efs_backup_policy
}

# Helper rule to check if a valid aws_efs_file_system exists
has_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
}

# Helper rule to check if a valid aws_efs_file_system_policy exists with required arguments
has_valid_aws_efs_backup_policy {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_backup_policy""
    resource.expressions.backup_policy
    resource.expressions.file_system_id
}
",2,"resource ""aws_efs_file_system"" ""fs"" {
  creation_token = ""my-product""
}

resource ""aws_efs_backup_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.fs.id

  backup_policy {
    status = ""ENABLED""
  }
}","Have one ""aws_efs_file_system"" resource

Have one ""aws_efs_backup_policy"" resource
with one ""file_system_id""
with one ""backup_policy"""," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region
}

resource ""aws_efs_file_system"" ""example"" {
  creation_token   = ""terraform-example""
  performance_mode = ""generalPurpose""
  throughput_mode  = ""bursting""
  encrypted        = true

  lifecycle_policy {
    transition_to_ia = ""AFTER_30_DAYS""
  }
}

resource ""aws_efs_backup_policy"" ""example"" {
  file_system_id = aws_efs_file_system.example.id

  backup_policy {
    status = ""ENABLED""
  }
}
```",True,Success,No error,No error,
aws_egress_only_internet_gateway,"creates an egress-only internet gateway named ""pike"" associated with a specified VPC, allowing IPv6-enabled instances to connect to the internet without allowing inbound internet traffic, and tags it with ""permissions"".","package terraform.validation

# Set default validation state for aws_egress_only_internet_gateway
default is_valid_egress_only_internet_gateway = false

# Validate aws_egress_only_internet_gateway resource
is_valid_egress_only_internet_gateway {
	some i
	resource := input.configuration.root_module.resources[i]
	resource.type == ""aws_egress_only_internet_gateway""
	resource.expressions.vpc_id.constant_value != null
	resource.expressions.tags.constant_value != null # Verifies it is tagged with ""permissions""
}",2,"# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_egress_only_internet_gateway"" ""pike"" {
  vpc_id = ""vpc-0c33dc8cd64f408c4""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_egress_only_internet_gateway"" resource:
Associated with a specified ""aws_vpc"" resource
Designed to allow IPv6-enabled instances within the VPC to connect to the internet while preventing inbound internet traffic
Tagged with ""permissions"" to categorize or specify its role or access levels within the infrastructure"," ```hcl
provider ""aws"" {
  region = ""us-west-2""
}

variable ""vpc_id"" {
  description = ""The ID of the VPC""
  default     = ""vpc-12345678""
}

resource ""aws_egress_only_internet_gateway"" ""pike"" {
  vpc_id = var.vpc_id

  tags = {
    Name        = ""pike""
    permissions = ""allow-ipv6""
  }
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_egress_only_internet_gateway': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_egress_only_internet_gateway,"creates an egress-only internet gateway associated with a specified VPC, allowing IPv6-enabled instances to connect to the internet without allowing inbound internet traffic","package terraform.validation

# Set default validation state for aws_egress_only_internet_gateway
default is_valid_egress_only_internet_gateway = false

# Validate aws_egress_only_internet_gateway resource
is_valid_egress_only_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_egress_only_internet_gateway""
        resource.expressions.vpc_id.constant_value != null
        
}",1,"resource ""aws_egress_only_internet_gateway"" ""pike"" {
  vpc_id = ""vpc-0c33dc8cd64f408c4""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_egress_only_internet_gateway"" resource:
Associated with a specified ""aws_vpc"" resource
Designed to allow IPv6-enabled instances within the VPC to connect to the internet while preventing inbound internet traffic
"," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Replace with your desired region
}

variable ""vpc_id"" {
  description = ""The ID of the VPC""
  default     = ""vpc-12345678"" # Replace with your actual VPC ID
}

resource ""aws_egress_only_internet_gateway"" ""example"" {
  vpc_id = var.vpc_id
}
```",True,Failure,No error,"Rule violation found. OPA complete output logged here: {'result': [{'expressions': [{'value': {'terraform': {'validation': {'is_valid_egress_only_internet_gateway': False}}}, 'text': 'data', 'location': {'row': 1, 'col': 1}}]}]}",
aws_nat_gateway,creates a NAT Gateway associated with a specified subnet and Elastic IP allocation ID. The NAT Gateway is configured for public connectivity.,"package terraform.validation

# Set default validation state
default is_valid_nat_gateway = false

# Validate aws_nat_gateway resource
is_valid_nat_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_nat_gateway""

        # Ensure it is associated with a specified subnet
        resource.expressions.subnet_id != null

        # Ensure it uses a specific Elastic IP allocation ID
        resource.expressions.allocation_id != null
}
",2,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
}","Has one ""aws_nat_gateway"" resource:
Associated with a specified ""aws_subnet"" resource for hosting the NAT Gateway.
Utilizes an ""Elastic IP allocation ID"" to provide the NAT Gateway with a public IP address.
Configured for public connectivity, allowing resources within the private subnet to access the internet securely."," ```hcl
provider ""aws"" {
  region = ""us-west-2"" # Example region, adjust as needed
}

variable ""subnet_id"" {
  description = ""The ID of the subnet to associate with the NAT Gateway""
  default     = """"
}

variable ""allocation_id"" {
  description = ""The Elastic IP allocation ID for the NAT Gateway""
  default     = """"
}

resource ""aws_nat_gateway"" ""example"" {
  subnet_id     = var.subnet_id
  allocation_id = var.allocation_id
}
```",True,Success,No error,No error,
